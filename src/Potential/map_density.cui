// -*-c++-*-
#include "copyright.h"

/// \brief A naive approach to mapping density: loop over all atoms and barrage the memory space
///        with atomicAdd() operations.  This will take 
__global__ void __launch_bounds__(small_block_size, MAPPING_BLOCKS)
KERNEL_NAME(PMIGridAccumulator pm_acc, const CellGridReader<TMAT, void, TCALC, T4> cgr,
            const SyNonbondedKit<TCALC, TCALC2> synbk) {

  // An array of each warp's system-specific cell inversion matrices
  __shared__ volatile TCALC cell_umat[(small_block_size >> warp_bits) * 9];

  // The necessity of storing the transformation matrices in __shared__ allocates less than 2kB
  // of memory if TCALC is float, less than 4 if TCALC is double, even under the densest thread
  // configurations.  Place other metrics and infrequently used incrementors in __shared__ to
  // save registers and use the L1 that has been spent (minimum 8kB allocation on NVIDIA hardware).
  __shared__ volatile int warp_pos[small_block_size >> warp_bits];
  __shared__ volatile int sh_current_chain[small_block_size >> warp_bits];
  __shared__ volatile int sh_system_idx[small_block_size >> warp_bits];
  __shared__ volatile int sh_chain_bpos[small_block_size >> warp_bits];
  __shared__ volatile int sh_chain_cpos[small_block_size >> warp_bits];
  __shared__ volatile int sh_chain_cell_init[small_block_size >> warp_bits];
  __shared__ volatile uint sh_final_cell_atom_limits[small_block_size >> warp_bits];
  __shared__ volatile int sh_warp_intake, sh_total_warps, sh_warps_per_chain;
#if 0
  // When stepping through the cell lists in a single loop, it is necessary to recover the exact
  // stencil point at each iteration.  This can be labrious when done in the innermost loop.  It is
  // more efficient to pre-compute these values during setup and store them in a block-wide array.
  __shared__ volatile stencil_indices[warp_size_int * ((ORDER * ORDER * ORDER + 3) / 4)];
#endif
  // The atoms are in a list with significant gaps, the CellGrid's image array.  In order to
  // traverse that list with reasonably dense thread occupancy, work chain by chain, assigning
  // a certain number of warps out of the launch grid to each chain.  The number of warps that will
  // be assigned to each chain is decided immediately and without consideration of the overall
  // lengths of each chain.  While chains will have similar lengths within a given system and for a
  // collection of many systems, for systems with vast differences in size the chains may differ in
  // length by about the cube root of the size difference.  The map stride is stored in registers
  // on all threads to avoid bank conflicts if threads diverge further down in the inner loop.
  const int total_warps = ((blockDim.x * gridDim.x) >> warp_bits);
  const int map_stride = 4;
  
  // Warps will now proceed along each chain of the cell grid, taking in warp_intake atoms at a
  // time, broadcasting them to all threads using shuffles, and then proceed over all chains to
  // perform the mappings.
  const int warp_idx = (threadIdx.x >> warp_bits);
  const int lane_idx = (threadIdx.x & warp_bits_mask_int);
  if (lane_idx == 0) {
    warp_pos[warp_idx] = ((threadIdx.x + (blockIdx.x * blockDim.x)) >> warp_bits);
  }
  if (threadIdx.x == 0) {
    sh_total_warps = total_warps;
    sh_warp_intake = (warp_size_int / map_stride);
    const int warp_division = total_warps / cgr.total_chain_count;
    sh_warps_per_chain = (warp_division > 4) ? warp_division : 4;
  }
  __syncthreads();
  while (warp_pos[warp_idx] < cgr.total_chain_count * sh_warps_per_chain) {

    // Read the atoms.  Each atom's coordinates and property will be read once, used, and never
    // needed again.  The indexing arrays, however, will be read again and again.  Therefore,
    // cache the values from the indexing arrays and keep the atom coordinates and properties out
    // of the L1 cache.
    const int current_chain = warp_pos[warp_idx] / sh_warps_per_chain;
    
    // The chain index is known and this will indicate the cell's location along the system's
    // unit cell B and C axes.  However, it is not confirmed whether the atom at img_idx is part of
    // the chain, or even a valid atom in the image.  Furthermore, the location along the unit cell
    // A axis must be determined by binary search.  Again, cache these indices in L1.  They will be
    // used again, and with each cache line up to 16 additional cell limits will be brought in with
    // them which might serve other warps working on nearby chains.
    const int system_idx = __ldca(&cgr.chain_system_owner[current_chain]);
    const ullint sys_cg_dims = __ldca(&cgr.system_cell_grids[system_idx]);
    const int sys_cell_init = (sys_cg_dims & 0xfffffffLLU);
    const int cell_na = ((sys_cg_dims >> 28) & 0xfff);
    const int cell_nb = ((sys_cg_dims >> 40) & 0xfff);
    const int chain_in_sys = current_chain - __ldca(&cgr.system_chain_bounds[system_idx]);
    const int chain_cpos = chain_in_sys / cell_nb;
    const int chain_bpos = chain_in_sys - (chain_cpos * cell_nb);
    const int chain_cell_init = sys_cell_init + (((chain_cpos * cell_nb) + chain_bpos) * cell_na);
    const int chain_cell_finl = chain_cell_init + cell_na - 1;
    const uint2 final_cell_atom_limits = cgr.cell_limits[chain_cell_finl];

    // Commit the above wisdom to __shared__ if it is unlikely to be used frequently, to help
    // keep it out of registers as the warp continues to process atoms in the same chain.
    if (lane_idx == 0) {
      sh_current_chain[warp_idx] = current_chain;
      sh_system_idx[warp_idx] = system_idx;
      sh_chain_bpos[warp_idx] = chain_bpos;
      sh_chain_cpos[warp_idx] = chain_cpos;
      sh_chain_cell_init[warp_idx] = chain_cell_init;
      sh_final_cell_atom_limits[warp_idx] = final_cell_atom_limits.x +
                                            (final_cell_atom_limits.y >> 16);
    }

    // Take in the system's cell fractional coordinate matrix.  This information will also be
    // accessed repeatedly, so cache it in L1.  The warp size is the padded length for all known
    // architectures.  Save the roundUp() call.
#if defined(STORMM_USE_CUDA) || defined(STORMM_USE_HIP) || defined(STORMM_USE_INTEL)
    if (lane_idx < 9) {
      cell_umat[(warp_idx * 9) + lane_idx] =
        __ldca(&cgr.system_cell_umat[(warp_size_int * system_idx) + lane_idx]);
    }
#endif
    // Synchronize to ensure that all threads have consistent information before entering the
    // inner loop.
    SYNCWARP;

    // Once situated on a chain of spatial decomposition cells, each warp proceeds to the end of
    // the chain, seeking an indication that all atoms have been processed.
    uint atom_pos_in_chain = ((warp_pos[warp_idx] -
                               (sh_current_chain[warp_idx] * sh_warps_per_chain)) *
                              sh_warp_intake) + lane_idx;
    
    // Step along the atoms of the cell chain until all have been processed.
    int chain_apos;
    do {
      
      // Get the next image index and immediately increment the atom position in the chain
      uint img_idx = __ldca(&cgr.chain_limits[sh_current_chain[warp_idx]]) + atom_pos_in_chain;
      atom_pos_in_chain += sh_warp_intake * sh_warps_per_chain;
      T4 crdq;
      if (lane_idx < sh_warp_intake && img_idx < sh_final_cell_atom_limits[warp_idx]) {
#ifdef TMAT_IS_LONG
        crdq = cgr.image[img_idx];
#else
        crdq = __ldcv(&cgr.image[img_idx]);
#endif
        // Determine the position of the A cell
        int low_cell_est = sh_chain_cell_init[warp_idx];
        int hgh_cell_est = low_cell_est + cell_na;
        int mid_cell_est = ((low_cell_est + hgh_cell_est) >> 1);
        bool found;
        do {
          const uint2 mid_lims = cgr.cell_limits[mid_cell_est];
          const uint mid_min = mid_lims.x;
          const uint mid_max = mid_min + (mid_lims.y >> 16);
          if (img_idx < mid_min) {
            hgh_cell_est = mid_cell_est;
            found = false;
          }
          else if (img_idx < mid_max) {
            found = true;
          }
          else {
            low_cell_est = mid_cell_est;
            found = false;
          }
          mid_cell_est = ((low_cell_est + hgh_cell_est) >> 1);
        } while (! found);
        chain_apos = mid_cell_est - sh_chain_cell_init[warp_idx];
      }
      else {
        crdq = { (TMAT)(0), (TMAT)(0), (TMAT)(0), (TMAT)(0) };

        // Marking the chain cell's position along the unit cell A axis as -1 signifies that the
        // the particle was invalid.
        chain_apos = -1;
      }
      if (map_stride > 1) {

        // Copy each atom to other threads in the warp in the order [ 0, 1, ..., N, 0, 1, ...,
        // N, ... 0, 1, ..., N ] with map_stride repeats of the sequence over warp_intake atoms.
        const int lane_source = (lane_idx & (sh_warp_intake - 1));
        crdq.x = SHFL(crdq.x, lane_source);
        crdq.y = SHFL(crdq.y, lane_source);
        crdq.z = SHFL(crdq.z, lane_source);
        crdq.w = SHFL(crdq.w, lane_source);
        chain_apos = SHFL(chain_apos, lane_source);
      }
      else {

        // Synchronization is needed before the spatial decomposition cell's transformation matrix
        // is repopulated, to prevent some threads from barreling through and changing the contents
        // before others have finished using it once.  This synchronization is handled implicitly
        // by shuffle instructions if the mapping stride is greater than one.
        SYNCWARP;
      }
    
      // The following are only performed for valid atoms.
      if (chain_apos < 0) {
        continue;
      }
    
      // Resolve the home grid point and the deltas
      const size_t w_nine = warp_idx * 9;
#ifdef TMAT_IS_REAL
      const TCALC frac_a = (cell_umat[w_nine    ] * crdq.x) + (cell_umat[w_nine + 3] * crdq.y) + 
                           (cell_umat[w_nine + 6] * crdq.z);
      const TCALC frac_b = (cell_umat[w_nine + 4] * crdq.y) + (cell_umat[w_nine + 7] * crdq.z);
      const TCALC frac_c =                                    (cell_umat[w_nine + 8] * crdq.z);

      // Resolve the density quantity to release the 16- or 32-byte crdq object from registers.
      TCALC q;
      switch (pm_acc.theme) {
      case NonbondedTheme::ELECTROSTATIC:
        switch (cgr.theme) {
        case NonbondedTheme::ELECTROSTATIC:
          q = crdq.w;
          break;
        case NonbondedTheme::VAN_DER_WAALS:
          break;
        case NonbondedTheme::ALL:
          {
#  ifdef TMAT_IS_LONG
            const llint qprm_as_lli = __double_as_longlong(crdq.w);
            const int qprm_idx = (qprm_as_lli & dp_charge_index_mask);
#  else
            const int qprm_as_i = __float_as_int(crdq.w);
            const int qprm_idx = (qprm_as_i & sp_charge_index_mask);
#  endif
            q = synbk.q_params[qprm_idx];
          }
          break;
        }
        break;
      case NonbondedTheme::VAN_DER_WAALS:
        switch (cgr.theme) {
        case NonbondedTheme::ELECTROSTATIC:
          break;
        case NonbondedTheme::VAN_DER_WAALS:
          {
#  ifdef TMAT_IS_LONG
            const int tlj_idx = __double_as_longlong(crdq.w);
#  else
            const int tlj_idx = __float_as_int(crdq.w);
#  endif
            q = synbk.ljb_coeff[synbk.ljabc_offsets[sh_system_idx[warp_idx]] +
                                ((synbk.n_lj_types[sh_system_idx[warp_idx]] + 1) * tlj_idx)];
#  ifdef TCALC_IS_DOUBLE
            q = sqrt((TCALC)(0.25) * q);
#  else
            q = sqrtf((TCALC)(0.25) * q);
#  endif
          }
          break;
        case NonbondedTheme::ALL:
          {
#  ifdef TMAT_IS_LONG
            const llint ljidx_as_lli = __double_as_longlong(crdq.w);
            const int tlj_idx = (ljidx_as_lli >> dp_charge_index_bits);
#  else
            const int ljidx_as_i = __float_as_int(crdq.w);
            const int tlj_idx = (ljidx_as_i >> sp_charge_index_bits);
#  endif
            q = synbk.ljb_coeff[synbk.ljabc_offsets[sh_system_idx[warp_idx]] +
                                ((synbk.n_lj_types[sh_system_idx[warp_idx]] + 1) * tlj_idx)];
#  ifdef TCALC_IS_DOUBLE
            q = sqrt((TCALC)(0.25) * q);
#  else
            q = sqrtf((TCALC)(0.25) * q);
#  endif
          }
          break;
        }
        break;
      case NonbondedTheme::ALL:
        break;
      }
#else // TMAT_IS_REAL
      const TCALC crd_x = (TCALC)(crdq.x) * cgr.lpos_inv_scale;
      const TCALC crd_y = (TCALC)(crdq.y) * cgr.lpos_inv_scale;
      const TCALC crd_z = (TCALC)(crdq.z) * cgr.lpos_inv_scale;
      const TCALC frac_a = (cell_umat[w_nine    ] * crd_x) + (cell_umat[w_nine + 3] * crd_y) + 
                           (cell_umat[w_nine + 6] * crd_z);
      const TCALC frac_b = (cell_umat[w_nine + 4] * crd_y) + (cell_umat[w_nine + 7] * crd_z);
      const TCALC frac_c =                                   (cell_umat[w_nine + 8] * crd_z);

      // Resolve the density quantity to release the 16- or 32-byte crdq object from registers.
      TCALC q;
      switch (pm_acc.theme) {
      case NonbondedTheme::ELECTROSTATIC:
        switch (cgr.theme) {
        case NonbondedTheme::ELECTROSTATIC:
#  ifdef TMAT_IS_LONG
          q = __longlong_as_double(crdq.w);
#  else
          q = __int_as_float(crdq.w);
#  endif
          break;
        case NonbondedTheme::VAN_DER_WAALS:
          break;
        case NonbondedTheme::ALL:
          {
#  ifdef TMAT_IS_LONG
            const int qprm_idx = (crdq.w & dp_charge_index_mask);
#  else
            const int qprm_idx = (crdq.w & sp_charge_index_mask);
#  endif
            q = synbk.q_params[qprm_idx];
          }
          break;
        }
        break;
      case NonbondedTheme::VAN_DER_WAALS:
        switch (cgr.theme) {
        case NonbondedTheme::ELECTROSTATIC:
          break;
        case NonbondedTheme::VAN_DER_WAALS:
          {
            const int tlj_idx = crdq.w;
            q = synbk.ljb_coeff[synbk.ljabc_offsets[sh_system_idx[warp_idx]] +
                                ((synbk.n_lj_types[sh_system_idx[warp_idx]] + 1) * tlj_idx)];
#  ifdef TCALC_IS_DOUBLE
            q = sqrt((TCALC)(0.25) * q);
#  else
            q = sqrtf((TCALC)(0.25) * q);
#  endif
          }
          break;
        case NonbondedTheme::ALL:
          {
#  ifdef TMAT_IS_LONG
            const int tlj_idx = (crdq.w >> dp_charge_index_bits);
#  else
            const int tlj_idx = (crdq.w >> sp_charge_index_bits);
#  endif
            q = synbk.ljb_coeff[synbk.ljabc_offsets[sh_system_idx[warp_idx]] +
                                ((synbk.n_lj_types[sh_system_idx[warp_idx]] + 1) * tlj_idx)];
#  ifdef TCALC_IS_DOUBLE
            q = sqrt((TCALC)(0.25) * q);
#  else
            q = sqrtf((TCALC)(0.25) * q);
#  endif
          }
          break;
        }
        break;
      case NonbondedTheme::ALL:
        break;
      }
#endif // TMAT_IS_REAL

      // Compute the grid index within the spatial decomposition cell
      int ida, idb, idc;
#ifdef TCALC_IS_DOUBLE
      const TCALC da = imageLocalFraction(frac_a, cgr.mesh_ticks, &ida);
      const TCALC db = imageLocalFraction(frac_b, cgr.mesh_ticks, &idb);
      const TCALC dc = imageLocalFraction(frac_c, cgr.mesh_ticks, &idc);
#else
      const TCALC da = imageLocalFractionf(frac_a, cgr.mesh_ticks, &ida);
      const TCALC db = imageLocalFractionf(frac_b, cgr.mesh_ticks, &idb);
      const TCALC dc = imageLocalFractionf(frac_c, cgr.mesh_ticks, &idc);
#endif
      idb += sh_chain_bpos[warp_idx] * cgr.mesh_ticks;
      idc += sh_chain_cpos[warp_idx] * cgr.mesh_ticks;
      ida += chain_apos * cgr.mesh_ticks;
    
      // Compute B-spline coefficients given the deltas.
      TCALC bspln_a_knots[ORDER], bspln_b_knots[ORDER], bspln_c_knots[ORDER];
#if ORDER == 4
      devcBSpline4(da, bspln_a_knots);
      devcBSpline4(db, bspln_b_knots);
      devcBSpline4(dc, bspln_c_knots);
#elif ORDER == 5
      devcBSpline5(da, bspln_a_knots);
      devcBSpline5(db, bspln_b_knots);
      devcBSpline5(dc, bspln_c_knots);
#elif ORDER == 6
      devcBSpline6(da, bspln_a_knots);
      devcBSpline6(db, bspln_b_knots);
      devcBSpline6(dc, bspln_c_knots);
#endif
      for (int i = 0; i < pm_acc.order; i++) {
        bspln_a_knots[i] *= q;
      }

      // Loop over all grid points, issuing atomic additions to the particle-mesh interaction grid.
      int itp = lane_idx / sh_warp_intake;
      const uint4 sys_pm_dims = pm_acc.dims[sh_system_idx[warp_idx]];
      switch (pm_acc.mode) {
      case PrecisionModel::DOUBLE:
        if (pm_acc.use_overflow) {
          while (itp < ORDER * ORDER * ORDER) {
            const int itp_c = itp / (ORDER * ORDER);
            const int itp_b = (itp - (itp_c * ORDER * ORDER)) / ORDER;
            const int itp_a = itp - (((itp_c * ORDER) + itp_b) * ORDER);
            uint gpos_a = ida + itp_a;
            uint gpos_b = idb + itp_b;
            uint gpos_c = idc + itp_c;
            gpos_a -= (gpos_a >= sys_pm_dims.x) * sys_pm_dims.x;
            gpos_b -= (gpos_b >= sys_pm_dims.y) * sys_pm_dims.y;
            gpos_c -= (gpos_c >= sys_pm_dims.z) * sys_pm_dims.z;
            const uint gpos = sys_pm_dims.w +
                              (((gpos_c * sys_pm_dims.y) + gpos_b) * sys_pm_dims.x) + gpos_a;
            const TCALC dq = bspln_a_knots[itp_a] * bspln_b_knots[itp_b] * bspln_c_knots[itp_c] *
                             pm_acc.fp_scale;
            atomicSplit(dq, gpos, pm_acc.lldata, pm_acc.overflow);
            itp += map_stride;
          }
        }
        else {
          while (itp < ORDER * ORDER * ORDER) {
            const int itp_c = itp / (ORDER * ORDER);
            const int itp_b = (itp - (itp_c * ORDER * ORDER)) / ORDER;
            const int itp_a = itp - (((itp_c * ORDER) + itp_b) * ORDER);
            uint gpos_a = ida + itp_a;
            uint gpos_b = idb + itp_b;
            uint gpos_c = idc + itp_c;
            gpos_a -= (gpos_a >= sys_pm_dims.x) * sys_pm_dims.x;
            gpos_b -= (gpos_b >= sys_pm_dims.y) * sys_pm_dims.y;
            gpos_c -= (gpos_c >= sys_pm_dims.z) * sys_pm_dims.z;
            const uint gpos = sys_pm_dims.w +
                              (((gpos_c * sys_pm_dims.y) + gpos_b) * sys_pm_dims.x) + gpos_a;
            const TCALC dq = bspln_a_knots[itp_a] * bspln_b_knots[itp_b] * bspln_c_knots[itp_c] *
                             pm_acc.fp_scale;
            atomicAdd((ullint*)&pm_acc.lldata[gpos], (ullint)(__double2ll_rn(dq)));
            itp += map_stride;
          }
        }
        break;
      case PrecisionModel::SINGLE:
        if (pm_acc.use_overflow) {
          while (itp < ORDER * ORDER * ORDER) {
            const int itp_c = itp / (ORDER * ORDER);
            const int itp_b = (itp - (itp_c * ORDER * ORDER)) / ORDER;
            const int itp_a = itp - (((itp_c * ORDER) + itp_b) * ORDER);
            uint gpos_a = ida + itp_a;
            uint gpos_b = idb + itp_b;
            uint gpos_c = idc + itp_c;
            gpos_a -= (gpos_a >= sys_pm_dims.x) * sys_pm_dims.x;
            gpos_b -= (gpos_b >= sys_pm_dims.y) * sys_pm_dims.y;
            gpos_c -= (gpos_c >= sys_pm_dims.z) * sys_pm_dims.z;
            const uint gpos = sys_pm_dims.w +
                              (((gpos_c * sys_pm_dims.y) + gpos_b) * sys_pm_dims.x) + gpos_a;
            const TCALC dq = bspln_a_knots[itp_a] * bspln_b_knots[itp_b] * bspln_c_knots[itp_c] *
                             pm_acc.fp_scale;
            atomicSplit(dq, gpos, pm_acc.idata, pm_acc.overflow);
            itp += map_stride;
          }
        }
        else {
          while (itp < ORDER * ORDER * ORDER) {
            const int itp_c = itp / (ORDER * ORDER);
            const int itp_b = (itp - (itp_c * ORDER * ORDER)) / ORDER;
            const int itp_a = itp - (((itp_c * ORDER) + itp_b) * ORDER);
            uint gpos_a = ida + itp_a;
            uint gpos_b = idb + itp_b;
            uint gpos_c = idc + itp_c;
            gpos_a -= (gpos_a >= sys_pm_dims.x) * sys_pm_dims.x;
            gpos_b -= (gpos_b >= sys_pm_dims.y) * sys_pm_dims.y;
            gpos_c -= (gpos_c >= sys_pm_dims.z) * sys_pm_dims.z;
            const uint gpos = sys_pm_dims.w +
                              (((gpos_c * sys_pm_dims.y) + gpos_b) * sys_pm_dims.x) + gpos_a;
            const TCALC dq = bspln_a_knots[itp_a] * bspln_b_knots[itp_b] * bspln_c_knots[itp_c] *
                             pm_acc.fp_scale;
            atomicAdd(&pm_acc.idata[gpos], (int)(__float2int_rn(dq)));
            itp += map_stride;
          }
        }
        break;
      }
    } while (BALLOT(chain_apos >= 0));

    // Increment the warp counter after completing the inner loop.  The warp vote above implies an
    // implicit synchronization, ensuring that this action does not disrupt the work of other
    // threads that might be referencing the __shared__ array.
    if (lane_idx == 0) {
      warp_pos[warp_idx] += sh_total_warps;
    }
    SYNCWARP;
  }
}
