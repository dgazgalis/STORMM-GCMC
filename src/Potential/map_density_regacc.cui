// -*-c++-*-
#include "copyright.h"

__global__ void __launch_bounds__(DENSITY_SPREADING_THREADS, MAPPING_BLOCKS)
KERNEL_NAME(PMIGridWriter pm_wrt, const CellGridReader<TMAT, void, TCALC, T4> cgr,
            const SyNonbondedKit<TCALC, TCALC2> synbk, MMControlKit<TCALC> ctrl) {

  // An array to store as many as CELL_STORE_A x CELL_STORE_BC x CELL_STORE_BC cells in the
  // appropriate floating-point precision.  For cells containing 4 x 4 x 4 grid points, the
  // storage is 256 bytes per cell for float and 512 for double.  It is possible to store up to
  // 60 such cells in 16 kB or 32 kB of __shared__ and still have room for the minutiae of cell
  // boundaries.  For 5 x 5 x 5 cells, it is possible to store 32 such cells in 16 kB or 32 kB of
  // __shared__ using float or double, respectively.  Different work units will compute B-splines
  // for the necessary arrangements of cells to fill out the density in their respective volumes
  // of storage density.
  __shared__ TCALC cell_umat[9];
#if ORDER == 5
  __shared__ TCALC real_storage[STORAGE_VOLUME];
  __shared__ uint bspln_footprint[max5s_atom_bearing_cross_section * warp_size_zu];
  __shared__ uint bspln_alignment[max5s_atom_bearing_cross_section * warp_size_zu];
  __shared__ TCALC bspln_a_coefficients[5 * max5s_atom_bearing_cross_section * warp_size_zu];
  __shared__ TCALC bspln_b_coefficients[5 * max5s_atom_bearing_cross_section * warp_size_zu];
  __shared__ TCALC bspln_c_coefficients[5 * max5s_atom_bearing_cross_section * warp_size_zu];
#elif ORDER == 6
  __shared__ TCALC real_storage[STORAGE_VOLUME];
  __shared__ ullint bspln_footprint[max6s_atom_bearing_cross_section * warp_size_zu];
  __shared__ ullint bspln_alignment[max6s_atom_bearing_cross_section * warp_size_zu];
  __shared__ TCALC bspln_a_coefficients[6 * max6s_atom_bearing_cross_section * warp_size_zu];
  __shared__ TCALC bspln_b_coefficients[6 * max6s_atom_bearing_cross_section * warp_size_zu];
  __shared__ TCALC bspln_c_coefficients[6 * max6s_atom_bearing_cross_section * warp_size_zu];
#endif
  __shared__ volatile int pmwu_idx, gmap_task_counter;
  __shared__ volatile uint warp_completions;

  // The particle->mesh work unit consists of 32 unsigned ints.  The update tracker contains two
  // sets of unsigned int masks, either of which is sufficient to track whether the grid-mapping
  // region in a given cell could have been affected by atoms processed in the most recent cycle
  // no matter what configuration the work unit is in or the order of interpolation the kernel is
  // designed to support.  See below.
  __shared__ volatile uint pmwu[density_mapping_wu_size], update_tracker[32];

  // The critical tasks tracker is refreshed for each pass over atoms in the atom-bearing region,
  // before updating the grid-mapping region.  Each task implies at least 32 elements of storage.
  // It is not expected that any configuration will have more than 512 tasks, which would imply
  // 64kB of storage and break the __shared__ memory model.  The array contains a bit-packed
  // arrangement for each task, containing the local cell coordinates along the A, B, and C axes
  // in bits 0-3, 4-7, and 8-11, the number of task within the particular cell in bits 12-23 (the
  // spatial decomposition cells might contain as many as 16 grid points on a side), and an
  // indication of whether the task is valid in the highest bit.
#if ORDER == 5
  __shared__ volatile uint critical_tasks[96];
#else
  __shared__ volatile uint critical_tasks[96];
#endif
  // The prefix sums are stored in groups of 8, with the maximum length of any cell chain being 6.
#if ORDER == 5
  __shared__ volatile int chain_prefix_sums[max5s_atom_bearing_cross_section * 8];
  __shared__ volatile uint chain_global_atom_limits[max5s_atom_bearing_cross_section * 8];
#else
  __shared__ volatile int chain_prefix_sums[max6s_atom_bearing_cross_section * 8];
  __shared__ volatile uint chain_global_atom_limits[max6s_atom_bearing_cross_section * 8];
#endif  
  // The kernel executes work units containing three phases each:
  //
  // - Read the work unit, compute prefix sums over each chain.  Zero out the primary and overflow
  //   storage accumulators in __shared__.
  // - Cycle through the following, handling one warp size worth of atoms per chain at a time:
  //   * Read the atoms of each cell in chains along the unit cell A axis.  Compute B-splines and
  //     alignments to the local grid, then commit the results to __shared__.  Record the extent of
  //     the newly contributed atoms along each chain, and whether each warp has reached the end of
  //     its respective chain.
  //   * Loop over all grid points, focusing one thread on each grid point that may have been
  //     affected by the newly contributed atoms.  For each grid point, loop over the atom grid
  //     alignments for the four chains that might contribute to the point and build up lists of
  //     atoms that will definitely contribute to the point.  Loop back over each list, and for
  //     each contributing atom pick out the coefficients by which it will contribute, then take
  //     their product and accumulate the result in registers.
  //   * Sum the results of grid point accumulations with the __shared__ primary and overflow
  //     accumulators (this is not an atomic process, as only one thread operates on each grid
  //     point per cycle).
  // - When all chains are finished, 
  //   
  // and finally re-read the storage volume and write results to global memory.  Typical work units
  // will apply to volumes of 6 x 3 x 3 cells in 5th order interpolation (4 x 4 x 4 grid points per
  // cell with 7 x 4 x 4 cells' worth of B-splines to compute for 2.07x the B-spline computation
  // work of a naive kernel, and roughly 25% hit rates when mapping particles to a particular mesh
  // point), or 6 x 3 x 2 in 6th order interpolation (5 x 5 x 5 grid points per cell, 7 x 4 x
  // 3 cells' worth of B-splines to compute for 2.33x the B-spline work relative to a naive kernel,
  // and 22% hit rates when mapping particles to a given mesh point).
  const int warp_idx = (threadIdx.x >> warp_bits);
  const int lane_idx = (threadIdx.x & warp_bits_mask_int);
  if (warp_idx == 0) {
    if (threadIdx.x == 0) {
      pmwu_idx = blockIdx.x;
    }
    SYNCWARP;
    for (int pos = lane_idx; pos < density_mapping_wu_size; pos += warp_size_int) {
      pmwu[pos] = pm_wrt.work_units[(pmwu_idx * density_mapping_wu_size) + pos];
    }

    // Getting the same information again from L1 avoids the need for another SYNCWARP call
    const int sysid = pm_wrt.work_units[(pmwu_idx * density_mapping_wu_size) + 16];
#if defined(STORMM_USE_CUDA) || defined(STORMM_USE_HIP) || defined(STORMM_USE_INTEL)
    if (lane_idx < 9) {
      cell_umat[lane_idx] = cgr.system_cell_umat[(sysid * warp_size_int) + lane_idx];
    }
#endif
  }
  else if (warp_idx == 1) {
    if (lane_idx == 0) {

      // Mark the high two warps as having "completed" the march across their respective
      // atom-bearing cell chains (these two warps do not deal with B-spline computations on atoms,
      // rather the accumulation of density due to B-spline coefficients in the second phase of
      // the inner loop).
      warp_completions = 0x30000;
    }
    for (int pos = lane_idx; pos < 32; pos += warp_size_int) {
      update_tracker[pos] = 0U;
    }
  }
  else {

    // The thread block is designed to have eighteen warps, sixteen cells in the cross section of
    // the atom-bearing region, and nine in the cross-section of the grid-mapping region.  When it
    // is time read atoms and compute B-spline coefficients, 16 of the eighteen warps are active.
    // When it comes time to compute the grid contributions, all eighteen warps become active.  At
    // this time, the remaining sixteen warps can be put to work zeroing the __shared__ grid
    // accumulators.
    for (int pos = threadIdx.x - twice_warp_size_int; pos < STORAGE_VOLUME;
         pos += 16 * warp_size_int) {
      real_storage[pos] = (TCALC)(0.0);
    }
  }
  __syncthreads();
  while (pmwu_idx < pm_wrt.wu_count) {

    // On NVIDIA architectures, a warp is 32 threads, which implies that, for cells spanned by
    // 4 x 4 x 4 grid elements, each thread in the warp will need to handle two grid points.  There
    // will be 18 warps in the block, each of the first 16 warps taking one cell of the
    // cross-section of the atom-bearing region of the work unit:
    //
    //          -- B -->
    //           0 1 2 3
    //    |   0  @ @ @ @     Atom-bearing cross section, @
    //    C   1  @ # # #
    //    |   2  @ # # #     Grid-filling cross section, #  
    //    v   3  @ # # #
    //
    // With the work unit read, each warp will work in its respective column, first computing a
    // prefix sum over all cells' atom counts within its two chains and then proceeding to take
    // atoms, one warp width at a time, from within the cells of its chains.  The AMD 64-threaded
    // "fat" warp and the Intel 16-threaded "skinny" warp will both also fit this paradigm, albeit
    // with some adjustments in terms of how the B-spline coefficients get computed.
    uint landmark = pmwu[warp_idx];
    const uint extent = (landmark >> 28);
    landmark &= 0xfffffff;
    const size_t wuc_idx = (8 * warp_idx) + lane_idx;
    if (lane_idx < extent) {
      int global_cell_idx = landmark + lane_idx;
      global_cell_idx -= (global_cell_idx >= pmwu[23]) * pmwu[23];
      const uint2 tmp_cl = cgr.cell_limits[global_cell_idx];
      chain_global_atom_limits[wuc_idx] = tmp_cl.x;
      chain_prefix_sums[wuc_idx] = (tmp_cl.y >> 16);
    }
    else if (lane_idx < 8) {
      chain_prefix_sums[wuc_idx] = 0;
    }
    if (lane_idx < 8) {
      int var = chain_prefix_sums[wuc_idx];
      var += ((lane_idx &  1) == 1) * __shfl_up_sync(0xff, var, 1);
      var += ((lane_idx &  3) == 3) * __shfl_up_sync(0xff, var, 2);
      var += (lane_idx  == 7) * __shfl_up_sync(0xff, var, 4);
      var += ((lane_idx &  3) == 1 && lane_idx >  4)  * __shfl_up_sync(0xff, var, 2);
      var += ((lane_idx &  1) == 0 && lane_idx >= 2) * __shfl_up_sync(0xff, var, 1);
      var = __shfl_up_sync(0xff, var, 1);
      if (lane_idx == 0) {
        var = 0;
      }
      chain_prefix_sums[wuc_idx] = var;
    }
    SYNCWARP;
    
    // Each thread of the warp will now take an atom from its chain, in order, and record the local
    // cell index where it lives in the chain.  The interpolation footprints along the A, B, and C
    // unit cell axes of the cell that the atom occupies and any cell to the right, back, or above
    // will be recorded in the first, second, and third stretches of N bits of a single unsigned
    // integer.  A fourth stretch of bits will have a single 1, signifying the cell, out of the
    // local region where the atom lies in its chain.  For interpolation order <= 5, N is 8 (four
    // grid points per spatial decomposition cell) and the maximum number of cells in a chain of
    // the atom-bearing region is 7, so all of this information will fit in four 8-bit segments of
    // a uint32_t (unsigned int).  For interpolation orders >= 6, the number of grid points per
    // spatial decomposition cell rises, and although the maximum depth of the chains in the
    // atom-bearing region falls due to memory limitations a uint64_t (unsigned long long int) is
    // needed.
    int atom_progress = lane_idx;
    int chain_cell_home = 8 * warp_idx;
    const int atom_goal = chain_prefix_sums[chain_cell_home + extent];
    int pass_index = 0;
    do {

      // Compute and log B-spline coefficients for all threads reading a valid atom.
      if (atom_progress < atom_goal) {

        // Determine the cell in the local chain, find the atom's offset within the cell, then
        // look up the atom in the cell grid.
        while (chain_prefix_sums[chain_cell_home + 1] < atom_progress) {
          chain_cell_home++;
        }
        const int atom_offset_in_cell = atom_progress - chain_prefix_sums[chain_cell_home];
        const uint atom_global_pos = chain_global_atom_limits[chain_cell_home] +
                                     atom_offset_in_cell;
#if ORDER <= 5
        // Begin to build up the footprint, starting with the local number of atom's cell home in
        // its own chain.  Push that information to the high eight bits of the number.
        uint footprint = (0x1 << (24 + chain_cell_home - (8 * warp_idx)));
#else
        ullint footprint = (0x1LLU << (48 + chain_cell_home - (8 * warp_idx)));
#endif
        const T4 crdq = cgr.image[atom_global_pos];
#ifdef TMAT_IS_REAL
        TCALC da = (cell_umat[0] * (TCALC)(crdq.x)) + (cell_umat[3] * (TCALC)(crdq.y)) +
                   (cell_umat[6] * (TCALC)(crdq.z));
        TCALC db = (cell_umat[4] * (TCALC)(crdq.y)) + (cell_umat[7] * (TCALC)(crdq.z));
        TCALC dc =                                    (cell_umat[8] * (TCALC)(crdq.z));

        // Obtain the density quantity here, to help the compiler release the 16- or 32-byte crdq
        // object from registers.
        TCALC q;
#  ifdef MAP_CHARGES
#    if defined(CG_THEME_ELECTROSTATIC)
        q = crdq.w;
#    elif defined(CG_THEME_ALL)
#      ifdef TMAT_IS_LONG
        const llint qprm_as_lli = __double_as_longlong(crdq.w);
        const int qprm_idx = (qprm_as_lli & dp_charge_index_mask);
#      else
        const int qprm_as_i = __float_as_int(crdq.w);
        const int qprm_idx = (qprm_as_i & sp_charge_index_mask);
#      endif
        q = synbk.q_params[qprm_idx];
#    endif
#  endif // MAP_CHARGES
#  ifdef MAP_DISPERSION
#    if defined(CG_THEME_VAN_DER_WAALS)
#      ifdef TMAT_IS_LONG
        const int tlj_idx = __double_as_longlong(crdq.w);
#      else
        const int tlj_idx = __float_as_int(crdq.w);
#      endif
#    elif defined(CG_THEME_ALL)
#      ifdef TMAT_IS_LONG
        const llint ljidx_as_lli = __double_as_longlong(crdq.w);
        const int tlj_idx = (ljidx_as_lli >> dp_charge_index_bits);
#      else
        const int ljidx_as_i = __float_as_int(crdq.w);
        const int tlj_idx = (ljidx_as_i >> sp_charge_index_bits);
#      endif
#    endif
        q = synbk.ljb_coeff[synbk.ljabc_offsets[pmwu[16]] +
                            ((synbk.n_lj_types[pmwu[16]] + 1) * tlj_idx)];
#    ifdef TCALC_IS_DOUBLE
        q = sqrt((TCALC)(0.25) * q);
#    else
        q = sqrtf((TCALC)(0.25) * q);
#    endif
#  endif // MAP_DISPERSION
#else // TMAT_IS_REAL
        const TCALC tmp_x = (TCALC)(crdq.x) * cgr.lpos_inv_scale;
        const TCALC tmp_y = (TCALC)(crdq.y) * cgr.lpos_inv_scale;
        const TCALC tmp_z = (TCALC)(crdq.z) * cgr.lpos_inv_scale;
        TCALC da = (cell_umat[0] * tmp_x) + (cell_umat[3] * tmp_y) + (cell_umat[6] * tmp_z);
        TCALC db =                          (cell_umat[4] * tmp_y) + (cell_umat[7] * tmp_z);
        TCALC dc =                                                   (cell_umat[8] * tmp_z);

        // Obtain the density quantity here, to help the compiler do away with the 16- or 32-byte
        // crdq object.
        TCALC q;
#  ifdef MAP_CHARGES
#    if defined(CG_THEME_ELECTROSTATIC)
#      ifdef TMAT_IS_LONG
        q = __longlong_as_double(crdq.w);
#      else
        q = __int_as_float(crdq.w);
#      endif
#    elif defined(CG_THEME_ALL)
#      ifdef TMAT_IS_LONG
        const int qprm_idx = (crdq.w & dp_charge_index_mask);
#      else
        const int qprm_idx = (crdq.w & sp_charge_index_mask);
#      endif
        q = synbk.q_params[qprm_idx];
#    endif
#  endif // MAP_CHARGES
#  ifdef MAP_DISPERSION
#    if defined(CG_THEME_VAN_DER_WAALS)
        const int tlj_idx = crdq.w
#    elif defined(CG_THEME_ALL)
#      ifdef TMAT_IS_LONG
        const int tlj_idx = (crdq.w >> dp_charge_index_bits);
#      else
        const int tlj_idx = (crdq.w >> sp_charge_index_bits);
#      endif
#    endif
        q = synbk.ljb_coeff[synbk.ljabc_offsets[pmwu[16]] +
                            ((synbk.n_lj_types[pmwu[16]] + 1) * tlj_idx)];
#    ifdef TCALC_IS_DOUBLE
        q = sqrt((TCALC)(0.25) * q);
#    else
        q = sqrtf((TCALC)(0.25) * q);
#    endif
#  endif // MAP_DISPERSION
#endif
        int ida, idb, idc;
        da = imageLocalFraction(da, cgr.mesh_ticks, &ida);
        db = imageLocalFraction(db, cgr.mesh_ticks, &idb);
        dc = imageLocalFraction(dc, cgr.mesh_ticks, &idc);
#if ORDER == 5
        const uint abc_footprint = 0x1fU;
        footprint |= (abc_footprint << (16 + idc));
        footprint |= (abc_footprint << ( 8 + idb));
        footprint |= (abc_footprint << (     ida));
#elif ORDER == 6
        const ullint abc_footprint = 0x3fLLU;
        footprint |= (abc_footprint << (32 + idc));
        footprint |= (abc_footprint << (16 + idb));
        footprint |= (abc_footprint << (     ida));
#endif
        bspln_footprint[threadIdx.x] = footprint;

        // Compute the local grid index for this point, as if there were a grid spanning the entire
        // atom-bearing region.  The grid-mapping region is smaller than this by cgr.mesh_ticks in
        // all directions, but this indexing will be useful to determine which coefficients to use
        // when known atoms are contributing to a specific grid point.
        const int local_gpt_a = ((chain_cell_home - (8 * warp_idx)) * cgr.mesh_ticks) + ida;
        const int local_c = warp_idx / pmwu[21];
        const int local_b = (warp_idx - (local_c * pmwu[21])) / pmwu[20]; 
        const int local_gpt_b = (local_b * cgr.mesh_ticks) + idb;
        const int local_gpt_c = (local_c * cgr.mesh_ticks) + idc;
        bspln_alignment[threadIdx.x] = (local_gpt_a | (local_gpt_b << 10) | (local_gpt_c << 10));

        // Compute the B-spline coefficients along the A axis and multiply through by the charge
        TCALC bspln_knots[ORDER];
#if ORDER == 5
        devcBSpline5(da, bspln_knots);
        const int ifac = max5s_atom_bearing_cross_section * warp_size_int;
#elif ORDER == 6
        devcBSpline6(da, bspln_knots);
        const int ifac = max6s_atom_bearing_cross_section * warp_size_int;
#endif
        for (int i = 0; i < pm_wrt.order; i++) {
          bspln_a_coefficients[threadIdx.x + (i * ifac)] = bspln_knots[i] * q;
        }

        // Compute the B-spline coefficients along the B axis
#if ORDER == 5
        devcBSpline5(db, bspln_knots);
#elif ORDER == 6
        devcBSpline6(db, bspln_knots);
#endif
        for (int i = 0; i < pm_wrt.order; i++) {
          bspln_b_coefficients[threadIdx.x + (i * ifac)] = bspln_knots[i];
        }

        
        // Compute the B-spline coefficients along the C axis
#if ORDER == 5
        devcBSpline5(dc, bspln_knots);
#elif ORDER == 6
        devcBSpline6(dc, bspln_knots);
#endif
        for (int i = 0; i < pm_wrt.order; i++) {
          bspln_c_coefficients[threadIdx.x + (i * ifac)] = bspln_knots[i];
        }
      }
      else {

        // Mark the footprint as zero to signify that no grid point will be affected by the lack
        // of an atom.
#if ORDER == 5
        bspln_footprint[threadIdx.x] = 0U;
#elif ORDER == 6
        bspln_footprint[threadIdx.x] = 0LLU;
#endif
        // The warp has completed its task.  Use an atomic operation to mark this fact (the
        // unsigned int in __shared__ is not an array but a single value, so its bits cannot be
        // set by independent threads without risk of a collision).
        const uint test_mask = (0x1U << warp_idx);
        if (lane_idx == warp_bits_mask_int && (warp_completions & test_mask) == 0U) {
          atomicOr((uint*)&warp_completions, test_mask);
        }
      }

      // The atom computations are proceeding forward along the unit cell A axis, forward along A
      // in the local frame of the grid-mapping volume.  Reduce all threads' contributions for the
      // home cells to determine what bad of the accumulated grid volume may be affected.
#if ORDER == 5
      uint reduced_footprint = bspln_footprint[threadIdx.x];
#elif ORDER == 6
      ullint reduced_footprint = bspln_footprint[threadIdx.x];
#endif
#ifdef STORMM_USE_CUDA
      reduced_footprint |= SHFL_DOWN(reduced_footprint, 16);
      reduced_footprint |= SHFL_DOWN(reduced_footprint, 8);
      reduced_footprint |= SHFL_DOWN(reduced_footprint, 4);
      reduced_footprint |= SHFL_DOWN(reduced_footprint, 2);
      reduced_footprint |= SHFL_DOWN(reduced_footprint, 1);
#endif
      if (lane_idx == 0) {

        // B-splines computed for an atom in cell {a, b, c} could affect grid mapping in cells
        // {a:a+1, b:b+1, c:c+1}.  Use a simple bitwise OR operation on the reduced footprint to
        // mark all cells a+1.  Mark the presence of populated cells in this chain, the chain +1
        // along the B unit cell axis, the chain +1 along C, and the chain +1 along both B and C.
        reduced_footprint |= (reduced_footprint << 1);
        const int nr_b = pmwu[21];
        const int nr_c = pmwu[22];

        // Skip for warps that were not involved in the actual computations.  This can happen for
        // atom-bearing regions with cross sections smaller than 4 x 4, e.g. 3 x 5 or 2 x 6.  Warps
        // that would not contribute to calculations in the atom-bearing region were marked with
        // zero cells' extent in their corresponding indices of the work unit (pmwu[0-15]).
        if (warp_idx < nr_b * nr_c) {
          const int warp_pos_c = warp_idx / nr_b;
          const int warp_pos_b = (warp_idx - (warp_pos_c * nr_b));

          // No setup will allocate a cross section of greater than 16 cells.
          const int pass_offset = (pass_index << 4);
          atomicOr((int*)&update_tracker[pass_offset + (warp_pos_c * nr_b) + warp_pos_b],
                   reduced_footprint);
          if (warp_pos_b < nr_b) {
            atomicOr((int*)&update_tracker[pass_offset + (warp_pos_c * nr_b) + warp_pos_b + 1],
                     reduced_footprint);
            if (warp_pos_c < nr_c) {
              atomicOr((int*)&update_tracker[pass_offset + ((warp_pos_c + 1) * nr_b) +
                                             warp_pos_b + 1], reduced_footprint);
            }
          }
          if (warp_pos_c < nr_c) {
            atomicOr((int*)&update_tracker[pass_offset + ((warp_pos_c + 1) * nr_b) + warp_pos_b],
                     reduced_footprint);
          }
        }
      }

      // Set a __shared__ counter for warps to traverse the grid-mapping region in an asynchronous
      // manner.
      if (threadIdx.x == 0) {
        gmap_task_counter = (blockIdx.x >> warp_bits);
      }
      __syncthreads();

      // Pre-compute whether each task involves a cell that might have updates.  For interpolation
      // orders <= 5, there will be up to 48 * 2 = 96 tasks for NVIDIA's 32-threaded warp.  For
      // interpolation order 6, the number of tasks becomes to 2 * 4 * 3 * 4 = 96 due to the
      // smaller available grid mapping region but larger number of warps needed to cover all grid
      // points in any given cell.  Having one thread flag each task and storing the results in bit
      // masks is more efficient than having all threads of a warp compute the necessity of a
      // particular task, or wait on one to do so.
      int gmap_task = threadIdx.x;
      while (gmap_task < (pmwu[27] & 0xffffU)) {

        // Determine where in the atom-bearing region this task lies.  The cell location in the
        // grid-mapping region is the local {a, b, c} cell coordinate minus {1, 1, 1}.  Multiply
        // by cgr.mesh_ticks for the grid coordinates in the local patch.
        const int nr_a = pmwu[20] - 1;
        const int nr_b = pmwu[21] - 1;
        const int nr_c = pmwu[22] - 1;
        const int nw   = (pmwu[27] >> 16);
        int local_c = gmap_task / (nr_b * nr_a * nw);
        int local_b = (gmap_task - (local_c * nr_b * nr_a * nw)) / (nr_a * nw);
        int local_a = (gmap_task - (((local_c * nr_b) + local_b) * nr_a * nw)) / nw;
        const int local_w = gmap_task - (((((local_c * nr_b) + local_b) * nr_a) + local_a) * nw);
        local_a++;
        local_b++;
        local_c++;
        
        // Determine whether the cell (as positioned in the atom-bearing region) has any updates
        // that might affect it.  If so, store the previous work to derive the local cell indices
        // and the task's position within the cell.
        if ((update_tracker[(pass_index << 4) + (local_c * nr_b) + local_b] >> local_a) & 0x1) {
          critical_tasks[gmap_task] = ((local_a) | (local_b << 4) | (local_c << 8) |
                                       (local_w << 12) | 0x80000000U);
        }
        else {
          critical_tasks[gmap_task] = 0U;
        }
        gmap_task += blockDim.x;
      }
      __syncthreads();
      
      // Whether the warps have completed their tasks or not, each has deposited a series of atoms
      // into the B-spline arrays.  Loop over all grid points that may have been affected, as
      // indicated by checked bits (set to 1) in the high 8 bits of the update tracker.  All warps
      // can begin to participate: the new cycle is to determine the patch of points to operate on,
      // then check the relevant update_tracker entry to determine whether there is any work to do.
      // If there is, loop over all atoms in the relevant four chains to develop four bit masks
      // (32-bit unsigned ints) indicating which atoms in the chain will contribute to the point.
      gmap_task = warp_idx;
      while (gmap_task < (pmwu[27] & 0xffff)) {
        const uint tmp_ctask = critical_tasks[gmap_task];
        if (tmp_ctask) {

          // Extract previous work to obtain the local position of the work unit in the cell.
          // Find the position of the grid point served by each thread.
          const int local_a = (tmp_ctask & 0xf);
          const int local_b = ((tmp_ctask >>  4) & 0xf);
          const int local_c = ((tmp_ctask >>  8) & 0xf);
          const int local_w = ((tmp_ctask >> 12) & 0xfff);
          const int cell_gpt_idx = (local_w * warp_size_int) + lane_idx;
          const int cell_gpt_c = cell_gpt_idx / cgr.mesh_ticks;
          const int cell_gpt_b = (cell_gpt_idx - (cell_gpt_c * cgr.mesh_ticks)) / cgr.mesh_ticks;
          const int cell_gpt_a = cell_gpt_idx - (((cell_gpt_c * cgr.mesh_ticks) +
                                                  cell_gpt_b) * cgr.mesh_ticks);

          // Loop over all atoms in each of four relevant chains, comparing their footprints to a
          // pair of masks computed for the grid point.  Atoms within two cells in each chain might
          // contribute, and the masks will identify which ones if all their bits are matched.
          uint cmask[4];
          cmask[0] = 0;
          cmask[1] = 0;
          cmask[2] = 0;
          cmask[3] = 0;
          for (int ib = 0; ib < 2; ib++) {
            for (int ic = 0; ic < 2; ic++) {
#if ORDER <= 5
              const uint mask_abc = ((0x1U << cell_gpt_a)                   |
                                     (0x1U << (cell_gpt_b + (ib * 4) +  8)) |
                                     (0x1U << (cell_gpt_c + (ic * 4) + 16)) |
                                     (0x1U << (local_a + 24)));
              const uint mask_ambc = ((0x1U << (cell_gpt_a + 4))             |
                                      (0x1U << (cell_gpt_b + (ib * 4) +  8)) |
                                      (0x1U << (cell_gpt_c + (ic * 4) + 16)) |
                                      (0x1U << (local_a + 23)));
#elif ORDER == 6
              const ullint mask_abc = ((0x1LLU << cell_gpt_a)                   |
                                       (0x1LLU << (cell_gpt_b + (ib * 5) + 16)) |
                                       (0x1LLU << (cell_gpt_c + (ic * 5) + 32)) |
                                       (0x1LLU << (local_a + 48)));
              const ullint mask_ambc = ((0x1LLU << (cell_gpt_a + 5))             |
                                        (0x1LLU << (cell_gpt_b + (ib * 5) + 16)) |
                                        (0x1LLU << (cell_gpt_c + (ic * 5) + 32)) |
                                        (0x1LLU << (local_a + 47)));
#endif
              const int chain_data_offset = warp_size_int * (((local_c - ic) * pmwu[21]) +
                                                             local_b - ib);
              const int ibc = (ic * 2) + ib;
              for (int i = 0; i < warp_size_int; i++) {
                const uint ifoot = bspln_footprint[chain_data_offset + i];
                cmask[ibc] |= ((ifoot & mask_abc ) == mask_abc ||
                               (ifoot & mask_ambc) == mask_ambc);
              }
            }
          }
          
          // Loop back over all bits of the four masks, using __popc() to count the number of
          // relevant atoms and then looping, using __ffs() to identify the next atom in each mask
          // and moving on to the next mask when there are no more atoms to process
          const int ncontribs = __popc(cmask[0]) + __popc(cmask[1]) + __popc(cmask[2]) +
                                __popc(cmask[3]);
          const int local_gpt_a = cell_gpt_a + (local_a * cgr.mesh_ticks);
          const int local_gpt_b = cell_gpt_b + (local_b * cgr.mesh_ticks);
          const int local_gpt_c = cell_gpt_c + (local_c * cgr.mesh_ticks);
          int cmask_con = 0;
          int chain_source = warp_size_int * ((local_c * pmwu[21]) + local_b);
          TCALC q_acc = 0.0;
          for (int i = 0; i < ncontribs; i++) {

            // Advance to a mask with content
            while (cmask[cmask_con] == 0U) {
              cmask_con++;

              // Determine which chain content is now coming from
              chain_source = warp_size_int * (((local_c - ((cmask_con & 0x2) >> 1)) * pmwu[21]) +
                                              local_b - (cmask_con & 0x1));
            }

            // Find the next contributing atom
            const int next = __ffs(cmask[cmask_con]);

            // Determine the proper B-spline coefficients based on the contributing atom's
            // alignment to the local patch and the thread's alignment to the local patch.
            const uint atom_alignment = bspln_alignment[chain_source + next];
            const int a_cof_idx = local_gpt_a - (atom_alignment & 0x3ff);
            const int b_cof_idx = local_gpt_b - ((atom_alignment >> 10) & 0x3ff);
            const int c_cof_idx = local_gpt_c - ((atom_alignment >> 20) & 0x3ff);

            // Retrieve the B-spline coefficients from their respective arrays, multiply them,
            // and accumulate the result.
#if ORDER == 5
            const int coef_stagger = max5s_atom_bearing_cross_section * warp_size_int;
#elif ORDER == 6
            const int coef_stagger = max6s_atom_bearing_cross_section * warp_size_int;
#endif
            q_acc += bspln_a_coefficients[(a_cof_idx * coef_stagger) + chain_source + next] *
                     bspln_b_coefficients[(b_cof_idx * coef_stagger) + chain_source + next] *
                     bspln_c_coefficients[(c_cof_idx * coef_stagger) + chain_source + next];

            // Erase the set bit to mark this atom contribution as complete
            cmask[cmask_con] &= ~(0x1U << next);
          }
          
          // Store the accumulated density
          const int gm_gpt_a = local_gpt_a - cgr.mesh_ticks;
          const int gm_gpt_b = local_gpt_b - cgr.mesh_ticks;
          const int gm_gpt_c = local_gpt_c - cgr.mesh_ticks;
          const int gm_len_a = (pmwu[20] - 1) * cgr.mesh_ticks;
          const int gm_len_b = (pmwu[21] - 1) * cgr.mesh_ticks;
          real_storage[(((gm_gpt_c * gm_len_b) + gm_gpt_b) * gm_len_a) + gm_gpt_a] += q_acc;
        }
        
        // Increment the task with an atomic operation
        SYNCWARP;
        if (lane_idx == 0) {
          gmap_task = atomicAdd((int*)&gmap_task_counter, 1);
        }
        SYNCWARP;
      }

      // Toggle the cycle counter to switch to a clean tracking bitmask on the next pass.  Every
      // thread must track this in order to use the correct array index without a race condition.
      // The alternative is to put it in __shared__ and rope off the update with another block-wide
      // synchronization.
      pass_index = 1 - pass_index;
      __syncthreads();
      
      // Increment the progress over atoms
      atom_progress += warp_size_int;
    } while (warp_completions != 0x3ffff);

    // Write the results of the storage area to global memory.
    const int pencil_len = cgr.mesh_ticks * (pmwu[20] - 1);
    const int nb_pencil  = cgr.mesh_ticks * (pmwu[21] - 1);
    const int nc_pencil  = cgr.mesh_ticks * (pmwu[22] - 1);
    const int a_pencil_start = cgr.mesh_ticks * (pmwu[17] + 1);
    const int b_pencil_start = cgr.mesh_ticks * (pmwu[18] + 1);
    const int c_pencil_start = cgr.mesh_ticks * (pmwu[19] + 1);
    int warp_pos = warp_idx;
    while (warp_pos < nb_pencil * nc_pencil) {
      uint cpos = (warp_pos / nc_pencil);
      uint bpos = warp_pos - (cpos * nc_pencil);
      bpos += b_pencil_start;
      cpos += c_pencil_start;
      bpos -= (bpos >= pmwu[24]) * pmwu[24];
      cpos -= (cpos >= pmwu[25]) * pmwu[25];
      for (int pos = lane_idx; pos < pencil_len; pos += warp_size_int) {
        uint apos = a_pencil_start + pos;
        apos -= (apos >= pmwu[23]) * pmwu[23];
        const size_t gidx = pmwu[26] + (((cpos * pmwu[24]) + bpos) * pmwu[23]) + apos;
#ifdef TCALC_IS_DOUBLE
        pm_wrt.ddata[gidx] = real_storage[(warp_pos * pencil_len) + pos];
#else
        pm_wrt.fdata[gidx] = real_storage[(warp_pos * pencil_len) + pos];
#endif
      }
      warp_pos += (blockDim.x >> warp_bits);
    }
    __syncthreads();

    // Select the next work unit for this thread block and initialize critical information.
    if (warp_idx == 0) {
      if (threadIdx.x == 0) {
        const size_t prog_counter_idx = (ctrl.step & twice_warp_bits_mask_int);
        pmwu_idx = atomicAdd(&ctrl.pmewu_progress[prog_counter_idx], 1);
      }
      SYNCWARP;
      for (int pos = lane_idx; pos < density_mapping_wu_size; pos += warp_size_int) {
        pmwu[pos] = pm_wrt.work_units[(density_mapping_wu_size * pmwu_idx) + pos];
      }
      const int sysid = pm_wrt.work_units[(pmwu_idx * density_mapping_wu_size) + 16];
#if defined(STORMM_USE_CUDA) || defined(STORMM_USE_HIP) || defined(STORMM_USE_INTEL)
      if (lane_idx < 9) {
        cell_umat[lane_idx] = cgr.system_cell_umat[(sysid * warp_size_int) + lane_idx];
      }
#endif
    }
    else if (warp_idx == 1) {
      if (lane_idx == 0) {
        warp_completions = 0x30000;
      }
      for (int pos = lane_idx; pos < 32; pos += warp_size_int) {
        update_tracker[pos] = 0U;
      }
    }
    else {
      for (int pos = threadIdx.x - twice_warp_size_int; pos < STORAGE_VOLUME;
           pos += 16 * warp_size_int) {
        real_storage[pos] = (TCALC)(0.0);
      }
    }
    __syncthreads();    
  }

  // Set the block counters for future iterations of this kernel
  if (blockIdx.x == 0 && threadIdx.x < warp_size_int) {
    const int step_modulus = (ctrl.step & twice_warp_bits_mask_int);
    if (step_modulus == 0) {
      ctrl.pmewu_progress[threadIdx.x + warp_size_int] = gridDim.x;
    }
    if (step_modulus == warp_size_int) {
      ctrl.pmewu_progress[threadIdx.x] = gridDim.x;
    }
  }
}
