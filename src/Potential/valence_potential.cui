// -*-c++-*-

#define EXCL_GMEM_OFFSET  (blockIdx.x * gmem_r.max_atoms)

//-------------------------------------------------------------------------------------------------
__global__ void __launch_bounds__(VALENCE_KERNEL_THREAD_COUNT, 1)
KERNEL_NAME(const SyValenceKit<TCALC> poly_vk, const SyRestraintKit<TCALC, TCALC2, TCALC4> poly_rk,
            MMControlKit<TCALC> ctrl, PsSynthesisWriter poly_psw,
#ifdef COMPUTE_ENERGY
            ScoreCardWriter scw,
#endif
            CacheResourceKit<TCALC> gmem_r) {

  // Coordinates and properties of particles are copied into special, L1-cached arrays of GMEM used
  // exclusively by this block.  In this manner, only forces are held in __shared__ where they can
  // be accumulated with more efficient atomics.
#ifdef COMPUTE_FORCE
#  ifdef SPLIT_FORCE_ACCUMULATION
  __shared__ int sh_xfrc[maximum_valence_work_unit_atoms];
  __shared__ int sh_yfrc[maximum_valence_work_unit_atoms];
  __shared__ int sh_zfrc[maximum_valence_work_unit_atoms];
  __shared__ int xoverflow_active[maximum_valence_work_unit_atoms / warp_size_int];
  __shared__ int yoverflow_active[maximum_valence_work_unit_atoms / warp_size_int];
  __shared__ int zoverflow_active[maximum_valence_work_unit_atoms / warp_size_int];
#  else
  __shared__ long long int sh_xfrc[maximum_valence_work_unit_atoms];
  __shared__ long long int sh_yfrc[maximum_valence_work_unit_atoms];
  __shared__ long long int sh_zfrc[maximum_valence_work_unit_atoms];
#  endif
#endif
#ifdef COMPUTE_ENERGY
  __shared__ llint sh_bond_acc[VALENCE_KERNEL_THREAD_COUNT >> warp_bits];
  __shared__ llint sh_angl_acc[VALENCE_KERNEL_THREAD_COUNT >> warp_bits];
  __shared__ llint sh_dihe_acc[VALENCE_KERNEL_THREAD_COUNT >> warp_bits];
  __shared__ llint sh_impr_acc[VALENCE_KERNEL_THREAD_COUNT >> warp_bits];
  __shared__ llint sh_ubrd_acc[VALENCE_KERNEL_THREAD_COUNT >> warp_bits];
  __shared__ llint sh_cimp_acc[VALENCE_KERNEL_THREAD_COUNT >> warp_bits];
  __shared__ llint sh_cmap_acc[VALENCE_KERNEL_THREAD_COUNT >> warp_bits];
  __shared__ llint sh_qq14_acc[VALENCE_KERNEL_THREAD_COUNT >> warp_bits];
  __shared__ llint sh_lj14_acc[VALENCE_KERNEL_THREAD_COUNT >> warp_bits];
  __shared__ llint sh_rstr_acc[VALENCE_KERNEL_THREAD_COUNT >> warp_bits];
#endif
  __shared__ int2 vwu_map[vwu_abstract_length];
  __shared__ int vwu_task_count[vwu_abstract_length];
  __shared__ int vwu_padded_task_count[vwu_abstract_length];
  __shared__ int vwu_idx;
  
  // Each block takes its first valence work unit based on its block index.
  if (threadIdx.x == 0) {
    vwu_idx = blockIdx.x;
  }
  __syncthreads();
  while (vwu_idx < poly_vk.nvwu) {
    
    // The instruction set map is read and stored in __shared__ for convenience, and to ensure
    // that it never leaves cache.  The instructions themselves are "streamed," which for purposes
    // of this documentation means read from global, used once, and not cached.  Each block must be
    // at least vwu_abstract_length + warp_size_int in size.  Currently that is 96 on NVIDIA GOUs
    // and 128 on commodity AMD GPUs.
#ifdef SPLIT_FORCE_ACCUMULATION
    const int noverflow_flags = maximum_valence_work_unit_atoms / warp_size_int;
#endif
    if (threadIdx.x < vwu_abstract_length) {
      vwu_map[threadIdx.x] = __ldcv(&poly_vk.vwu_abstracts[(vwu_idx * vwu_abstract_length) +
                                                           threadIdx.x]);
      vwu_task_count[threadIdx.x] = vwu_map[threadIdx.x].y - vwu_map[threadIdx.x].x;
      vwu_padded_task_count[threadIdx.x] = devcRoundUp(vwu_task_count[threadIdx.x], warp_size_int);
    }
#ifdef SPLIT_FORCE_ACCUMULATION
    else if (threadIdx.x < vwu_abstract_length + noverflow_flags) {
      xoverflow_active[threadIdx.x - vwu_abstract_length] = 0;
    }
    else if (threadIdx.x < vwu_abstract_length + (2 * noverflow_flags)) {
      yoverflow_active[threadIdx.x - vwu_abstract_length - noverflow_flags] = 0;
    }
    else if (threadIdx.x < vwu_abstract_length + (3 * noverflow_flags)) {
      zoverflow_active[threadIdx.x - vwu_abstract_length - (2 * noverflow_flags)] = 0;
    }
#endif
    __syncthreads();
    
    // Import atomic coordinates, properties, and (if appropriate) velocities.  This employs all
    // threads of the block, breaking up each set of information at the warp level.
    const int import_llim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].x;
    const int import_hlim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].y;
    const int import_count  = import_hlim - import_llim;
    const int import_stride = devcRoundUp(import_hlim - import_llim, warp_size_int);
    int pos = threadIdx.x;
    while (pos < import_stride) {
      if (pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + pos;
        __stwb(&gmem_r.xcrd[write_idx], __ldcs(&poly_psw.xcrd[read_idx]));
      }
      pos += blockDim.x;
    }
    while (pos < 2 * import_stride) {
      const int rel_pos = pos - import_stride;
      if (rel_pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.ycrd[write_idx], __ldcs(&poly_psw.ycrd[read_idx]));
      }
      pos += blockDim.x;
    }
    while (pos < 3 * import_stride) {
      const int rel_pos = pos - (2 * import_stride);
      if (rel_pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.zcrd[write_idx], __ldcs(&poly_psw.zcrd[read_idx]));
      }
      pos += blockDim.x;
    }
    while (pos < 4 * import_stride) {
      const int rel_pos = pos - (3 * import_stride);
      if (rel_pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.charges[write_idx], __ldcs(&poly_vk.charges[read_idx]));
      }
      pos += blockDim.x;
    }
    while (pos < 5 * import_stride) {
      const int rel_pos = pos - (4 * import_stride);
      if (rel_pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.lj_idx[write_idx], __ldcs(&poly_vk.lj_idx[read_idx]));
      }
      pos += blockDim.x;
    }
#ifdef UPDATE_ATOMS
    // Import velocities into the local, block-exclusive L1-resident arrays
    while (pos < 6 * import_stride) {
      const int rel_pos = pos - (5 * import_stride);
      if (rel_pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.xvel[write_idx], __ldcs(&poly_psw.xvel[read_idx]));
      }
      pos += blockDim.x;
    }
    while (pos < 7 * import_stride) {
      const int rel_pos = pos - (6 * import_stride);
      if (rel_pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.yvel[write_idx], __ldcs(&poly_psw.yvel[read_idx]));
      }
      pos += blockDim.x;
    }
    while (pos < 8 * import_stride) {
      const int rel_pos = pos - (7 * import_stride);
      if (rel_pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.zvel[write_idx], __ldcs(&poly_psw.zvel[read_idx]));
      }
      pos += blockDim.x;
    }
#else
    // Advance the position counter to keep pace with the following loop limits
    pos += 3 * import_stride;
#endif
#ifdef COMPUTE_FORCE
    // Initialize the force accumulators, including the GMEM-based, non-shared force accumulators
    // that will have their atomic operations occur in L2.
    while (pos < 9 * import_stride) {
      const int rel_pos = pos - (8 * import_stride);
      if (rel_pos < import_count) {
#  ifdef SPLIT_FORCE_ACCUMULATION
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.xfrc_overflow[write_idx], 0);
        sh_xfrc[rel_pos] = 0;
#  else        
        sh_xfrc[rel_pos] = 0LL;
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 10 * import_stride) {
      const int rel_pos = pos - (9 * import_stride);
      if (rel_pos < import_count) {
#  ifdef SPLIT_FORCE_ACCUMULATION
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.yfrc_overflow[write_idx], 0);
        sh_yfrc[rel_pos] = 0;
#  else
        sh_yfrc[rel_pos] = 0LL;
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 11 * import_stride) {
      const int rel_pos = pos - (10 * import_stride);
      if (rel_pos < import_count) {
#  ifdef SPLIT_FORCE_ACCUMULATION
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.zfrc_overflow[write_idx], 0);
        sh_zfrc[rel_pos] = 0;
#  else
        sh_zfrc[rel_pos] = 0LL;
#  endif
      }
      pos += blockDim.x;
    }
#endif
    __syncthreads();

    // Perform each force-related task in the valence work unit, starting with the most
    // expensive operations and backfilling with less expensive ones.
    pos = threadIdx.x;
    int vterm_limit = vwu_padded_task_count[(size_t)(VwuAbstractMap::CMAP)];
#ifdef COMPUTE_ENERGY
    llint cmap_acc = 0LL;
#endif
    while (pos < vterm_limit) {
      if (pos < vwu_task_count[(size_t)(VwuAbstractMap::CMAP)]) {

        // Obtain the instruction
        const int task_offset = vwu_map[(size_t)(VwuAbstractMap::CMAP)].x;
        const uint2 tinsr = __ldcs(&poly_vk.cmap_insr[task_offset + pos]);
        int i_atom = (tinsr.x & 0x3ff) + EXCL_GMEM_OFFSET;
        int j_atom = ((tinsr.x >> 10) & 0x3ff) + EXCL_GMEM_OFFSET;
        int k_atom = ((tinsr.x >> 20) & 0x3ff) + EXCL_GMEM_OFFSET;
        int l_atom = (tinsr.y & 0x3ff) + EXCL_GMEM_OFFSET;
        int m_atom = ((tinsr.y >> 10) & 0x3ff) + EXCL_GMEM_OFFSET;
        const int surf_idx = ((tinsr.y >> 20) & 0xfff);

        // Compute displacements
        const llint ixloc = __ldca(&gmem_r.xcrd[i_atom]);
        const llint iyloc = __ldca(&gmem_r.ycrd[i_atom]);
        const llint izloc = __ldca(&gmem_r.zcrd[i_atom]);
        const llint jxloc = __ldca(&gmem_r.xcrd[j_atom]);
        const llint jyloc = __ldca(&gmem_r.ycrd[j_atom]);
        const llint jzloc = __ldca(&gmem_r.zcrd[j_atom]);
        const TCALC3 ab = { (TCALC)(jxloc - ixloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(jyloc - iyloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(jzloc - izloc) * poly_psw.inv_gpos_scale_f };
        const llint kxloc = __ldca(&gmem_r.xcrd[k_atom]);
        const llint kyloc = __ldca(&gmem_r.ycrd[k_atom]);
        const llint kzloc = __ldca(&gmem_r.zcrd[k_atom]);
        const TCALC3 bc = { (TCALC)(kxloc - jxloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(kyloc - jyloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(kzloc - jzloc) * poly_psw.inv_gpos_scale_f };
        const llint lxloc = __ldca(&gmem_r.xcrd[l_atom]);
        const llint lyloc = __ldca(&gmem_r.ycrd[l_atom]);
        const llint lzloc = __ldca(&gmem_r.zcrd[l_atom]);
        const TCALC3 cd = { (TCALC)(lxloc - kxloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(lyloc - kyloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(lzloc - kzloc) * poly_psw.inv_gpos_scale_f };
        const llint mxloc = __ldca(&gmem_r.xcrd[m_atom]);
        const llint myloc = __ldca(&gmem_r.ycrd[m_atom]);
        const llint mzloc = __ldca(&gmem_r.zcrd[m_atom]);
        const TCALC3 de = { (TCALC)(mxloc - lxloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(myloc - lyloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(mzloc - lzloc) * poly_psw.inv_gpos_scale_f };

        // Compute the first dihedral angle
        TCALC3 crabbc = crossProduct(ab, bc);
        TCALC3 crbccd = crossProduct(bc, cd);
        TCALC cos_phi = (crabbc.x * crbccd.x) + (crabbc.y * crbccd.y) + (crabbc.z * crbccd.z);
        cos_phi /= SQRT_FUNC(((crabbc.x * crabbc.x) + (crabbc.y * crabbc.y) +
                              (crabbc.z * crabbc.z)) *
                             ((crbccd.x * crbccd.x) + (crbccd.y * crbccd.y) +
                              (crbccd.z * crbccd.z)));
        TCALC3 scr_phi = crossProduct(crabbc, crbccd);
#ifdef CHECK_COSARG
        TCALC phi = angleVerification(cos_phi, crabbc, crbccd, bc, scr_phi);

        // The single-precision floating point representation of PI is not the same as the
        // double-precision floating point representation, so use the fact that single precision
        // calculations use this branch of the code to specify the correct constant for each
        // precision mode.
        phi += pi_f;
        phi = (phi < (TCALC)(0.0)) ? phi + twopi_f : (phi >= twopi_f) ? phi - twopi_f : phi;
#else
        cos_phi = (cos_phi < (TCALC)(-1.0)) ? (TCALC)(-1.0) :
                                              (cos_phi > (TCALC)(1.0)) ? (TCALC)(1.0) : cos_phi;
        TCALC phi = ((scr_phi.x * bc.x) + (scr_phi.y * bc.y) + (scr_phi.z * bc.z) > (TCALC)(0.0)) ?
                    ACOS_FUNC(cos_phi) : -ACOS_FUNC(cos_phi);
        phi += pi;
        phi = (phi < (TCALC)(0.0)) ? phi + twopi : (phi >= twopi) ? phi - twopi : phi;
#endif

        // Compute the second dihedral angle
        TCALC3 crcdde = crossProduct(cd, de);
        TCALC cos_psi = (crbccd.x * crcdde.x) + (crbccd.y * crcdde.y) + (crbccd.z * crcdde.z);
        cos_psi /= SQRT_FUNC(((crbccd.x * crbccd.x) + (crbccd.y * crbccd.y) +
                              (crbccd.z * crbccd.z)) *
                             ((crcdde.x * crcdde.x) + (crcdde.y * crcdde.y) +
                              (crcdde.z * crcdde.z)));
        TCALC3 scr_psi = crossProduct(crbccd, crcdde);
#ifdef CHECK_COSARG
        TCALC psi = angleVerification(cos_psi, crbccd, crcdde, cd, scr_psi);
        psi += pi_f;
        psi = (psi < (TCALC)(0.0)) ? psi + twopi_f : (psi >= twopi_f) ? psi - twopi_f : psi;
#else
        cos_psi = (cos_psi < (TCALC)(-1.0)) ? (TCALC)(-1.0) :
                                              (cos_psi > (TCALC)(1.0)) ? (TCALC)(1.0) : cos_psi;
        TCALC psi = ((scr_psi.x * cd.x) + (scr_psi.y * cd.y) + (scr_psi.z * cd.z) > (TCALC)(0.0)) ?
                    ACOS_FUNC(cos_psi) : -ACOS_FUNC(cos_psi);
        psi += pi;
        psi = (psi < (TCALC)(0.0)) ? psi + twopi : (psi >= twopi) ? psi - twopi : psi;
#endif
        
        // Compute the patch index
        const int int_surf_dim    = __ldca(&poly_vk.cmap_dim[surf_idx]);
        const TCALC real_surf_dim = (TCALC)(int_surf_dim);
        const TCALC phi_grid = phi * real_surf_dim * inverse_twopi_f;
        const TCALC psi_grid = psi * real_surf_dim * inverse_twopi_f;
        const int idx_phi = phi_grid;
        const int idx_psi = psi_grid;
        const TCALC phifrac = phi_grid - (TCALC)(idx_phi);
        const TCALC psifrac = psi_grid - (TCALC)(idx_psi);

        // Draw in the matrix of spline values and derivatives
        const int patch_idx = __ldca(&poly_vk.cmap_patch_bounds[surf_idx]) +
                              (((idx_psi * int_surf_dim) + idx_phi) * 16);
        TCALC acoef[16];
        for (int i = 0; i < 16; i++) {
          acoef[i] = __ldca(&poly_vk.cmap_patches[patch_idx + i]);
        }
#ifdef COMPUTE_ENERGY
        const int acc_elem   = pos / uint_bit_count_int;
        const int acc_bit    = pos - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::CMAP_NRG)].x;
        const bool accflag = ((__ldca(&poly_vk.cmap_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1);
        if (accflag) {

          TCALC phi_progression[4], psi_progression[4], acoef_psi[4];
          phi_progression[0] = (TCALC)(1.0);
          psi_progression[0] = (TCALC)(1.0);
          for (int i = 1; i < 4; i++) {
            phi_progression[i] = phi_progression[i - 1] * phifrac;
            psi_progression[i] = psi_progression[i - 1] * psifrac;
          }
          acoef_psi[0] = (acoef[ 0] * psi_progression[0]) + (acoef[ 4] * psi_progression[1]) +
                         (acoef[ 8] * psi_progression[2]) + (acoef[12] * psi_progression[3]);
          acoef_psi[1] = (acoef[ 1] * psi_progression[0]) + (acoef[ 5] * psi_progression[1]) +
                         (acoef[ 9] * psi_progression[2]) + (acoef[13] * psi_progression[3]);
          acoef_psi[2] = (acoef[ 2] * psi_progression[0]) + (acoef[ 6] * psi_progression[1]) +
                         (acoef[10] * psi_progression[2]) + (acoef[14] * psi_progression[3]);
          acoef_psi[3] = (acoef[ 3] * psi_progression[0]) + (acoef[ 7] * psi_progression[1]) +
                         (acoef[11] * psi_progression[2]) + (acoef[15] * psi_progression[3]);
          const TCALC contrib = (phi_progression[0] * acoef_psi[0]) +
                                (phi_progression[1] * acoef_psi[1]) +
                                (phi_progression[2] * acoef_psi[2]) +
                                (phi_progression[3] * acoef_psi[3]);
          cmap_acc += LLCONV_FUNC(contrib * scw.nrg_scale_f);
        }
#endif
#ifdef COMPUTE_FORCE
#  ifndef UPDATE_ATOMS
#    ifndef COMPUTE_ENERGY
        // If atom updating is not the goal, then it is necessary to test whether the energy of
        // this interaction should be accumulated in order to know whether this block should also
        // be responsible for accumulating the force.  This repeats the evaluation from above, but
        // only on the first stage of a line minimization evaluation will both energies and forces
        // be evaluated.  
        const int acc_elem   = pos / uint_bit_count_int;
        const int acc_bit    = pos - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::CMAP_NRG)].x;
        const bool accflag = ((__ldca(&poly_vk.cmap_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1);
#    endif
        if (accflag) {
#  endif
        // Compute the derivatives of the CMAP function along its grid
        TCALC dphi = ((((TCALC)(3.0) * acoef[15] * phifrac) +
                       ((TCALC)(2.0) * acoef[14])) * phifrac) + acoef[13];
        dphi *= psifrac;
        dphi +=      ((((TCALC)(3.0) * acoef[11] * phifrac) +
                       ((TCALC)(2.0) * acoef[10])) * phifrac) + acoef[ 9];
        dphi *= psifrac;
        dphi +=      ((((TCALC)(3.0) * acoef[ 7] * phifrac) +
                       ((TCALC)(2.0) * acoef[ 6])) * phifrac) + acoef[ 5];
        dphi *= psifrac;
        dphi +=      ((((TCALC)(3.0) * acoef[ 3] * phifrac) +
                       ((TCALC)(2.0) * acoef[ 2])) * phifrac) + acoef[ 1];
        TCALC dpsi = ((((TCALC)(3.0) * acoef[15] * psifrac) +
                       ((TCALC)(2.0) * acoef[11])) * psifrac) + acoef[ 7];
        dpsi *= phifrac;
        dpsi +=      ((((TCALC)(3.0) * acoef[14] * psifrac) +
                       ((TCALC)(2.0) * acoef[10])) * psifrac) + acoef[ 6];
        dpsi *= phifrac;
        dpsi +=      ((((TCALC)(3.0) * acoef[13] * psifrac) +
                       ((TCALC)(2.0) * acoef[ 9])) * psifrac) + acoef[ 5];
        dpsi *= phifrac;
        dpsi +=      ((((TCALC)(3.0) * acoef[12] * psifrac) +
                       ((TCALC)(2.0) * acoef[ 8])) * psifrac) + acoef[ 4];
        dphi *= real_surf_dim / twopi_f;
        dpsi *= real_surf_dim / twopi_f;
        const TCALC mgab = SQRT_FUNC((ab.x * ab.x) + (ab.y * ab.y) + (ab.z * ab.z));
        const TCALC mgbc = SQRT_FUNC((bc.x * bc.x) + (bc.y * bc.y) + (bc.z * bc.z));
        const TCALC mgcd = SQRT_FUNC((cd.x * cd.x) + (cd.y * cd.y) + (cd.z * cd.z));
        const TCALC mgde = SQRT_FUNC((de.x * de.x) + (de.y * de.y) + (de.z * de.z));

        // With the derivative in hand, evaluate the transformation of coordinates for either the
        // phi or psi dihedrals.  As before in the transformation of dihedral forces, scale the
        // cross product vectors by the fixed-precision scaling factor.
        const TCALC invab = (TCALC)(1.0) / mgab;
        const TCALC invbc = (TCALC)(1.0) / mgbc;
        const TCALC invcd = (TCALC)(1.0) / mgcd;
        const TCALC invde = (TCALC)(1.0) / mgde;
        const TCALC invabc = invab * invbc;
        const TCALC invbcd = invbc * invcd;
        const TCALC invcde = invcd * invde;
        crabbc.x *= invabc * poly_psw.frc_scale_f;
        crabbc.y *= invabc * poly_psw.frc_scale_f;
        crabbc.z *= invabc * poly_psw.frc_scale_f;
        crbccd.x *= invbcd * poly_psw.frc_scale_f;
        crbccd.y *= invbcd * poly_psw.frc_scale_f;
        crbccd.z *= invbcd * poly_psw.frc_scale_f;
        crcdde.x *= invcde * poly_psw.frc_scale_f;
        crcdde.y *= invcde * poly_psw.frc_scale_f;
        crcdde.z *= invcde * poly_psw.frc_scale_f;

        // Feed the gradient, negative of the derivative, into the functions below
        dphi *= (TCALC)(-1.0);
        dpsi *= (TCALC)(-1.0);

        // Phi accumulation: transform the rotational derivatives to cartesian coordinates
        const TCALC phi_cosb = -((ab.x * bc.x) + (ab.y * bc.y) + (ab.z * bc.z)) * invab * invbc;
        const TCALC phi_cosc = -((bc.x * cd.x) + (bc.y * cd.y) + (bc.z * cd.z)) * invbc * invcd;
        const TCALC phi_isinb2 = (phi_cosb * phi_cosb < asymptotic_to_one_f) ?
                                 dphi / ((TCALC)(1.0) - (phi_cosb * phi_cosb)) :
                                 dphi * inverse_one_minus_asymptote_f;
        const TCALC phi_isinc2 = (phi_cosc * phi_cosc < asymptotic_to_one_f) ?
                                 dphi / ((TCALC)(1.0) - (phi_cosc * phi_cosc)) :
                                 dphi * inverse_one_minus_asymptote_f;
        const TCALC phi_fa  = -invab * phi_isinb2;
        const TCALC phi_fb1 = (mgbc - (mgab * phi_cosb)) * invabc * phi_isinb2;
        const TCALC phi_fb2 = phi_cosc * invbc * phi_isinc2;
        const TCALC phi_fc1 = (mgbc - (mgcd * phi_cosc)) * invbcd * phi_isinc2;
        const TCALC phi_fc2 = phi_cosb * invbc * phi_isinb2;
        const TCALC phi_fd  = -invcd * phi_isinc2;

        // Psi accumulation: transform the rotational derivatives to cartesian coordinates
        const TCALC psi_cosb = -((bc.x * cd.x) + (bc.y * cd.y) + (bc.z * cd.z)) * invbc * invcd;
        const TCALC psi_cosc = -((cd.x * de.x) + (cd.y * de.y) + (cd.z * de.z)) * invcd * invde;
        const TCALC psi_isinb2 = (psi_cosb * psi_cosb < asymptotic_to_one_f) ?
                                 dpsi / ((TCALC)(1.0) - (psi_cosb * psi_cosb)) :
                                 dpsi * inverse_one_minus_asymptote_f;
        const TCALC psi_isinc2 = (psi_cosc * psi_cosc < asymptotic_to_one_f) ?
                                 dpsi / ((TCALC)(1.0) - (psi_cosc * psi_cosc)) :
                                 dpsi * inverse_one_minus_asymptote_f;
        const TCALC psi_fa  = -invbc * psi_isinb2;
        const TCALC psi_fb1 = (mgcd - (mgbc * psi_cosb)) * invbcd * psi_isinb2;
        const TCALC psi_fb2 = psi_cosc * invcd * psi_isinc2;
        const TCALC psi_fc1 = (mgcd - (mgde * psi_cosc)) * invcde * psi_isinc2;
        const TCALC psi_fc2 = psi_cosb * invcd * psi_isinb2;
        const TCALC psi_fd  = -invde * psi_isinc2;
        
        // Accumulate forces on the five atoms, one dimension at a time
        i_atom -= EXCL_GMEM_OFFSET;
        j_atom -= EXCL_GMEM_OFFSET;
        k_atom -= EXCL_GMEM_OFFSET;
        l_atom -= EXCL_GMEM_OFFSET;
        m_atom -= EXCL_GMEM_OFFSET;
#  ifdef SPLIT_FORCE_ACCUMULATION
        const int2 iphi_ix  = convertSplitFixedPrecision(crabbc.x * phi_fa);
        const int2 iphi_jx  = convertSplitFixedPrecision((phi_fb1 * crabbc.x) -
                                                         (phi_fb2 * crbccd.x));
        const int2 iphi_lx  = convertSplitFixedPrecision(-phi_fd * crbccd.x);
        const int2 iphi_ijx = combineSplitFixedPrecision(iphi_ix, iphi_jx);
        const int2 iphi_kx  = antiCombineSplitFixedPrecision(iphi_ijx, iphi_lx);

        const int2 ipsi_jx  = convertSplitFixedPrecision(crbccd.x * psi_fa);
        const int2 ipsi_kx  = convertSplitFixedPrecision((psi_fb1 * crbccd.x) -
                                                         (psi_fb2 * crcdde.x));
        const int2 ipsi_mx  = convertSplitFixedPrecision(-psi_fd * crcdde.x);
        const int2 ipsi_jkx = combineSplitFixedPrecision(ipsi_jx, ipsi_kx);
        const int2 ipsi_lx  = antiCombineSplitFixedPrecision(ipsi_jkx, ipsi_mx);
        const int2 icmb_jx  = combineSplitFixedPrecision(iphi_jx, ipsi_jx);
        const int2 icmb_kx  = combineSplitFixedPrecision(iphi_kx, ipsi_kx);
        const int2 icmb_lx  = combineSplitFixedPrecision(iphi_lx, ipsi_lx);
        addSplitFixedPrecision(iphi_ix, i_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icmb_jx, j_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icmb_kx, k_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icmb_lx, l_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ipsi_mx, m_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        const int2 iphi_iy  = convertSplitFixedPrecision(crabbc.y * phi_fa);
        const int2 iphi_jy  = convertSplitFixedPrecision((phi_fb1 * crabbc.y) -
                                                         (phi_fb2 * crbccd.y));
        const int2 iphi_ly  = convertSplitFixedPrecision(-phi_fd * crbccd.y);
        const int2 iphi_ijy = combineSplitFixedPrecision(iphi_iy, iphi_jy);
        const int2 iphi_ky  = antiCombineSplitFixedPrecision(iphi_ijy, iphi_ly);

        const int2 ipsi_jy  = convertSplitFixedPrecision(crbccd.y * psi_fa);
        const int2 ipsi_ky  = convertSplitFixedPrecision((psi_fb1 * crbccd.y) -
                                                         (psi_fb2 * crcdde.y));
        const int2 ipsi_my  = convertSplitFixedPrecision(-psi_fd * crcdde.y);
        const int2 ipsi_jky = combineSplitFixedPrecision(ipsi_jy, ipsi_ky);
        const int2 ipsi_ly  = antiCombineSplitFixedPrecision(ipsi_jky, ipsi_my);
        const int2 icmb_jy  = combineSplitFixedPrecision(iphi_jy, ipsi_jy);
        const int2 icmb_ky  = combineSplitFixedPrecision(iphi_ky, ipsi_ky);
        const int2 icmb_ly  = combineSplitFixedPrecision(iphi_ly, ipsi_ly);
        addSplitFixedPrecision(iphi_iy, i_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icmb_jy, j_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icmb_ky, k_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icmb_ly, l_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ipsi_my, m_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        const int2 iphi_iz  = convertSplitFixedPrecision(crabbc.z * phi_fa);
        const int2 iphi_jz  = convertSplitFixedPrecision((phi_fb1 * crabbc.z) -
                                                         (phi_fb2 * crbccd.z));
        const int2 iphi_lz  = convertSplitFixedPrecision(-phi_fd * crbccd.z);
        const int2 iphi_ijz = combineSplitFixedPrecision(iphi_iz, iphi_jz);
        const int2 iphi_kz  = antiCombineSplitFixedPrecision(iphi_ijz, iphi_lz);

        const int2 ipsi_jz  = convertSplitFixedPrecision(crbccd.z * psi_fa);
        const int2 ipsi_kz  = convertSplitFixedPrecision((psi_fb1 * crbccd.z) -
                                                         (psi_fb2 * crcdde.z));
        const int2 ipsi_mz  = convertSplitFixedPrecision(-psi_fd * crcdde.z);
        const int2 ipsi_jkz = combineSplitFixedPrecision(ipsi_jz, ipsi_kz);
        const int2 ipsi_lz  = antiCombineSplitFixedPrecision(ipsi_jkz, ipsi_mz);
        const int2 icmb_jz  = combineSplitFixedPrecision(iphi_jz, ipsi_jz);
        const int2 icmb_kz  = combineSplitFixedPrecision(iphi_kz, ipsi_kz);
        const int2 icmb_lz  = combineSplitFixedPrecision(iphi_lz, ipsi_lz);
        addSplitFixedPrecision(iphi_iz, i_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icmb_jz, j_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icmb_kz, k_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icmb_lz, l_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ipsi_mz, m_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
#  else // SPLIT_FORCE_ACCUMULATION
        const llint iphi_ix = LLCONV_FUNC(crabbc.x * phi_fa);
        const llint iphi_jx = LLCONV_FUNC((phi_fb1 * crabbc.x) - (phi_fb2 * crbccd.x));
        const llint iphi_lx = LLCONV_FUNC(-phi_fd * crbccd.x);
        const llint ipsi_jx = LLCONV_FUNC(crbccd.x * psi_fa);
        const llint ipsi_kx = LLCONV_FUNC((psi_fb1 * crbccd.x) - (psi_fb2 * crcdde.x));
        const llint ipsi_mx = LLCONV_FUNC(-psi_fd * crcdde.x);
        atomicAdd((ullint*)&sh_xfrc[i_atom], lliToUlli(iphi_ix));
        atomicAdd((ullint*)&sh_xfrc[j_atom], lliToUlli(iphi_jx + ipsi_jx));
        atomicAdd((ullint*)&sh_xfrc[k_atom], lliToUlli(ipsi_kx - iphi_ix - iphi_jx - iphi_lx));
        atomicAdd((ullint*)&sh_xfrc[l_atom], lliToUlli(iphi_lx - ipsi_jx - ipsi_kx - ipsi_mx));
        atomicAdd((ullint*)&sh_xfrc[m_atom], lliToUlli(ipsi_mx));
        const llint iphi_iy = LLCONV_FUNC(crabbc.y * phi_fa);
        const llint iphi_jy = LLCONV_FUNC((phi_fb1 * crabbc.y) - (phi_fb2 * crbccd.y));
        const llint iphi_ly = LLCONV_FUNC(-phi_fd * crbccd.y);
        const llint ipsi_jy = LLCONV_FUNC(crbccd.y * psi_fa);
        const llint ipsi_ky = LLCONV_FUNC((psi_fb1 * crbccd.y) - (psi_fb2 * crcdde.y));
        const llint ipsi_my = LLCONV_FUNC(-psi_fd * crcdde.y);
        atomicAdd((ullint*)&sh_yfrc[i_atom], lliToUlli(iphi_iy));
        atomicAdd((ullint*)&sh_yfrc[j_atom], lliToUlli(iphi_jy + ipsi_jy));
        atomicAdd((ullint*)&sh_yfrc[k_atom], lliToUlli(ipsi_ky - iphi_iy - iphi_jy - iphi_ly));
        atomicAdd((ullint*)&sh_yfrc[l_atom], lliToUlli(iphi_ly - ipsi_jy - ipsi_ky - ipsi_my));
        atomicAdd((ullint*)&sh_yfrc[m_atom], lliToUlli(ipsi_my));
        const llint iphi_iz = LLCONV_FUNC(crabbc.z * phi_fa);
        const llint iphi_jz = LLCONV_FUNC((phi_fb1 * crabbc.z) - (phi_fb2 * crbccd.z));
        const llint iphi_lz = LLCONV_FUNC(-phi_fd * crbccd.z);
        const llint ipsi_jz = LLCONV_FUNC(crbccd.z * psi_fa);
        const llint ipsi_kz = LLCONV_FUNC((psi_fb1 * crbccd.z) - (psi_fb2 * crcdde.z));
        const llint ipsi_mz = LLCONV_FUNC(-psi_fd * crcdde.z);
        atomicAdd((ullint*)&sh_zfrc[i_atom], lliToUlli(iphi_iz));
        atomicAdd((ullint*)&sh_zfrc[j_atom], lliToUlli(iphi_jz + ipsi_jz));
        atomicAdd((ullint*)&sh_zfrc[k_atom], lliToUlli(ipsi_kz - iphi_iz - iphi_jz - iphi_lz));
        atomicAdd((ullint*)&sh_zfrc[l_atom], lliToUlli(iphi_lz - ipsi_jz - ipsi_kz - ipsi_mz));
        atomicAdd((ullint*)&sh_zfrc[m_atom], lliToUlli(ipsi_mz));
#  endif
#  ifndef UPDATE_ATOMS
        }
#  endif
#endif
      }
      pos += blockDim.x;
    }
#ifdef COMPUTE_ENERGY
    // Stash the threads' accumulated CMAP energy values
    WARP_REDUCE_DOWN(cmap_acc)
    if ((threadIdx.x & warp_bits_mask_int) == 0) {
      sh_cmap_acc[(threadIdx.x >> warp_bits)] = cmap_acc;
    }
#endif
    int vterm_offset = vterm_limit;
    vterm_limit = vterm_offset + vwu_padded_task_count[(size_t)(VwuAbstractMap::CDHE)];
#ifdef COMPUTE_ENERGY
    llint dihe_acc = 0LL;
    llint impr_acc = 0LL;
    llint cimp_acc = 0LL;
    llint qq14_acc = 0LL;
    llint lj14_acc = 0LL;
#endif
    while (pos < vterm_limit) {
      if (pos - vterm_offset < vwu_task_count[(size_t)(VwuAbstractMap::CDHE)]) {
        const int insr_idx = vwu_map[(size_t)(VwuAbstractMap::CDHE)].x + pos - vterm_offset;
        const uint2 tinsr = __ldcs(&poly_vk.cdhe_insr[insr_idx]);
        int i_atom = (tinsr.x & 0x3ff) + EXCL_GMEM_OFFSET;
        int j_atom = ((tinsr.x >> 10) & 0x3ff) + EXCL_GMEM_OFFSET;
        int k_atom = ((tinsr.x >> 20) & 0x3ff) + EXCL_GMEM_OFFSET;
        int l_atom = (tinsr.y & 0x3ff) + EXCL_GMEM_OFFSET;
        const int param_idx = ((tinsr.y >> 16) & 0xffff);
        const llint ixloc = __ldca(&gmem_r.xcrd[i_atom]);
        const llint iyloc = __ldca(&gmem_r.ycrd[i_atom]);
        const llint izloc = __ldca(&gmem_r.zcrd[i_atom]);
        const llint jxloc = __ldca(&gmem_r.xcrd[j_atom]);
        const llint jyloc = __ldca(&gmem_r.ycrd[j_atom]);
        const llint jzloc = __ldca(&gmem_r.zcrd[j_atom]);
        const TCALC3 ab = { (TCALC)(jxloc - ixloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(jyloc - iyloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(jzloc - izloc) * poly_psw.inv_gpos_scale_f };
        const llint kxloc = __ldca(&gmem_r.xcrd[k_atom]);
        const llint kyloc = __ldca(&gmem_r.ycrd[k_atom]);
        const llint kzloc = __ldca(&gmem_r.zcrd[k_atom]);
        const TCALC3 bc = { (TCALC)(kxloc - jxloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(kyloc - jyloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(kzloc - jzloc) * poly_psw.inv_gpos_scale_f };
        const llint lxloc = __ldca(&gmem_r.xcrd[l_atom]);
        const llint lyloc = __ldca(&gmem_r.ycrd[l_atom]);
        const llint lzloc = __ldca(&gmem_r.zcrd[l_atom]);
        const TCALC3 cd = { (TCALC)(lxloc - kxloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(lyloc - kyloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(lzloc - kzloc) * poly_psw.inv_gpos_scale_f };
        const TCALC3 ad = { (TCALC)(lxloc - ixloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(lyloc - iyloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(lzloc - izloc) * poly_psw.inv_gpos_scale_f };
        TCALC3 crabbc = crossProduct(ab, bc);
        TCALC3 crbccd = crossProduct(bc, cd);
        const TCALC3 scr = crossProduct(crabbc, crbccd);
        TCALC costheta = (crabbc.x * crbccd.x) + (crabbc.y * crbccd.y) + (crabbc.z * crbccd.z);
        costheta /= SQRT_FUNC(((crabbc.x * crabbc.x) + (crabbc.y * crabbc.y) +
                               (crabbc.z * crabbc.z)) *
                              ((crbccd.x * crbccd.x) + (crbccd.y * crbccd.y) +
                               (crbccd.z * crbccd.z)));
#ifdef CHECK_COSARG
        const TCALC theta = angleVerification(costheta, crabbc, crbccd, bc, scr);
#else
        costheta = (costheta < (TCALC)(-1.0)) ?
                   (TCALC)(-1.0) : (costheta > (TCALC)(1.0)) ? (TCALC)(1.0) : costheta;
        const TCALC theta = ((scr.x * bc.x) + (scr.y * bc.y) + (scr.z * bc.z) > (TCALC)(0.0)) ?
                            ACOS_FUNC(costheta) : -ACOS_FUNC(costheta);
#endif
        TCALC sangle, sangle_ii, stiffness;
#ifdef COMPUTE_FORCE
        TCALC stiffness_ii;
        TCALC f14 = (TCALC)(0.0);
#endif
        // Evaluate the accumulation bit once for all aspects of ths composite interaction
#if defined(COMPUTE_ENERGY) || !defined(UPDATE_ATOMS)
        const int acc_elem    = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit     = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset  = vwu_map[(size_t)(VwuAbstractMap::CDHE_NRG)].x;
        const bool accflag = ((__ldca(&poly_vk.cdhe_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1);
#endif
        // Evaluate each aspect of the interaction
        if ((tinsr.x >> 30) & 0x1) {
          stiffness = __ldca(&poly_vk.cimp_keq[param_idx]);
          sangle = theta - __ldca(&poly_vk.cimp_phi[param_idx]);
#ifdef COMPUTE_ENERGY
          if (accflag) {
            const TCALC contrib = stiffness * sangle * sangle;
            cimp_acc += LLCONV_FUNC(contrib * scw.nrg_scale_f);
          }
#endif
        }
        else {
          TCALC ampl = (TCALC)(0.0);
          TCALC freq = (TCALC)(0.0);
          TCALC phi  = (TCALC)(0.0);
          if (param_idx < 65535) {
            ampl = __ldca(&poly_vk.dihe_amp[param_idx]);
            freq = __ldca(&poly_vk.dihe_freq[param_idx]);
            phi  = __ldca(&poly_vk.dihe_phi[param_idx]);
          }
          stiffness = ampl * freq;
          sangle = (freq * theta) - phi;

          // Determine whether this cosine-based dihedral has a 1:4 term to apply
          int attn14_idx = ((tinsr.y >> 10) & 0x1f);
#ifdef COMPUTE_ENERGY
          TCALC contrib = ampl * ((TCALC)(1.0) + COS_FUNC(sangle));
#endif
          // Superimpose a second dihedral onto the same four atoms.  Check for a new 1:4 screening
          // parameter set index if one has not yet been found.
          if ((tinsr.y >> 15) & 0x1) {
            const uint tinsr_z = __ldcs(&poly_vk.cdhe_ovrt_insr[insr_idx]);
            const int param_ii_idx = (tinsr_z & 0xfffff);
            const TCALC ampl_ii = __ldca(&poly_vk.dihe_amp[param_ii_idx]);
            const TCALC freq_ii = __ldca(&poly_vk.dihe_freq[param_ii_idx]);
            const TCALC phi_ii  = __ldca(&poly_vk.dihe_phi[param_ii_idx]);
            if (attn14_idx == 0) {
              attn14_idx = ((tinsr_z >> 20) & 0xfff);
            }
#ifdef COMPUTE_FORCE        
            stiffness_ii = ampl_ii * freq_ii;
#endif
            sangle_ii = (freq_ii * theta) - phi_ii;
#ifdef COMPUTE_ENERGY
            contrib += ampl_ii * ((TCALC)(1.0) + COS_FUNC(sangle_ii));
#endif
          }
#ifdef COMPUTE_ENERGY
          if (accflag) {
            if ((tinsr.x >> 31) & 0x1) {
              impr_acc += LLCONV_FUNC(contrib * scw.nrg_scale_f);
            }
            else {
              dihe_acc += LLCONV_FUNC(contrib * scw.nrg_scale_f);
            }
          }
#endif
          // If there is a 1:4 interaction, compute its force and energy now
          if (attn14_idx > 0) {
            const TCALC attn_qq14 = __ldca(&poly_vk.attn14_elec[attn14_idx]);
            const TCALC attn_lj14 = __ldca(&poly_vk.attn14_vdw[attn14_idx]);
            const int i_ljidx = __ldca(&gmem_r.lj_idx[i_atom]);
            const int l_ljidx = __ldca(&gmem_r.lj_idx[l_atom]);
            const int sysid = vwu_map[(size_t)(VwuAbstractMap::SYSTEM_ID)].x;
            const int il_ljidx = __ldca(&poly_vk.ljabc_offsets[sysid]) +
                                 (__ldca(&poly_vk.n_lj_types[sysid]) * l_ljidx) + i_ljidx;
            const TCALC qq_il = __ldca(&gmem_r.charges[i_atom]) * __ldca(&gmem_r.charges[l_atom]) *
                                poly_vk.coulomb / attn_qq14;
            const TCALC lja_il = __ldca(&poly_vk.lja_14_coeff[il_ljidx]);
            const TCALC ljb_il = __ldca(&poly_vk.ljb_14_coeff[il_ljidx]);
            const TCALC invr_il = (TCALC)(1.0) /
                                  SQRT_FUNC((ad.x * ad.x) + (ad.y * ad.y) + (ad.z * ad.z));
            const TCALC invr2_il = invr_il * invr_il;
            const TCALC invr4_il = invr2_il * invr2_il;
#ifdef COMPUTE_ENERGY
            if (accflag) {
              const TCALC qq14_contrib = qq_il * invr_il;
              qq14_acc += LLCONV_FUNC(qq14_contrib * scw.nrg_scale_f);
              const TCALC lj14_contrib = ((lja_il * invr4_il * invr4_il) - (ljb_il * invr2_il)) *
                                         invr4_il / attn_lj14;
              lj14_acc += LLCONV_FUNC(lj14_contrib * scw.nrg_scale_f);
            }
#endif
#ifdef COMPUTE_FORCE
#  ifndef UPDATE_ATOMS
            if (accflag) {
#  endif
            f14 = -(qq_il * invr2_il * invr_il) +
                  ((((TCALC)(6.0) * ljb_il) - ((TCALC)(12.0) * lja_il * invr4_il * invr2_il)) *
                   invr4_il * invr4_il / attn_lj14);
            f14 *= poly_psw.frc_scale_f;
#  ifndef UPDATE_ATOMS
            }
#  endif
#endif
          }
        }
#ifdef COMPUTE_FORCE        
#  ifndef UPDATE_ATOMS
        if (accflag) {
#  endif
        TCALC fr;
        if ((tinsr.x >> 30) & 0x1) {

          // Evaluate a CHARMM harmonic improper dihedral.
          fr = (TCALC)(-2.0) * stiffness * sangle;
        }
        else {

          // Evaluate a cosine-based dihedral.
          fr = stiffness * SIN_FUNC(sangle);
          if ((tinsr.y >> 15) & 0x1) {
            fr += stiffness_ii * SIN_FUNC(sangle_ii);
          }
        }
        const TCALC mgab = SQRT_FUNC((ab.x * ab.x) + (ab.y * ab.y) + (ab.z * ab.z));
        const TCALC invab = (TCALC)(1.0) / mgab;
        const TCALC mgbc = SQRT_FUNC((bc.x * bc.x) + (bc.y * bc.y) + (bc.z * bc.z));
        const TCALC invbc = (TCALC)(1.0) / mgbc;
        const TCALC mgcd = SQRT_FUNC((cd.x * cd.x) + (cd.y * cd.y) + (cd.z * cd.z));
        const TCALC invcd = (TCALC)(1.0) / mgcd;
        const TCALC cosb = -((ab.x * bc.x) + (ab.y * bc.y) + (ab.z * bc.z)) * invab * invbc;
        const TCALC isinb2 = (cosb * cosb < asymptotic_to_one_lf) ?
                             fr / ((TCALC)(1.0) - (cosb * cosb)) :
                             fr * inverse_one_minus_asymptote_lf;
        const TCALC cosc = -((bc.x * cd.x) + (bc.y * cd.y) + (bc.z * cd.z)) * invbc * invcd;
        const TCALC isinc2 = (cosc * cosc < asymptotic_to_one_lf) ?
                             fr / ((TCALC)(1.0) - (cosc * cosc)) :
                             fr * inverse_one_minus_asymptote_lf;
        const TCALC invabc = invab * invbc;
        const TCALC invbcd = invbc * invcd;
        crabbc.x *= invabc * poly_psw.frc_scale_f;
        crabbc.y *= invabc * poly_psw.frc_scale_f;
        crabbc.z *= invabc * poly_psw.frc_scale_f;
        crbccd.x *= invbcd * poly_psw.frc_scale_f;
        crbccd.y *= invbcd * poly_psw.frc_scale_f;
        crbccd.z *= invbcd * poly_psw.frc_scale_f;
        const TCALC fa = -invab * isinb2;
        const TCALC fb1 = (mgbc - (mgab * cosb)) * invabc * isinb2;
        const TCALC fb2 = cosc * invbc * isinc2;
        const TCALC fc1 = (mgbc - (mgcd * cosc)) * invbcd * isinc2;
        const TCALC fc2 = cosb * invbc * isinb2;
        const TCALC fd = -invcd * isinc2;
        i_atom -= EXCL_GMEM_OFFSET;
        j_atom -= EXCL_GMEM_OFFSET;
        k_atom -= EXCL_GMEM_OFFSET;
        l_atom -= EXCL_GMEM_OFFSET;        
#  ifdef SPLIT_FORCE_ACCUMULATION
        const int2 ifrc_ix = convertSplitFixedPrecision((crabbc.x * fa) + (f14 * ad.x));
        const int2 ifrc_jx = convertSplitFixedPrecision((fb1 * crabbc.x) - (fb2 * crbccd.x));
        const int2 ifrc_lx = convertSplitFixedPrecision((-fd * crbccd.x) - (f14 * ad.x));
        addSplitFixedPrecision(ifrc_ix, i_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ifrc_jx, j_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ifrc_lx, l_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        const int2 ifrc_ijx = combineSplitFixedPrecision(ifrc_ix, ifrc_jx);
        const int2 ifrc_kx = antiCombineSplitFixedPrecision(ifrc_ijx, ifrc_lx);
        addSplitFixedPrecision(ifrc_kx, k_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        const int2 ifrc_iy = convertSplitFixedPrecision((crabbc.y * fa) + (f14 * ad.y));
        const int2 ifrc_jy = convertSplitFixedPrecision((fb1 * crabbc.y) - (fb2 * crbccd.y));
        const int2 ifrc_ly = convertSplitFixedPrecision((-fd * crbccd.y) - (f14 * ad.y));
        addSplitFixedPrecision(ifrc_iy, i_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ifrc_jy, j_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ifrc_ly, l_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        const int2 ifrc_ijy = combineSplitFixedPrecision(ifrc_iy, ifrc_jy);
        const int2 ifrc_ky = antiCombineSplitFixedPrecision(ifrc_ijy, ifrc_ly);
        addSplitFixedPrecision(ifrc_ky, k_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        const int2 ifrc_iz = convertSplitFixedPrecision((crabbc.z * fa) + (f14 * ad.z));
        const int2 ifrc_jz = convertSplitFixedPrecision((fb1 * crabbc.z) - (fb2 * crbccd.z));
        const int2 ifrc_lz = convertSplitFixedPrecision((-fd * crbccd.z) - (f14 * ad.z));
        addSplitFixedPrecision(ifrc_iz, i_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ifrc_jz, j_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ifrc_lz, l_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
        const int2 ifrc_ijz = combineSplitFixedPrecision(ifrc_iz, ifrc_jz);
        const int2 ifrc_kz = antiCombineSplitFixedPrecision(ifrc_ijz, ifrc_lz);
        addSplitFixedPrecision(ifrc_kz, k_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
#  else // SPLIT_FORCE_ACCUMULATION
        const llint ifrc_ix = LLCONV_FUNC((crabbc.x * fa) + (f14 * ad.x));
        const llint ifrc_jx = LLCONV_FUNC((fb1 * crabbc.x) - (fb2 * crbccd.x));
        const llint ifrc_lx = LLCONV_FUNC((-fd * crbccd.x) - (f14 * ad.x));
        atomicAdd((ullint*)&sh_xfrc[i_atom], lliToUlli(ifrc_ix));
        atomicAdd((ullint*)&sh_xfrc[j_atom], lliToUlli(ifrc_jx));
        atomicAdd((ullint*)&sh_xfrc[k_atom], lliToUlli(-(ifrc_ix + ifrc_jx + ifrc_lx)));
        atomicAdd((ullint*)&sh_xfrc[l_atom], lliToUlli(ifrc_lx));
        const llint ifrc_iy = LLCONV_FUNC((crabbc.y * fa) + (f14 * ad.y));
        const llint ifrc_jy = LLCONV_FUNC((fb1 * crabbc.y) - (fb2 * crbccd.y));
        const llint ifrc_ly = LLCONV_FUNC((-fd * crbccd.y) - (f14 * ad.y));
        atomicAdd((ullint*)&sh_yfrc[i_atom], lliToUlli(ifrc_iy));
        atomicAdd((ullint*)&sh_yfrc[j_atom], lliToUlli(ifrc_jy));
        atomicAdd((ullint*)&sh_yfrc[k_atom], lliToUlli(-(ifrc_iy + ifrc_jy + ifrc_ly)));
        atomicAdd((ullint*)&sh_yfrc[l_atom], lliToUlli(ifrc_ly));
        const llint ifrc_iz = LLCONV_FUNC((crabbc.z * fa) + (f14 * ad.z));
        const llint ifrc_jz = LLCONV_FUNC((fb1 * crabbc.z) - (fb2 * crbccd.z));
        const llint ifrc_lz = LLCONV_FUNC((-fd * crbccd.z) - (f14 * ad.z));
        atomicAdd((ullint*)&sh_zfrc[i_atom], lliToUlli(ifrc_iz));
        atomicAdd((ullint*)&sh_zfrc[j_atom], lliToUlli(ifrc_jz));
        atomicAdd((ullint*)&sh_zfrc[k_atom], lliToUlli(-(ifrc_iz + ifrc_jz + ifrc_lz)));
        atomicAdd((ullint*)&sh_zfrc[l_atom], lliToUlli(ifrc_lz));
#  endif
#  ifndef UPDATE_ATOMS
        }
#  endif
#endif
      }
      pos += blockDim.x;
    }
#ifdef COMPUTE_ENERGY
    // Stash the threads' accumulated dihedral and 1:4 energy values
    WARP_REDUCE_DOWN(dihe_acc)
    WARP_REDUCE_DOWN(impr_acc)
    WARP_REDUCE_DOWN(cimp_acc)
    WARP_REDUCE_DOWN(qq14_acc)
    WARP_REDUCE_DOWN(lj14_acc)
    if ((threadIdx.x & warp_bits_mask_int) == 0) {
      sh_dihe_acc[(threadIdx.x >> warp_bits)] = dihe_acc;
      sh_impr_acc[(threadIdx.x >> warp_bits)] = impr_acc;
      sh_cimp_acc[(threadIdx.x >> warp_bits)] = cimp_acc;
      sh_qq14_acc[(threadIdx.x >> warp_bits)] = qq14_acc;
      sh_lj14_acc[(threadIdx.x >> warp_bits)] = lj14_acc;
    }
#endif
    vterm_offset = vterm_limit;
    vterm_limit = vterm_offset + vwu_padded_task_count[(size_t)(VwuAbstractMap::ANGL)];
#ifdef COMPUTE_ENERGY
    llint angl_acc = 0LL;
#endif
    while (pos < vterm_limit) {
      if (pos - vterm_offset < vwu_task_count[(size_t)(VwuAbstractMap::ANGL)]) {
        const int task_offset = vwu_map[(size_t)(VwuAbstractMap::ANGL)].x;
        const uint2 tinsr = __ldcs(&poly_vk.angl_insr[task_offset + pos - vterm_offset]);
        int i_atom = (tinsr.x & 0x3ff) + EXCL_GMEM_OFFSET;
        int j_atom = ((tinsr.x >> 10) & 0x3ff) + EXCL_GMEM_OFFSET;
        int k_atom = ((tinsr.x >> 20) & 0x3ff) + EXCL_GMEM_OFFSET;
        const int param_idx = tinsr.y;
        const TCALC keq      = __ldca(&poly_vk.angl_keq[param_idx]);
        const TCALC theta_eq = __ldca(&poly_vk.angl_theta[param_idx]);
        const llint ixloc = __ldca(&gmem_r.xcrd[i_atom]);
        const llint jxloc = __ldca(&gmem_r.xcrd[j_atom]);
        const llint kxloc = __ldca(&gmem_r.xcrd[k_atom]);
        const TCALC ba_x = (TCALC)(ixloc - jxloc) * poly_psw.inv_gpos_scale_f;
        const TCALC bc_x = (TCALC)(kxloc - jxloc) * poly_psw.inv_gpos_scale_f;
        const llint iyloc = __ldca(&gmem_r.ycrd[i_atom]);
        const llint jyloc = __ldca(&gmem_r.ycrd[j_atom]);
        const llint kyloc = __ldca(&gmem_r.ycrd[k_atom]);
        const TCALC ba_y = (TCALC)(iyloc - jyloc) * poly_psw.inv_gpos_scale_f;
        const TCALC bc_y = (TCALC)(kyloc - jyloc) * poly_psw.inv_gpos_scale_f;
        const llint izloc = __ldca(&gmem_r.zcrd[i_atom]);
        const llint jzloc = __ldca(&gmem_r.zcrd[j_atom]);
        const llint kzloc = __ldca(&gmem_r.zcrd[k_atom]);
        const TCALC ba_z = (TCALC)(izloc - jzloc) * poly_psw.inv_gpos_scale_f;
        const TCALC bc_z = (TCALC)(kzloc - jzloc) * poly_psw.inv_gpos_scale_f;
        const TCALC mgba = (ba_x * ba_x) + (ba_y * ba_y) + (ba_z * ba_z);
        const TCALC mgbc = (bc_x * bc_x) + (bc_y * bc_y) + (bc_z * bc_z);
        const TCALC invbabc = (TCALC)(1.0) / SQRT_FUNC(mgba * mgbc);
        TCALC costheta = ((ba_x * bc_x) + (ba_y * bc_y) + (ba_z * bc_z)) * invbabc;
        costheta = (costheta < (TCALC)(-1.0)) ?
                   (TCALC)(-1.0) : (costheta > (TCALC)(1.0)) ? (TCALC)(1.0) : costheta;
        const TCALC theta = ACOS_FUNC(costheta);
        const TCALC dtheta = theta - theta_eq;
#ifdef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::ANGL_NRG)].x;
        if ((__ldca(&poly_vk.angl_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
          angl_acc += LLCONV_FUNC(keq * dtheta * dtheta * scw.nrg_scale_f);
        }
#endif
#ifdef COMPUTE_FORCE
#  ifndef UPDATE_ATOMS
#    ifndef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::ANGL_NRG)].x;
#    endif
        if ((__ldca(&poly_vk.angl_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
#  endif
        const TCALC dA = (TCALC)(-2.0) * keq * dtheta * poly_psw.frc_scale /
                         SQRT_FUNC((TCALC)(1.0) - (costheta * costheta));
        const TCALC sqba = dA / mgba;
        const TCALC sqbc = dA / mgbc;
        const TCALC mbabc = dA * invbabc;
        i_atom -= EXCL_GMEM_OFFSET;
        j_atom -= EXCL_GMEM_OFFSET;
        k_atom -= EXCL_GMEM_OFFSET;
#  ifdef SPLIT_FORCE_ACCUMULATION

        // The accumulation proceeds by computing the opposite of the values for iadf and icdf
        // in the CPU, which are then added to the I and K atoms, respectively.  The "anti-sum"
        // of the two (the most convenient way to flip the signs of both elements of an int2) is
        // then added to the J atom.
        const int2 iadf_x = convertSplitFixedPrecision((costheta * ba_x * sqba) - (bc_x * mbabc));
        const int2 iadf_y = convertSplitFixedPrecision((costheta * ba_y * sqba) - (bc_y * mbabc));
        const int2 iadf_z = convertSplitFixedPrecision((costheta * ba_z * sqba) - (bc_z * mbabc));
        addSplitFixedPrecision(iadf_x, i_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(iadf_y, i_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(iadf_z, i_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
        const int2 icdf_x = convertSplitFixedPrecision((costheta * bc_x * sqbc) - (ba_x * mbabc));
        const int2 icdf_y = convertSplitFixedPrecision((costheta * bc_y * sqbc) - (ba_y * mbabc));
        const int2 icdf_z = convertSplitFixedPrecision((costheta * bc_z * sqbc) - (ba_z * mbabc));
        addSplitFixedPrecision(icdf_x, k_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icdf_y, k_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icdf_z, k_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
        const int2 iacdf_x = antiCombineSplitFixedPrecision(iadf_x, icdf_x);
        const int2 iacdf_y = antiCombineSplitFixedPrecision(iadf_y, icdf_y);
        const int2 iacdf_z = antiCombineSplitFixedPrecision(iadf_z, icdf_z);
        addSplitFixedPrecision(iacdf_x, j_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(iacdf_y, j_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(iacdf_z, j_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
#  else // SPLIT_FORCE_ACCUMULATION
        const llint iadf_x = LLCONV_FUNC((bc_x * mbabc) - (costheta * ba_x * sqba));
        const llint iadf_y = LLCONV_FUNC((bc_y * mbabc) - (costheta * ba_y * sqba));
        const llint iadf_z = LLCONV_FUNC((bc_z * mbabc) - (costheta * ba_z * sqba));
        const llint icdf_x = LLCONV_FUNC((ba_x * mbabc) - (costheta * bc_x * sqbc));
        const llint icdf_y = LLCONV_FUNC((ba_y * mbabc) - (costheta * bc_y * sqbc));
        const llint icdf_z = LLCONV_FUNC((ba_z * mbabc) - (costheta * bc_z * sqbc));
        atomicAdd((ullint*)&sh_xfrc[i_atom], lliToUlli(-iadf_x));
        atomicAdd((ullint*)&sh_yfrc[i_atom], lliToUlli(-iadf_y));
        atomicAdd((ullint*)&sh_zfrc[i_atom], lliToUlli(-iadf_z));
        atomicAdd((ullint*)&sh_xfrc[j_atom], lliToUlli(iadf_x + icdf_x));
        atomicAdd((ullint*)&sh_yfrc[j_atom], lliToUlli(iadf_y + icdf_y));
        atomicAdd((ullint*)&sh_zfrc[j_atom], lliToUlli(iadf_z + icdf_z));
        atomicAdd((ullint*)&sh_xfrc[k_atom], lliToUlli(-icdf_x));
        atomicAdd((ullint*)&sh_yfrc[k_atom], lliToUlli(-icdf_y));
        atomicAdd((ullint*)&sh_zfrc[k_atom], lliToUlli(-icdf_z));
#  endif
#  ifndef UPDATE_ATOMS
        }
#  endif
#endif
      }
      pos += blockDim.x;
    }
#ifdef COMPUTE_ENERGY
    // Stash the threads' accumulated angle energy values
    WARP_REDUCE_DOWN(angl_acc)
    if ((threadIdx.x & warp_bits_mask_int) == 0) {
      sh_angl_acc[(threadIdx.x >> warp_bits)] = angl_acc;
    }
#endif
    vterm_offset = vterm_limit;
    vterm_limit  = vterm_offset + vwu_padded_task_count[(size_t)(VwuAbstractMap::CBND)];
#ifdef COMPUTE_ENERGY
    llint bond_acc = 0LL;
    llint ubrd_acc = 0LL;
#endif
    while (pos < vterm_limit) {
      if (pos - vterm_offset < vwu_task_count[(size_t)(VwuAbstractMap::CBND)]) {
        const int task_offset = vwu_map[(size_t)(VwuAbstractMap::CBND)].x;
        const uint2 tinsr = __ldcs(&poly_vk.cbnd_insr[task_offset + pos - vterm_offset]);
        const bool is_urey_bradley = ((tinsr.x >> 20) & 0x1);
        int i_atom = (tinsr.x & 0x3ff) + EXCL_GMEM_OFFSET;
        int j_atom = ((tinsr.x >> 10) & 0x3ff) + EXCL_GMEM_OFFSET;
        const int param_idx = tinsr.y;
        const TCALC keq = (is_urey_bradley) ? __ldca(&poly_vk.ubrd_keq[param_idx]) :
                                              __ldca(&poly_vk.bond_keq[param_idx]);
        const TCALC leq = (is_urey_bradley) ? __ldca(&poly_vk.ubrd_leq[param_idx]) :
                                              ABS_FUNC(__ldca(&poly_vk.bond_leq[param_idx]));
        const llint ixloc = __ldca(&gmem_r.xcrd[i_atom]);
        const llint jxloc = __ldca(&gmem_r.xcrd[j_atom]);
        const TCALC dx = (TCALC)(jxloc - ixloc) * poly_psw.inv_gpos_scale_f;
        const llint iyloc = __ldca(&gmem_r.ycrd[i_atom]);
        const llint jyloc = __ldca(&gmem_r.ycrd[j_atom]);
        const TCALC dy = (TCALC)(jyloc - iyloc) * poly_psw.inv_gpos_scale_f;
        const llint izloc = __ldca(&gmem_r.zcrd[i_atom]);
        const llint jzloc = __ldca(&gmem_r.zcrd[j_atom]);
        const TCALC dz = (TCALC)(jzloc - izloc) * poly_psw.inv_gpos_scale_f;
        const TCALC dr = SQRT_FUNC((dx * dx) + (dy * dy) + (dz * dz));
        const TCALC dl = dr - leq;
#ifdef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::CBND_NRG)].x;
        if ((__ldca(&poly_vk.cbnd_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {

          // The single-precision variant of the scaling factor is exact, like the double-precision
          // variant.  If TCALC is double, it will get promoted in an inexpensive operation.
          if (is_urey_bradley) {
            ubrd_acc += LLCONV_FUNC(keq * dl * dl * scw.nrg_scale_f);
          }
          else {
            bond_acc += LLCONV_FUNC(keq * dl * dl * scw.nrg_scale_f);
          }
        }
#endif
#ifdef COMPUTE_FORCE
#  ifndef UPDATE_ATOMS
#    ifndef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::CBND_NRG)].x;
#    endif
        if ((__ldca(&poly_vk.cbnd_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
#  endif
        const TCALC fmag = poly_psw.frc_scale * (TCALC)(2.0) * keq * dl / dr;
        const TCALC fmag_dx = fmag * dx;
        const TCALC fmag_dy = fmag * dy;
        const TCALC fmag_dz = fmag * dz;
        i_atom -= EXCL_GMEM_OFFSET;
        j_atom -= EXCL_GMEM_OFFSET;
#  ifdef SPLIT_FORCE_ACCUMULATION
        splitForceContribution( fmag_dx, i_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution( fmag_dy, i_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution( fmag_dz, i_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution(-fmag_dx, j_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution(-fmag_dy, j_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution(-fmag_dz, j_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
#  else // SPLIT_FORCE_ACCUMULATION
        const llint ifmag_dx = LLCONV_FUNC(fmag_dx);
        const llint ifmag_dy = LLCONV_FUNC(fmag_dy);
        const llint ifmag_dz = LLCONV_FUNC(fmag_dz);
        atomicAdd((ullint*)&sh_xfrc[i_atom], lliToUlli( ifmag_dx));
        atomicAdd((ullint*)&sh_yfrc[i_atom], lliToUlli( ifmag_dy));
        atomicAdd((ullint*)&sh_zfrc[i_atom], lliToUlli( ifmag_dz));
        atomicAdd((ullint*)&sh_xfrc[j_atom], lliToUlli(-ifmag_dx));
        atomicAdd((ullint*)&sh_yfrc[j_atom], lliToUlli(-ifmag_dy));
        atomicAdd((ullint*)&sh_zfrc[j_atom], lliToUlli(-ifmag_dz));
#  endif
#  ifndef UPDATE_ATOMS
        // See above for the reason these force accumulations sit within a conditional scope
        }
#  endif
#endif
      }
      pos += blockDim.x;
    }
#ifdef COMPUTE_ENERGY
    // Stash the threads' accumulated bond energy values
    WARP_REDUCE_DOWN(bond_acc)
    WARP_REDUCE_DOWN(ubrd_acc)
    if ((threadIdx.x & warp_bits_mask_int) == 0) {
      sh_bond_acc[(threadIdx.x >> warp_bits)] = bond_acc;
      sh_ubrd_acc[(threadIdx.x >> warp_bits)] = ubrd_acc;
    }
#endif
    vterm_offset = vterm_limit;
    vterm_limit = vterm_offset + vwu_padded_task_count[(size_t)(VwuAbstractMap::INFR14)];
#ifdef COMPUTE_ENERGY
    qq14_acc = 0LL;
    lj14_acc = 0LL;
#endif
    while (pos < vterm_limit) {
      if (pos - vterm_offset < vwu_task_count[(size_t)(VwuAbstractMap::INFR14)]) {
        const int task_offset = vwu_map[(size_t)(VwuAbstractMap::INFR14)].x;
        const uint tinsr = __ldcs(&poly_vk.infr14_insr[task_offset + pos - vterm_offset]);
        int i_atom = (tinsr & 0x3ff) + EXCL_GMEM_OFFSET;
        int l_atom = ((tinsr >> 10) & 0x3ff) + EXCL_GMEM_OFFSET;
        const int attn_idx = ((tinsr >> 20) & 0xfff);
        TCALC attn_qq14 = (TCALC)(0.0);
        TCALC attn_lj14 = (TCALC)(0.0);
        if (attn_idx > 0) {
          attn_qq14 = (TCALC)(1.0) / __ldca(&poly_vk.attn14_elec[attn_idx]);
          attn_lj14 = (TCALC)(1.0) / __ldca(&poly_vk.attn14_vdw[attn_idx]);
        }
        const llint ixloc = __ldca(&gmem_r.xcrd[i_atom]);
        const llint lxloc = __ldca(&gmem_r.xcrd[l_atom]);
        const TCALC dx = (TCALC)(lxloc - ixloc) * poly_psw.inv_gpos_scale_f;
        const llint iyloc = __ldca(&gmem_r.ycrd[i_atom]);
        const llint lyloc = __ldca(&gmem_r.ycrd[l_atom]);
        const TCALC dy = (TCALC)(lyloc - iyloc) * poly_psw.inv_gpos_scale_f;
        const llint izloc = __ldca(&gmem_r.zcrd[i_atom]);
        const llint lzloc = __ldca(&gmem_r.zcrd[l_atom]);
        const TCALC dz = (TCALC)(lzloc - izloc) * poly_psw.inv_gpos_scale_f;
        const int i_ljidx = __ldca(&gmem_r.lj_idx[i_atom]);
        const int l_ljidx = __ldca(&gmem_r.lj_idx[l_atom]);
        const int sysid = vwu_map[(size_t)(VwuAbstractMap::SYSTEM_ID)].x;
        const int il_ljidx = __ldca(&poly_vk.ljabc_offsets[sysid]) +
                             (__ldca(&poly_vk.n_lj_types[sysid]) * l_ljidx) + i_ljidx;
        const TCALC qq_il = __ldca(&gmem_r.charges[i_atom]) * __ldca(&gmem_r.charges[l_atom]) *
                            poly_vk.coulomb * attn_qq14;
        const TCALC lja_il = __ldca(&poly_vk.lja_14_coeff[il_ljidx]) * attn_lj14;
        const TCALC ljb_il = __ldca(&poly_vk.ljb_14_coeff[il_ljidx]) * attn_lj14;
        const TCALC invr = (TCALC)(1.0) / SQRT_FUNC((dx * dx) + (dy * dy) + (dz * dz));
        const TCALC invr2 = invr  * invr;
        const TCALC invr4 = invr2 * invr2;
#ifdef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::INFR14_NRG)].x;
        if ((__ldca(&poly_vk.infr14_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
          const TCALC ele_contrib = qq_il * invr;
          const TCALC vdw_contrib = (lja_il * invr4 * invr4 * invr4) - (ljb_il * invr4 * invr2);
          qq14_acc += LLCONV_FUNC(ele_contrib * scw.nrg_scale_f);
          lj14_acc += LLCONV_FUNC(vdw_contrib * scw.nrg_scale_f);
        }
#endif
#ifdef COMPUTE_FORCE
#  ifndef UPDATE_ATOMS
#    ifndef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::INFR14_NRG)].x;
#    endif
        if ((__ldca(&poly_vk.infr14_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
#  endif        
        const TCALC fmag = poly_psw.frc_scale * (-(qq_il * invr * invr2) +
                                                 (((TCALC)(6.0) * ljb_il) -
                                                  ((TCALC)(12.0) * lja_il * invr4 * invr2)) *
                                                 invr4 * invr4);
        i_atom -= EXCL_GMEM_OFFSET;
        l_atom -= EXCL_GMEM_OFFSET;
#  ifdef SPLIT_FORCE_ACCUMULATION
        const TCALC fmag_dx = fmag * dx;
        const TCALC fmag_dy = fmag * dy;
        const TCALC fmag_dz = fmag * dz;
        splitForceContribution( fmag_dx, i_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution( fmag_dy, i_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution( fmag_dz, i_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution(-fmag_dx, l_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution(-fmag_dy, l_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution(-fmag_dz, l_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
#  else // SPLIT_FORCE_ACCUMULATION
        const llint ifmag_dx = LLCONV_FUNC(fmag * dx);
        const llint ifmag_dy = LLCONV_FUNC(fmag * dy);
        const llint ifmag_dz = LLCONV_FUNC(fmag * dz);
        atomicAdd((ullint*)&sh_xfrc[i_atom], lliToUlli( ifmag_dx));
        atomicAdd((ullint*)&sh_yfrc[i_atom], lliToUlli( ifmag_dy));
        atomicAdd((ullint*)&sh_zfrc[i_atom], lliToUlli( ifmag_dz));
        atomicAdd((ullint*)&sh_xfrc[l_atom], lliToUlli(-ifmag_dx));
        atomicAdd((ullint*)&sh_yfrc[l_atom], lliToUlli(-ifmag_dy));
        atomicAdd((ullint*)&sh_zfrc[l_atom], lliToUlli(-ifmag_dz));
#  endif
#  ifndef UPDATE_ATOMS
        }
#  endif
#endif
      }
      pos += blockDim.x;
    }
#ifdef COMPUTE_ENERGY
    // Stash the threads' accumulated 1:4 interaction energy values.  There may (likely, will)
    // already be accumulated 1:4 energies in these slots, so in this particular case the
    // new energies add to existing ones rather than just setting the __shared__ array values.
    WARP_REDUCE_DOWN(qq14_acc)
    WARP_REDUCE_DOWN(lj14_acc)
    if ((threadIdx.x & warp_bits_mask_int) == 0) {
      sh_qq14_acc[(threadIdx.x >> warp_bits)] += qq14_acc;
      sh_lj14_acc[(threadIdx.x >> warp_bits)] += lj14_acc;
    }
#endif
    vterm_offset = vterm_limit;
    vterm_limit = vterm_offset + vwu_padded_task_count[(size_t)(VwuAbstractMap::RPOSN)];
#ifdef COMPUTE_ENERGY
    llint rstr_acc = 0LL;
#endif
    while (pos < vterm_limit) {
      if (pos - vterm_offset < vwu_task_count[(size_t)(VwuAbstractMap::RPOSN)]) {
        const int task_offset = vwu_map[(size_t)(VwuAbstractMap::RPOSN)].x;
        const uint2 tinsr = __ldcs(&poly_rk.rposn_insr[task_offset + pos - vterm_offset]);
        int p_atom = (tinsr.x & 0x3ff) + EXCL_GMEM_OFFSET;
        const int kr_param_idx = ((tinsr.x >> 10) & 0x1fffff);
        const int xyz_param_idx = tinsr.y;
        
        // The reference coordinate storage in floating point numbers may not be as good for
        // precision in the calculations, but restraints of this sort are generally employed
        // at the beginning of a simulation, when the molecule is near the origin (and hence
        // the numbers will not be large), and also with the intention of bleeding off excess
        // energy in the system while keeping its general structure intact.  There are a few
        // applications which may involve both positional restraints and energy conservation,
        // but the damage done by keeping positional restraints in 32-bit floats will likely
        // be marginal in those cases.
        const int2 step_bounds = __ldca(&poly_rk.rposn_step_bounds[kr_param_idx]);
        const TCALC2 mixwt = MIX_FUNC(ctrl.step, step_bounds.x, step_bounds.y);
        const TCALC2 init_xy = __ldca(&poly_rk.rposn_init_xy[xyz_param_idx]);
        const TCALC2 finl_xy = __ldca(&poly_rk.rposn_finl_xy[xyz_param_idx]);
        const TCALC  init_z  = __ldca(&poly_rk.rposn_init_z[xyz_param_idx]);
        const TCALC  finl_z  = __ldca(&poly_rk.rposn_finl_z[xyz_param_idx]);
        const TCALC dx = ((TCALC)(__ldca(&gmem_r.xcrd[p_atom])) * poly_psw.inv_gpos_scale_f) -
                         ((mixwt.x * init_xy.x) + (mixwt.y * finl_xy.x));
        const TCALC dy = ((TCALC)(__ldca(&gmem_r.ycrd[p_atom])) * poly_psw.inv_gpos_scale_f) -
                         ((mixwt.x * init_xy.y) + (mixwt.y * finl_xy.y));
        const TCALC dz = ((TCALC)(__ldca(&gmem_r.zcrd[p_atom])) * poly_psw.inv_gpos_scale_f) -
                         ((mixwt.x * init_z) + (mixwt.y * finl_z));
        const TCALC dr = SQRT_FUNC((dx * dx) + (dy * dy) + (dz * dz));

        // There is no 256-bit operation for global memory loads, so __ldca cannot be applied to
        // double4 types.  The __ldca operation will be implemented for a naive L1-caching scheme,
        // so regardless of data type just use the straight memory reference for the 4-tuple data.
        const TCALC3 rst_eval = restraintDelta(__ldca(&poly_rk.rposn_init_k[kr_param_idx]),
                                               __ldca(&poly_rk.rposn_finl_k[kr_param_idx]),
                                               poly_rk.rposn_init_r[kr_param_idx],
                                               poly_rk.rposn_finl_r[kr_param_idx], mixwt, dr);
#ifdef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::RPOSN_NRG)].x;
        if ((__ldca(&poly_rk.rposn_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
          rstr_acc += LLCONV_FUNC(rst_eval.z * scw.nrg_scale_f);
        }
#endif
#ifdef COMPUTE_FORCE
#  ifndef UPDATE_ATOMS
#    ifndef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::RPOSN_NRG)].x;
#    endif
        if ((__ldca(&poly_rk.rposn_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
#  endif
        p_atom -= EXCL_GMEM_OFFSET;
        if (dr < constants::tiny_f) {

          // See the explanation in the CPU routine (valence_potential.tpp) as to why an
          // arbitrary direction must be applied to the force.
          const TCALC fmag = poly_psw.frc_scale * (TCALC)(2.0) * rst_eval.x * rst_eval.y /
                             SQRT_FUNC((TCALC)(3.0));
#  ifdef SPLIT_FORCE_ACCUMULATION
          const int2 ifmag = convertSplitFixedPrecision(-fmag);
          addSplitFixedPrecision(ifmag, p_atom, sh_xfrc, xoverflow_active, gmem_r.xfrc_overflow,
                                 EXCL_GMEM_OFFSET);
          addSplitFixedPrecision(ifmag, p_atom, sh_yfrc, yoverflow_active, gmem_r.yfrc_overflow,
                                 EXCL_GMEM_OFFSET);
          addSplitFixedPrecision(ifmag, p_atom, sh_zfrc, zoverflow_active, gmem_r.zfrc_overflow,
                                 EXCL_GMEM_OFFSET);
#  else // SPLIT_FORCE_ACCUMULATION
          const llint ifmag = LLCONV_FUNC(-fmag);
          atomicAdd((ullint*)&sh_xfrc[p_atom], lliToUlli(ifmag));
          atomicAdd((ullint*)&sh_yfrc[p_atom], lliToUlli(ifmag));
          atomicAdd((ullint*)&sh_zfrc[p_atom], lliToUlli(ifmag));
#  endif
        }
        else {
          const TCALC fmag = poly_psw.frc_scale * (TCALC)(2.0) * rst_eval.x * rst_eval.y / dr;
#  ifdef SPLIT_FORCE_ACCUMULATION
          splitForceContribution(-fmag * dx, p_atom, sh_xfrc, xoverflow_active,
                                 gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
          splitForceContribution(-fmag * dy, p_atom, sh_yfrc, yoverflow_active,
                                 gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
          splitForceContribution(-fmag * dz, p_atom, sh_zfrc, zoverflow_active,
                                 gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
#  else // SPLIT_FORCE_ACCUMULATION
          atomicAdd((ullint*)&sh_xfrc[p_atom], lliToUlli(LLCONV_FUNC(-fmag * dx)));
          atomicAdd((ullint*)&sh_yfrc[p_atom], lliToUlli(LLCONV_FUNC(-fmag * dy)));
          atomicAdd((ullint*)&sh_zfrc[p_atom], lliToUlli(LLCONV_FUNC(-fmag * dz)));
#  endif          
        }
#  ifndef UPDATE_ATOMS
        }
#  endif
#endif
      }
      pos += blockDim.x;
    }
    vterm_offset = vterm_limit;
    vterm_limit = vterm_offset + vwu_padded_task_count[(size_t)(VwuAbstractMap::RBOND)];
    while (pos < vterm_limit) {
      if (pos - vterm_offset < vwu_task_count[(size_t)(VwuAbstractMap::RBOND)]) {
        const int task_offset = vwu_map[(size_t)(VwuAbstractMap::RBOND)].x;
        const uint2 tinsr = __ldcs(&poly_rk.rbond_insr[task_offset + pos - vterm_offset]);
        int i_atom = (tinsr.x & 0x3ff) + EXCL_GMEM_OFFSET;
        int j_atom = ((tinsr.x >> 10) & 0x3ff) + EXCL_GMEM_OFFSET;
        const int kr_param_idx = tinsr.y;
        const int2 step_bounds = __ldca(&poly_rk.rbond_step_bounds[kr_param_idx]);
        const TCALC2 mixwt = MIX_FUNC(ctrl.step, step_bounds.x, step_bounds.y);
        const TCALC dx = ((TCALC)(__ldca(&gmem_r.xcrd[j_atom])) * poly_psw.inv_gpos_scale_f) -
                         ((TCALC)(__ldca(&gmem_r.xcrd[i_atom])) * poly_psw.inv_gpos_scale_f);
        const TCALC dy = ((TCALC)(__ldca(&gmem_r.ycrd[j_atom])) * poly_psw.inv_gpos_scale_f) -
                         ((TCALC)(__ldca(&gmem_r.ycrd[i_atom])) * poly_psw.inv_gpos_scale_f);
        const TCALC dz = ((TCALC)(__ldca(&gmem_r.zcrd[j_atom])) * poly_psw.inv_gpos_scale_f) -
                         ((TCALC)(__ldca(&gmem_r.zcrd[i_atom])) * poly_psw.inv_gpos_scale_f);
        const TCALC dr = SQRT_FUNC((dx * dx) + (dy * dy) + (dz * dz));
        const TCALC3 rst_eval = restraintDelta(__ldca(&poly_rk.rbond_init_k[kr_param_idx]),
                                               __ldca(&poly_rk.rbond_finl_k[kr_param_idx]),
                                               poly_rk.rbond_init_r[kr_param_idx],
                                               poly_rk.rbond_finl_r[kr_param_idx], mixwt, dr);
#ifdef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::RBOND_NRG)].x;
        if ((__ldca(&poly_rk.rbond_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
          rstr_acc += LLCONV_FUNC(rst_eval.z * scw.nrg_scale_f);
        }
#endif
#ifdef COMPUTE_FORCE
#  ifndef UPDATE_ATOMS
#    ifndef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::RBOND_NRG)].x;
#    endif
        if ((__ldca(&poly_rk.rbond_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
#  endif
        const TCALC fmag = poly_psw.frc_scale * (TCALC)(2.0) * rst_eval.x * rst_eval.y / dr;
        i_atom -= EXCL_GMEM_OFFSET;
        j_atom -= EXCL_GMEM_OFFSET;
#  ifdef SPLIT_FORCE_ACCUMULATION
        const TCALC fmag_dx = fmag * dx;
        const TCALC fmag_dy = fmag * dy;
        const TCALC fmag_dz = fmag * dz;
        splitForceContribution( fmag_dx, i_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution( fmag_dy, i_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution( fmag_dz, i_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution(-fmag_dx, j_atom, sh_xfrc, xoverflow_active,
                               gmem_r.xfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution(-fmag_dy, j_atom, sh_yfrc, yoverflow_active,
                               gmem_r.yfrc_overflow, EXCL_GMEM_OFFSET);
        splitForceContribution(-fmag_dz, j_atom, sh_zfrc, zoverflow_active,
                               gmem_r.zfrc_overflow, EXCL_GMEM_OFFSET);
#  else // SPLIT_FORCE_ACCUMULATION
        const llint ifmag_dx = LLCONV_FUNC(fmag * dx);
        const llint ifmag_dy = LLCONV_FUNC(fmag * dy);
        const llint ifmag_dz = LLCONV_FUNC(fmag * dz);
        atomicAdd((ullint*)&sh_xfrc[i_atom], lliToUlli( ifmag_dx));
        atomicAdd((ullint*)&sh_yfrc[i_atom], lliToUlli( ifmag_dy));
        atomicAdd((ullint*)&sh_zfrc[i_atom], lliToUlli( ifmag_dz));
        atomicAdd((ullint*)&sh_xfrc[j_atom], lliToUlli(-ifmag_dx));
        atomicAdd((ullint*)&sh_yfrc[j_atom], lliToUlli(-ifmag_dy));
        atomicAdd((ullint*)&sh_zfrc[j_atom], lliToUlli(-ifmag_dz));
#  endif
#  ifndef UPDATE_ATOMS
        }
#  endif
#endif
      }
      pos += blockDim.x;
    }
    vterm_offset = vterm_limit;
    vterm_limit = vterm_offset + vwu_padded_task_count[(size_t)(VwuAbstractMap::RANGL)];
    while (pos < vterm_limit) {
      if (pos - vterm_offset < vwu_task_count[(size_t)(VwuAbstractMap::RANGL)]) {
        const int task_offset = vwu_map[(size_t)(VwuAbstractMap::RANGL)].x;
        const uint2 tinsr = __ldcs(&poly_rk.rangl_insr[task_offset + pos - vterm_offset]);
        int i_atom = (tinsr.x & 0x3ff) + EXCL_GMEM_OFFSET;
        int j_atom = ((tinsr.x >> 10) & 0x3ff) + EXCL_GMEM_OFFSET;
        int k_atom = ((tinsr.x >> 20) & 0x3ff) + EXCL_GMEM_OFFSET;
        const int kr_param_idx = tinsr.y;
        const llint ixloc = __ldca(&gmem_r.xcrd[i_atom]);
        const llint jxloc = __ldca(&gmem_r.xcrd[j_atom]);
        const llint kxloc = __ldca(&gmem_r.xcrd[k_atom]);
        const TCALC ba_x = (TCALC)(ixloc - jxloc) * poly_psw.inv_gpos_scale_f;
        const TCALC bc_x = (TCALC)(kxloc - jxloc) * poly_psw.inv_gpos_scale_f;
        const llint iyloc = __ldca(&gmem_r.ycrd[i_atom]);
        const llint jyloc = __ldca(&gmem_r.ycrd[j_atom]);
        const llint kyloc = __ldca(&gmem_r.ycrd[k_atom]);
        const TCALC ba_y = (TCALC)(iyloc - jyloc) * poly_psw.inv_gpos_scale_f;
        const TCALC bc_y = (TCALC)(kyloc - jyloc) * poly_psw.inv_gpos_scale_f;
        const llint izloc = __ldca(&gmem_r.zcrd[i_atom]);
        const llint jzloc = __ldca(&gmem_r.zcrd[j_atom]);
        const llint kzloc = __ldca(&gmem_r.zcrd[k_atom]);
        const TCALC ba_z = (TCALC)(izloc - jzloc) * poly_psw.inv_gpos_scale_f;
        const TCALC bc_z = (TCALC)(kzloc - jzloc) * poly_psw.inv_gpos_scale_f;
        const TCALC mgba = (ba_x * ba_x) + (ba_y * ba_y) + (ba_z * ba_z);
        const TCALC mgbc = (bc_x * bc_x) + (bc_y * bc_y) + (bc_z * bc_z);
        const TCALC invbabc = (TCALC)(1.0) / SQRT_FUNC(mgba * mgbc);
        TCALC costheta = ((ba_x * bc_x) + (ba_y * bc_y) + (ba_z * bc_z)) * invbabc;
        costheta = (costheta < (TCALC)(-1.0)) ?
                   (TCALC)(-1.0) : (costheta > (TCALC)(1.0)) ? (TCALC)(1.0) : costheta;
        const TCALC theta = ACOS_FUNC(costheta);
        const int2 step_bounds = __ldca(&poly_rk.rbond_step_bounds[kr_param_idx]);
        const TCALC2 mixwt = MIX_FUNC(ctrl.step, step_bounds.x, step_bounds.y);
        const TCALC3 rst_eval = restraintDelta(__ldca(&poly_rk.rangl_init_k[kr_param_idx]),
                                               __ldca(&poly_rk.rangl_finl_k[kr_param_idx]),
                                               poly_rk.rangl_init_r[kr_param_idx],
                                               poly_rk.rangl_finl_r[kr_param_idx], mixwt, theta);
#ifdef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::RANGL_NRG)].x;
        if ((__ldca(&poly_rk.rangl_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
          rstr_acc += LLCONV_FUNC(rst_eval.z * scw.nrg_scale_f);
        }
#endif
#ifdef COMPUTE_FORCE
#  ifndef UPDATE_ATOMS
#    ifndef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::RANGL_NRG)].x;
#    endif
        if ((__ldca(&poly_rk.rangl_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
#  endif
        const TCALC dA = (TCALC)(-2.0) * rst_eval.x * rst_eval.y * poly_psw.frc_scale_f /
                         SQRT_FUNC((TCALC)(1.0) - (costheta * costheta));
        const TCALC sqba = dA / mgba;
        const TCALC sqbc = dA / mgbc;
        const TCALC mbabc = dA * invbabc;
        i_atom -= EXCL_GMEM_OFFSET;
        j_atom -= EXCL_GMEM_OFFSET;
        k_atom -= EXCL_GMEM_OFFSET;
#  ifdef SPLIT_FORCE_ACCUMULATION
        const int2 iadf_x = convertSplitFixedPrecision((costheta * ba_x * sqba) - (bc_x * mbabc));
        const int2 iadf_y = convertSplitFixedPrecision((costheta * ba_y * sqba) - (bc_y * mbabc));
        const int2 iadf_z = convertSplitFixedPrecision((costheta * ba_z * sqba) - (bc_z * mbabc));
        addSplitFixedPrecision(iadf_x, i_atom, sh_xfrc, xoverflow_active, gmem_r.xfrc_overflow,
                               EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(iadf_y, i_atom, sh_yfrc, yoverflow_active, gmem_r.yfrc_overflow,
                               EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(iadf_z, i_atom, sh_zfrc, zoverflow_active, gmem_r.zfrc_overflow,
                               EXCL_GMEM_OFFSET);
        const int2 icdf_x = convertSplitFixedPrecision((costheta * bc_x * sqbc) - (ba_x * mbabc));
        const int2 icdf_y = convertSplitFixedPrecision((costheta * bc_y * sqbc) - (ba_y * mbabc));
        const int2 icdf_z = convertSplitFixedPrecision((costheta * bc_z * sqbc) - (ba_z * mbabc));
        addSplitFixedPrecision(icdf_x, k_atom, sh_xfrc, xoverflow_active, gmem_r.xfrc_overflow,
                               EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icdf_y, k_atom, sh_yfrc, yoverflow_active, gmem_r.yfrc_overflow,
                               EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(icdf_z, k_atom, sh_zfrc, zoverflow_active, gmem_r.zfrc_overflow,
                               EXCL_GMEM_OFFSET);
        const int2 iacdf_x = antiCombineSplitFixedPrecision(iadf_x, icdf_x);
        const int2 iacdf_y = antiCombineSplitFixedPrecision(iadf_y, icdf_y);
        const int2 iacdf_z = antiCombineSplitFixedPrecision(iadf_z, icdf_z);
        addSplitFixedPrecision(iacdf_x, j_atom, sh_xfrc, xoverflow_active, gmem_r.xfrc_overflow,
                               EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(iacdf_y, j_atom, sh_yfrc, yoverflow_active, gmem_r.yfrc_overflow,
                               EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(iacdf_z, j_atom, sh_zfrc, zoverflow_active, gmem_r.zfrc_overflow,
                               EXCL_GMEM_OFFSET);
#  else // SPLIT_FORCE_ACCUMULATION
        const llint iadf_x = LLCONV_FUNC((bc_x * mbabc) - (costheta * ba_x * sqba));
        const llint iadf_y = LLCONV_FUNC((bc_y * mbabc) - (costheta * ba_y * sqba));
        const llint iadf_z = LLCONV_FUNC((bc_z * mbabc) - (costheta * ba_z * sqba));
        const llint icdf_x = LLCONV_FUNC((ba_x * mbabc) - (costheta * bc_x * sqbc));
        const llint icdf_y = LLCONV_FUNC((ba_y * mbabc) - (costheta * bc_y * sqbc));
        const llint icdf_z = LLCONV_FUNC((ba_z * mbabc) - (costheta * bc_z * sqbc));
        atomicAdd((ullint*)&sh_xfrc[i_atom], lliToUlli(-iadf_x));
        atomicAdd((ullint*)&sh_yfrc[i_atom], lliToUlli(-iadf_y));
        atomicAdd((ullint*)&sh_zfrc[i_atom], lliToUlli(-iadf_z));
        atomicAdd((ullint*)&sh_xfrc[j_atom], lliToUlli(iadf_x + icdf_x));
        atomicAdd((ullint*)&sh_yfrc[j_atom], lliToUlli(iadf_y + icdf_y));
        atomicAdd((ullint*)&sh_zfrc[j_atom], lliToUlli(iadf_z + icdf_z));
        atomicAdd((ullint*)&sh_xfrc[k_atom], lliToUlli(-icdf_x));
        atomicAdd((ullint*)&sh_yfrc[k_atom], lliToUlli(-icdf_y));
        atomicAdd((ullint*)&sh_zfrc[k_atom], lliToUlli(-icdf_z));
#  endif
#  ifndef UPDATE_ATOMS
        }
#  endif
#endif
      }
      pos += blockDim.x;
    }
    vterm_offset = vterm_limit;
    vterm_limit = vterm_offset + vwu_padded_task_count[(size_t)(VwuAbstractMap::RDIHE)];
    while (pos < vterm_limit) {
      if (pos - vterm_offset < vwu_task_count[(size_t)(VwuAbstractMap::RDIHE)]) {
        const int task_offset = vwu_map[(size_t)(VwuAbstractMap::RDIHE)].x;
        const uint2 tinsr = __ldcs(&poly_rk.rdihe_insr[task_offset + pos - vterm_offset]);
        int i_atom = (tinsr.x & 0x3ff) + EXCL_GMEM_OFFSET;
        int j_atom = ((tinsr.x >> 10) & 0x3ff) + EXCL_GMEM_OFFSET;
        int k_atom = ((tinsr.x >> 20) & 0x3ff) + EXCL_GMEM_OFFSET;
        int l_atom = (tinsr.y & 0x3ff) + EXCL_GMEM_OFFSET;
        const int kr_param_idx = ((tinsr.y >> 10) & 0x3fffff);
        const llint ixloc = __ldca(&gmem_r.xcrd[i_atom]);
        const llint iyloc = __ldca(&gmem_r.ycrd[i_atom]);
        const llint izloc = __ldca(&gmem_r.zcrd[i_atom]);
        const llint jxloc = __ldca(&gmem_r.xcrd[j_atom]);
        const llint jyloc = __ldca(&gmem_r.ycrd[j_atom]);
        const llint jzloc = __ldca(&gmem_r.zcrd[j_atom]);
        const TCALC3 ab = { (TCALC)(jxloc - ixloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(jyloc - iyloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(jzloc - izloc) * poly_psw.inv_gpos_scale_f };
        const llint kxloc = __ldca(&gmem_r.xcrd[k_atom]);
        const llint kyloc = __ldca(&gmem_r.ycrd[k_atom]);
        const llint kzloc = __ldca(&gmem_r.zcrd[k_atom]);
        const TCALC3 bc = { (TCALC)(kxloc - jxloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(kyloc - jyloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(kzloc - jzloc) * poly_psw.inv_gpos_scale_f };
        const llint lxloc = __ldca(&gmem_r.xcrd[l_atom]);
        const llint lyloc = __ldca(&gmem_r.ycrd[l_atom]);
        const llint lzloc = __ldca(&gmem_r.zcrd[l_atom]);
        const TCALC3 cd = { (TCALC)(lxloc - kxloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(lyloc - kyloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(lzloc - kzloc) * poly_psw.inv_gpos_scale_f };
        const TCALC3 ad = { (TCALC)(lxloc - ixloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(lyloc - iyloc) * poly_psw.inv_gpos_scale_f,
                            (TCALC)(lzloc - izloc) * poly_psw.inv_gpos_scale_f };
        TCALC3 crabbc = crossProduct(ab, bc);
        TCALC3 crbccd = crossProduct(bc, cd);
        const TCALC3 scr = crossProduct(crabbc, crbccd);
        TCALC costheta = (crabbc.x * crbccd.x) + (crabbc.y * crbccd.y) + (crabbc.z * crbccd.z);
        costheta /= SQRT_FUNC(((crabbc.x * crabbc.x) + (crabbc.y * crabbc.y) +
                               (crabbc.z * crabbc.z)) *
                              ((crbccd.x * crbccd.x) + (crbccd.y * crbccd.y) +
                               (crbccd.z * crbccd.z)));
#ifdef CHECK_COSARG
        TCALC theta = angleVerification(costheta, crabbc, crbccd, bc, scr);
#else
        costheta = (costheta < (TCALC)(-1.0)) ?
                   (TCALC)(-1.0) : (costheta > (TCALC)(1.0)) ? (TCALC)(1.0) : costheta;
        TCALC theta = ((scr.x * bc.x) + (scr.y * bc.y) + (scr.z * bc.z) > (TCALC)(0.0)) ?
                      ACOS_FUNC(costheta) : -ACOS_FUNC(costheta);
#endif
        const int2 step_bounds = __ldca(&poly_rk.rbond_step_bounds[kr_param_idx]);
        const TCALC2 mixwt = MIX_FUNC(ctrl.step, step_bounds.x, step_bounds.y);
        const TCALC4 init_r = poly_rk.rdihe_init_r[kr_param_idx];
        const TCALC4 finl_r = poly_rk.rdihe_finl_r[kr_param_idx];
        const TCALC midpoint = (TCALC)(0.5) * ((mixwt.x * (init_r.y + init_r.z)) +
                                               (mixwt.y * (finl_r.y + finl_r.z)));
#ifdef CHECK_COSARG
        TCALC frac = (theta - midpoint) / twopi_f;
        frac -= ((frac >= 0.5f) *  ceilf(frac - 0.5f)) + ((frac  < 0.5f) * floorf(frac - 0.5f));
        const TCALC midpoint_delta = (frac - (frac >= 0.5f)) * twopi_f;
#else
        TCALC frac = (theta - midpoint) / twopi;
        frac -= ((frac >= 0.5) *  ceil(frac - 0.5)) + ((frac  < 0.5) * floor(frac - 0.5));
        const TCALC midpoint_delta = (frac - (frac >= 0.5)) * twopi;
#endif
        theta += midpoint_delta - (theta - midpoint);
        const TCALC3 rst_eval = restraintDelta(__ldca(&poly_rk.rdihe_init_k[kr_param_idx]),
                                               __ldca(&poly_rk.rdihe_finl_k[kr_param_idx]),
                                               init_r, finl_r, mixwt, theta);
#ifdef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::RDIHE_NRG)].x;
        if ((__ldca(&poly_rk.rdihe_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
          rstr_acc += LLCONV_FUNC(rst_eval.z * scw.nrg_scale_f);
        }
#endif
#ifdef COMPUTE_FORCE
#  ifndef UPDATE_ATOMS
#    ifndef COMPUTE_ENERGY
        const int acc_elem   = (pos - vterm_offset) / uint_bit_count_int;
        const int acc_bit    = pos - vterm_offset - (acc_elem * uint_bit_count_int);
        const int acc_offset = vwu_map[(size_t)(VwuAbstractMap::RDIHE_NRG)].x;
#    endif
        if ((__ldca(&poly_rk.rdihe_acc[acc_offset + acc_elem]) >> acc_bit) & 0x1) {
#  endif
        const TCALC fr = (TCALC)(-2.0) * rst_eval.x * rst_eval.y * poly_psw.frc_scale_f;
        const TCALC mgab = SQRT_FUNC((ab.x * ab.x) + (ab.y * ab.y) + (ab.z * ab.z));
        const TCALC mgbc = SQRT_FUNC((bc.x * bc.x) + (bc.y * bc.y) + (bc.z * bc.z));
        const TCALC mgcd = SQRT_FUNC((cd.x * cd.x) + (cd.y * cd.y) + (cd.z * cd.z));
        const TCALC invab = (TCALC)(1.0) / mgab;
        const TCALC invbc = (TCALC)(1.0) / mgbc;
        const TCALC invcd = (TCALC)(1.0) / mgcd;
        const TCALC cosb = -((ab.x * bc.x) + (ab.y * bc.y) + (ab.z * bc.z)) * invab * invbc;
        const TCALC cosc = -((bc.x * cd.x) + (bc.y * cd.y) + (bc.z * cd.z)) * invbc * invcd;
#ifdef CHECK_COSARG
        const TCALC isinb2 = (cosb * cosb < asymptotic_to_one_lf) ?
                             fr / (1.0 - (cosb * cosb)) : fr * inverse_one_minus_asymptote_lf;
        const TCALC isinc2 = (cosc * cosc < asymptotic_to_one_lf) ?
                             fr / (1.0 - (cosc * cosc)) : fr * inverse_one_minus_asymptote_lf;
#else // CHECK_COSARG
        const TCALC isinb2 = (cosb * cosb < asymptotic_to_one_f) ?
                             fr / (1.0f - (cosb * cosb)) : fr * inverse_one_minus_asymptote_f;
        const TCALC isinc2 = (cosc * cosc < asymptotic_to_one_f) ?
                             fr / (1.0f - (cosc * cosc)) : fr * inverse_one_minus_asymptote_f;
#endif
        const TCALC invabc = invab * invbc;
        const TCALC invbcd = invbc * invcd;
        crabbc.x *= invabc;
        crabbc.y *= invabc;
        crabbc.z *= invabc;
        crbccd.x *= invbcd;
        crbccd.y *= invbcd;
        crbccd.z *= invbcd;
        const TCALC fa = -invab * isinb2;
        const TCALC fb1 = (mgbc - (mgab * cosb)) * invabc * isinb2;
        const TCALC fb2 = cosc * invbc * isinc2;
        const TCALC fc1 = (mgbc - (mgcd * cosc)) * invbcd * isinc2;
        const TCALC fc2 = cosb * invbc * isinb2;
        const TCALC fd = -invcd * isinc2;
        i_atom -= EXCL_GMEM_OFFSET;
        j_atom -= EXCL_GMEM_OFFSET;
        k_atom -= EXCL_GMEM_OFFSET;
        l_atom -= EXCL_GMEM_OFFSET;
#  ifdef SPLIT_FORCE_ACCUMULATION
        const int2 ifrc_ix = convertSplitFixedPrecision(crabbc.x * fa);
        const int2 ifrc_jx = convertSplitFixedPrecision((fb1 * crabbc.x) - (fb2 * crbccd.x));
        const int2 ifrc_lx = convertSplitFixedPrecision(-fd * crbccd.x);
        addSplitFixedPrecision(ifrc_ix, i_atom, sh_xfrc, xoverflow_active, gmem_r.xfrc_overflow,
                               EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ifrc_jx, j_atom, sh_xfrc, xoverflow_active, gmem_r.xfrc_overflow,
                               EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ifrc_lx, l_atom, sh_xfrc, xoverflow_active, gmem_r.xfrc_overflow,
                               EXCL_GMEM_OFFSET);
        const int2 ifrc_ijx = combineSplitFixedPrecision(ifrc_ix, ifrc_jx);
        const int2 ifrc_kx = antiCombineSplitFixedPrecision(ifrc_ijx, ifrc_lx);
        addSplitFixedPrecision(ifrc_kx, k_atom, sh_xfrc, xoverflow_active, gmem_r.xfrc_overflow,
                               EXCL_GMEM_OFFSET);
        const int2 ifrc_iy = convertSplitFixedPrecision(crabbc.y * fa);
        const int2 ifrc_jy = convertSplitFixedPrecision((fb1 * crabbc.y) - (fb2 * crbccd.y));
        const int2 ifrc_ly = convertSplitFixedPrecision(-fd * crbccd.y);
        addSplitFixedPrecision(ifrc_iy, i_atom, sh_yfrc, yoverflow_active, gmem_r.yfrc_overflow,
                               EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ifrc_jy, j_atom, sh_yfrc, yoverflow_active, gmem_r.yfrc_overflow,
                               EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ifrc_ly, l_atom, sh_yfrc, yoverflow_active, gmem_r.yfrc_overflow,
                               EXCL_GMEM_OFFSET);
        const int2 ifrc_ijy = combineSplitFixedPrecision(ifrc_iy, ifrc_jy);
        const int2 ifrc_ky = antiCombineSplitFixedPrecision(ifrc_ijy, ifrc_ly);
        addSplitFixedPrecision(ifrc_ky, k_atom, sh_yfrc, yoverflow_active, gmem_r.yfrc_overflow,
                               EXCL_GMEM_OFFSET);
        const int2 ifrc_iz = convertSplitFixedPrecision(crabbc.z * fa);
        const int2 ifrc_jz = convertSplitFixedPrecision((fb1 * crabbc.z) - (fb2 * crbccd.z));
        const int2 ifrc_lz = convertSplitFixedPrecision(-fd * crbccd.z);
        addSplitFixedPrecision(ifrc_iz, i_atom, sh_zfrc, zoverflow_active, gmem_r.zfrc_overflow,
                               EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ifrc_jz, j_atom, sh_zfrc, zoverflow_active, gmem_r.zfrc_overflow,
                               EXCL_GMEM_OFFSET);
        addSplitFixedPrecision(ifrc_lz, l_atom, sh_zfrc, zoverflow_active, gmem_r.zfrc_overflow,
                               EXCL_GMEM_OFFSET);
        const int2 ifrc_ijz = combineSplitFixedPrecision(ifrc_iz, ifrc_jz);
        const int2 ifrc_kz = antiCombineSplitFixedPrecision(ifrc_ijz, ifrc_lz);
        addSplitFixedPrecision(ifrc_kz, k_atom, sh_zfrc, zoverflow_active, gmem_r.zfrc_overflow,
                               EXCL_GMEM_OFFSET);
#  else // SPLIT_FORCE_ACCUMULATION
        const llint ifrc_ix = LLCONV_FUNC(crabbc.x * fa);
        const llint ifrc_jx = LLCONV_FUNC((fb1 * crabbc.x) - (fb2 * crbccd.x));
        const llint ifrc_lx = LLCONV_FUNC(-fd * crbccd.x);
        atomicAdd((ullint*)&sh_xfrc[i_atom], lliToUlli(ifrc_ix));
        atomicAdd((ullint*)&sh_xfrc[j_atom], lliToUlli(ifrc_jx));
        atomicAdd((ullint*)&sh_xfrc[k_atom], lliToUlli(-(ifrc_ix + ifrc_jx + ifrc_lx)));
        atomicAdd((ullint*)&sh_xfrc[l_atom], lliToUlli(ifrc_lx));
        const llint ifrc_iy = LLCONV_FUNC(crabbc.y * fa);
        const llint ifrc_jy = LLCONV_FUNC((fb1 * crabbc.y) - (fb2 * crbccd.y));
        const llint ifrc_ly = LLCONV_FUNC(-fd * crbccd.y);
        atomicAdd((ullint*)&sh_yfrc[i_atom], lliToUlli(ifrc_iy));
        atomicAdd((ullint*)&sh_yfrc[j_atom], lliToUlli(ifrc_jy));
        atomicAdd((ullint*)&sh_yfrc[k_atom], lliToUlli(-(ifrc_iy + ifrc_jy + ifrc_ly)));
        atomicAdd((ullint*)&sh_yfrc[l_atom], lliToUlli(ifrc_ly));
        const llint ifrc_iz = LLCONV_FUNC(crabbc.z * fa);
        const llint ifrc_jz = LLCONV_FUNC((fb1 * crabbc.z) - (fb2 * crbccd.z));
        const llint ifrc_lz = LLCONV_FUNC(-fd * crbccd.z);
        atomicAdd((ullint*)&sh_zfrc[i_atom], lliToUlli(ifrc_iz));
        atomicAdd((ullint*)&sh_zfrc[j_atom], lliToUlli(ifrc_jz));
        atomicAdd((ullint*)&sh_zfrc[k_atom], lliToUlli(-(ifrc_iz + ifrc_jz + ifrc_lz)));
        atomicAdd((ullint*)&sh_zfrc[l_atom], lliToUlli(ifrc_lz));
#  endif
#  ifndef UPDATE_ATOMS
        }
#  endif
#endif
      }
      pos += blockDim.x;
    }
#ifdef COMPUTE_ENERGY
    // Stash the threads' accumulated restraint energy penalty values
    WARP_REDUCE_DOWN(rstr_acc)
    if ((threadIdx.x & warp_bits_mask_int) == 0) {
      sh_rstr_acc[(threadIdx.x >> warp_bits)] = rstr_acc;
    }
#endif
    __syncthreads();
#ifdef COMPUTE_ENERGY
    // Contribute energies to global accumulators--get these numbers out of the way to make
    // room for any other operations.
    __syncthreads();
    const int lane_idx = (threadIdx.x & warp_bits_mask_int);
    pos = (threadIdx.x >> warp_bits);
    const bool iread = (lane_idx < (blockDim.x >> warp_bits));
    while (pos < (int)(StateVariable::ALL_STATES)) {
      const int sys_pos = pos + (vwu_map[(size_t)(VwuAbstractMap::SYSTEM_ID)].x * scw.data_stride);
      const StateVariable sv_pos = (StateVariable)(pos);
      if (sv_pos == StateVariable::BOND) {
        llint sum_bond_acc = (iread) ? sh_bond_acc[lane_idx] : 0LL;
        WARP_REDUCE_DOWN(sum_bond_acc);
        if (lane_idx == 0) {
          atomicAdd((ullint*)&scw.instantaneous_accumulators[sys_pos], lliToUlli(sum_bond_acc));
        }
      }
      else if (sv_pos == StateVariable::ANGLE) {
        llint sum_angl_acc = (iread) ? sh_angl_acc[lane_idx] : 0LL;
        WARP_REDUCE_DOWN(sum_angl_acc);
        if (lane_idx == 0) {
          atomicAdd((ullint*)&scw.instantaneous_accumulators[sys_pos], lliToUlli(sum_angl_acc));
        }
      }
      else if (sv_pos == StateVariable::PROPER_DIHEDRAL) {
        llint sum_dihe_acc = (iread) ? sh_dihe_acc[lane_idx] : 0LL;
        WARP_REDUCE_DOWN(sum_dihe_acc);
        if (lane_idx == 0) {
          atomicAdd((ullint*)&scw.instantaneous_accumulators[sys_pos], lliToUlli(sum_dihe_acc));
        }
      }
      else if (sv_pos == StateVariable::IMPROPER_DIHEDRAL) {
        llint sum_impr_acc = (iread) ? sh_impr_acc[lane_idx] : 0LL;
        WARP_REDUCE_DOWN(sum_impr_acc);
        if (lane_idx == 0) {
          atomicAdd((ullint*)&scw.instantaneous_accumulators[sys_pos], lliToUlli(sum_impr_acc));
        }
      }
      else if (sv_pos == StateVariable::UREY_BRADLEY) {
        llint sum_ubrd_acc = (iread) ? sh_ubrd_acc[lane_idx] : 0LL;
        WARP_REDUCE_DOWN(sum_ubrd_acc);
        if (lane_idx == 0) {
          atomicAdd((ullint*)&scw.instantaneous_accumulators[sys_pos], lliToUlli(sum_ubrd_acc));
        }
      }
      else if (sv_pos == StateVariable::CHARMM_IMPROPER) {
        llint sum_cimp_acc = (iread) ? sh_cimp_acc[lane_idx] : 0LL;
        WARP_REDUCE_DOWN(sum_cimp_acc);
        if (lane_idx == 0) {
          atomicAdd((ullint*)&scw.instantaneous_accumulators[sys_pos], lliToUlli(sum_cimp_acc));
        }
      }
      else if (sv_pos == StateVariable::CMAP) {
        llint sum_cmap_acc = (iread) ? sh_cmap_acc[lane_idx] : 0LL;
        WARP_REDUCE_DOWN(sum_cmap_acc);
        if (lane_idx == 0) {
          atomicAdd((ullint*)&scw.instantaneous_accumulators[sys_pos], lliToUlli(sum_cmap_acc));
        }
      }
      else if (sv_pos == StateVariable::VDW_ONE_FOUR) {
        llint sum_lj14_acc = (iread) ? sh_lj14_acc[lane_idx] : 0LL;
        WARP_REDUCE_DOWN(sum_lj14_acc);
        if (lane_idx == 0) {
          atomicAdd((ullint*)&scw.instantaneous_accumulators[sys_pos], lliToUlli(sum_lj14_acc));
        }
      }
      else if (sv_pos == StateVariable::ELECTROSTATIC_ONE_FOUR) {
        llint sum_qq14_acc = (iread) ? sh_qq14_acc[lane_idx] : 0LL;
        WARP_REDUCE_DOWN(sum_qq14_acc);
        if (lane_idx == 0) {
          atomicAdd((ullint*)&scw.instantaneous_accumulators[sys_pos], lliToUlli(sum_qq14_acc));
        }
      }
      else if (sv_pos == StateVariable::RESTRAINT) {
        llint sum_rstr_acc = (iread) ? sh_rstr_acc[lane_idx] : 0LL;
        WARP_REDUCE_DOWN(sum_rstr_acc);
        if (lane_idx == 0) {
          atomicAdd((ullint*)&scw.instantaneous_accumulators[sys_pos], lliToUlli(sum_rstr_acc));
        }
      }
      pos += (blockDim.x >> warp_bits);
    }
#endif

    // Loop back over all Cartesian X forces, Y forces, and then Z forces.  Reassemble forces if
    // the accumulation was split.  Move atoms if there is any reason to do so.  Log forces in
    // their unified global accumulators otherwise.
#if defined(UPDATE_ATOMS) || defined(COMPUTE_FORCE)
    pos = threadIdx.x;
    while (pos < import_stride) {
      if (pos < import_count) {
#  ifdef SPLIT_FORCE_ACCUMULATION
        const size_t ovrf_group = pos / warp_size_int;
        llint ixfrc;
        if (xoverflow_active[ovrf_group]) {
          ixfrc = __ldca(&gmem_r.xfrc_overflow[pos + EXCL_GMEM_OFFSET]);
          ixfrc *= max_int_accumulation_ll;
        }
        else {
          ixfrc = 0LL;
        }
        ixfrc += (llint)(sh_xfrc[pos]);
#  else // SPLIT_FORCE_ACCUMULATION
        const llint ixfrc = sh_xfrc[pos];
#  endif
#  ifdef UPDATE_ATOMS

#  else // UPDATE_ATOMS
#    ifdef COMPUTE_FORCE
        // Contribute forces to global accumulators
        const size_t write_idx  = __ldca(&poly_vk.vwu_imports[import_llim + pos]);
        atomicAdd((ullint*)&poly_psw.xfrc[write_idx], lliToUlli(ixfrc));
#    endif
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 2 * import_stride) {
      const int rel_pos = pos - import_stride;
      if (rel_pos < import_count) {
#  ifdef SPLIT_FORCE_ACCUMULATION
        const size_t ovrf_group = rel_pos / warp_size_int;
        llint iyfrc;
        if (yoverflow_active[ovrf_group]) {
          iyfrc = __ldca(&gmem_r.yfrc_overflow[pos + EXCL_GMEM_OFFSET]);
          iyfrc *= max_int_accumulation_ll;
        }
        else {
          iyfrc = 0LL;
        }
        iyfrc += (llint)(sh_yfrc[rel_pos]);
#  else // SPLIT_FORCE_ACCUMULATION
        const llint iyfrc = sh_yfrc[rel_pos];
#  endif
#  ifdef UPDATE_ATOMS

#  else // UPDATE_ATOMS
#    ifdef COMPUTE_FORCE
        // Contribute forces to global accumulators
        const size_t write_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        atomicAdd((ullint*)&poly_psw.yfrc[write_idx], lliToUlli(iyfrc));
#    endif
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 3 * import_stride) {
      const int rel_pos = pos - (2 * import_stride);
      if (rel_pos < import_count) {
#  ifdef SPLIT_FORCE_ACCUMULATION
        const size_t ovrf_group = rel_pos / warp_size_int;
        llint izfrc;
        if (zoverflow_active[ovrf_group]) {
          izfrc = __ldca(&gmem_r.zfrc_overflow[pos + EXCL_GMEM_OFFSET]);
          izfrc *= max_int_accumulation_ll;
        }
        else {
          izfrc = 0LL;
        }
        izfrc += (llint)(sh_zfrc[rel_pos]);
#  else // SPLIT_FORCE_ACCUMULATION
        const llint izfrc = sh_zfrc[rel_pos];
#  endif
#  ifdef UPDATE_ATOMS

#  else // UPDATE_ATOMS
#    ifdef COMPUTE_FORCE
        // Contribute forces to global accumulators
        const size_t write_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        atomicAdd((ullint*)&poly_psw.zfrc[write_idx], lliToUlli(izfrc));
#    endif
#  endif
      }
      pos += blockDim.x;
    }
#endif
    // Proceed to the next valence work unit, incrementing one of a series of global counters
    // based on the current step number stored in the PhaseSpaceSynthesis object.  The CPU will
    // update the step counter just prior to launching this kernel so that the correct step number
    // comes in via the constants cache during the kernel launch.  The step number will determine
    // which block progress counter is used.  At appropriate times (based on the modulo of the
    // step number), the kernel will reset some of its block progress counters.
    __syncthreads();
    if (threadIdx.x == 0) {
      const size_t prog_counter_idx = (ctrl.step & twice_warp_bits_mask_int);
      vwu_idx = atomicAdd(&ctrl.vwu_progress[prog_counter_idx], 1);
    }
    __syncthreads();
  }

  // Set the block counters for future iterations of this kernel
  if (blockIdx.x == 0 && threadIdx.x < warp_size_int) {
    const int step_modulus = (ctrl.step & twice_warp_bits_mask_int);
    if (step_modulus == 0) {
      ctrl.vwu_progress[threadIdx.x + warp_size_int] = gridDim.x;
    }
    if (step_modulus == warp_size_int) {
      ctrl.vwu_progress[threadIdx.x] = gridDim.x;
    }
  }
}

#undef EXCL_GMEM_OFFSET
