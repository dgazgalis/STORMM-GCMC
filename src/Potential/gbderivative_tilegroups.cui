// -*-c++-*-
#include "copyright.h"

#define EXCL_GMEM_OFFSET  (blockIdx.x * gmem_r.max_atoms)

/// \brief Compute the Generalized Born radii for all particles in all systems.
///
/// \param poly_nbk  Condensed non-bonded parameter tables and atomic properties for all systems
/// \param ngb_kit   Neck Generalized Born parameters
/// \param ctrl      Molecular mechanics control data
/// \param poly_psw  Coordinates and forces for all particles
/// \param gmem_r    Workspaces for each thread block
__global__ void __launch_bounds__(small_block_size, GBRADII_KERNEL_BLOCKS_MULTIPLIER)
KERNEL_NAME(const SyNonbondedKit<TCALC, TCALC2> poly_nbk, MMControlKit<TCALC> ctrl,
            PsSynthesisWriter poly_psw, CacheResourceKit<TCALC> gmem_r,
            ISWorkspaceKit<TCALC> iswk) {

  // Coordinate and properties of particles are copied into special, L1-cached arrays of GMEM used
  // exclusively by this block.
  __shared__ TCALC sh_tile_xcog[small_block_max_imports];
  __shared__ TCALC sh_tile_ycog[small_block_max_imports];
  __shared__ TCALC sh_tile_zcog[small_block_max_imports];
  __shared__ TCALC sh_tile_tpts[small_block_max_imports];
  __shared__ TCALC sh_psi[small_block_max_atoms];
  __shared__ TCALC sh_screen[small_block_max_atoms];
#ifdef COMPUTE_FORCE
#  ifdef TCALC_IS_SINGLE
#    ifdef SPLIT_FORCE_ACCUMULATION
  __shared__ int sh_xfrc[small_block_max_atoms];
  __shared__ int sh_yfrc[small_block_max_atoms];
  __shared__ int sh_zfrc[small_block_max_atoms];
  __shared__ int sh_xfrc_overflow[small_block_max_atoms];
  __shared__ int sh_yfrc_overflow[small_block_max_atoms];
  __shared__ int sh_zfrc_overflow[small_block_max_atoms];
#    else
  __shared__ llint sh_xfrc[small_block_max_atoms];
  __shared__ llint sh_yfrc[small_block_max_atoms];
  __shared__ llint sh_zfrc[small_block_max_atoms];
#    endif
#  else
  __shared__ llint sh_xfrc[small_block_max_atoms];
  __shared__ llint sh_yfrc[small_block_max_atoms];
  __shared__ llint sh_zfrc[small_block_max_atoms];
  __shared__ int sh_xfrc_overflow[small_block_max_atoms];
  __shared__ int sh_yfrc_overflow[small_block_max_atoms];
  __shared__ int sh_zfrc_overflow[small_block_max_atoms];
#  endif
#endif
  __shared__ int nbwu_map[tile_groups_wu_abstract_length];
  __shared__ int gbdwu_idx;

  // Read the non-bonded work unit abstracts
  if (threadIdx.x == 0) {
    gbdwu_idx = blockIdx.x;
  }
  __syncthreads();
  while (gbdwu_idx < poly_nbk.nnbwu) {
    if (threadIdx.x < tile_groups_wu_abstract_length) {
      nbwu_map[threadIdx.x] =__ldcv(&poly_nbk.nbwu_abstracts[(gbdwu_idx *
                                                              tile_groups_wu_abstract_length) +
                                                             threadIdx.x]);
    }
    __syncthreads();

    // Import atomic coordinates and properties.  Each warp will handle importing one of the
    // Cartesian coordinates or properties of as many tile sides as it can handle, in order to
    // get the most threads reaching out to global memory.
    const int tile_sides_per_warp = (warp_size_int / tile_length);
    const int warp_idx = (threadIdx.x >> warp_bits);
    const int warp_lane_idx = (threadIdx.x & warp_bits_mask_int);
    const int tile_side_offset = warp_lane_idx / tile_length;
    int pos = (tile_sides_per_warp * warp_idx) + tile_side_offset;
#ifdef TCALC_IS_SINGLE
    pos = loadTileCoordinates(pos, 0, nbwu_map, poly_psw.xcrd, &gmem_r.xcrd[EXCL_GMEM_OFFSET],
                              sh_tile_xcog, poly_psw.gpos_scale_f);
    pos = loadTileCoordinates(pos, 1, nbwu_map, poly_psw.ycrd, &gmem_r.ycrd[EXCL_GMEM_OFFSET],
                              sh_tile_ycog, poly_psw.gpos_scale_f);
    pos = loadTileCoordinates(pos, 2, nbwu_map, poly_psw.zcrd, &gmem_r.zcrd[EXCL_GMEM_OFFSET],
                              sh_tile_zcog, poly_psw.gpos_scale_f);
#else
    pos = loadTileCoordinates(pos, 0, nbwu_map, poly_psw.xcrd, &gmem_r.xcrd[EXCL_GMEM_OFFSET],
                              poly_psw.xcrd_ovrf, &gmem_r.xcrd_ovrf[EXCL_GMEM_OFFSET],
                              sh_tile_xcog, poly_psw.gpos_scale);
    pos = loadTileCoordinates(pos, 1, nbwu_map, poly_psw.ycrd, &gmem_r.ycrd[EXCL_GMEM_OFFSET],
                              poly_psw.ycrd_ovrf, &gmem_r.ycrd_ovrf[EXCL_GMEM_OFFSET],
                              sh_tile_ycog, poly_psw.gpos_scale);
    pos = loadTileCoordinates(pos, 2, nbwu_map, poly_psw.zcrd, &gmem_r.zcrd[EXCL_GMEM_OFFSET],
                              poly_psw.zcrd_ovrf, &gmem_r.zcrd_ovrf[EXCL_GMEM_OFFSET],
                              sh_tile_zcog, poly_psw.gpos_scale);
#endif
    // Use the charge and Lennard-Jones index arrays of the thread-block exclusive workspace to
    // store the baseline GB radii and, if applicable, the neck GB indices.
    pos = loadTileProperty(pos, 3, nbwu_map, poly_nbk.pb_radii, -poly_nbk.gb_offset,
                           &gmem_r.charges[EXCL_GMEM_OFFSET]);
    pos = loadTileProperty(pos, 4, nbwu_map, poly_nbk.neck_gb_idx,
                           &gmem_r.lj_idx[EXCL_GMEM_OFFSET]);
    const int import_count = nbwu_map[0];
    const int padded_import_count = devcRoundUp(import_count, tile_sides_per_warp);
    const int warps_per_block = blockDim.x >> warp_bits;
    const int tile_lane_idx = (threadIdx.x & tile_length_bits_mask);
    while (pos < 6 * padded_import_count) {
      const int rel_pos = pos - (5 * padded_import_count);
      if (rel_pos < import_count) {
        const size_t read_idx = (size_t)(nbwu_map[rel_pos + 1] + tile_lane_idx);
        const size_t write_idx = (size_t)((rel_pos * tile_length) + tile_lane_idx +
                                          EXCL_GMEM_OFFSET);
      }
      pos += tile_sides_per_warp * warps_per_block;
    }
#  ifdef SPLIT_FORCE_ACCUMULATION
#  else
#  endif
    __syncthreads();


    
    // Increment the work unit counter
    __syncthreads();
    if (threadIdx.x == 0) {
      const size_t prog_counter_idx = (ctrl.step & twice_warp_bits_mask_int);
      gbdwu_idx = atomicAdd(&ctrl.gbdwu_progress[prog_counter_idx], 1);
    }
    __syncthreads();
  }
}

#undef EXCL_GMEM_OFFSET
