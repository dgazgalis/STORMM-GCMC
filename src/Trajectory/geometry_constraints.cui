// -*-c++-*-
#include "copyright.h"

// Define the block's atom capacity, if this code is not included in another block where it is
// already defined.  See Structure/valence_potential.cui for more details.
#ifdef CONSTRAINT_STANDALONE
#  define EXCL_GMEM_OFFSET  (blockIdx.x * gmem_r.max_atoms)
#  ifdef TCALC_IS_SINGLE
#    if INTEG_KERNEL_THREAD_COUNT == 64
#      define VALENCE_ATOM_CAPACITY eighth_valence_work_unit_atoms
#    elif INTEG_KERNEL_THREAD_COUNT <= 128
#      define VALENCE_ATOM_CAPACITY quarter_valence_work_unit_atoms
#    elif INTEG_KERNEL_THREAD_COUNT <= 256
#      define VALENCE_ATOM_CAPACITY half_valence_work_unit_atoms
#    else
#      define VALENCE_ATOM_CAPACITY maximum_valence_work_unit_atoms
#    endif
#  else
#    define VALENCE_ATOM_CAPACITY maximum_valence_work_unit_atoms
#  endif
#endif

/// \brief The velocity update kernel will handle import of critical information (atom positions as
///        were used to compute forces).  Both the standalone kernel and includable code will then
///        process the development particle velocities to ensure that they are orthogonal to the
///        displacement along the constrained bonds, to within the tolerance.  The kernel will
///        finish by updating velocities according to each work unit's responsibilities.
#ifdef CONSTRAINT_STANDALONE
__global__ void __launch_bounds__(INTEG_KERNEL_THREAD_COUNT, INTEG_BLOCK_MULTIPLICITY)
KERNEL_NAME(const SyValenceKit<TCALC> poly_vk, MMControlKit<TCALC> ctrl,
            PsSynthesisWriter poly_psw, const SyAtomUpdateKit<TCALC, TCALC2, TCALC4> poly_auk,
            const ThermostatWriter<TCALC> tstw, CacheResourceKit<TCALC> gmem_r) {

  // Arrays named sh_xfrc and the like will hold the velocities to facilitate incorporation of the
  // core code into the valence kernels.
#  ifdef SPLIT_FORCE_ACCUMULATION
#    ifdef TCALC_IS_SINGLE
  __shared__ int sh_xfrc[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_yfrc[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_zfrc[VALENCE_ATOM_CAPACITY];
#    else
  __shared__ llint sh_xfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_yfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_zfrc[VALENCE_ATOM_CAPACITY];
#    endif
  __shared__ int sh_xfrc_overflow[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_yfrc_overflow[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_zfrc_overflow[VALENCE_ATOM_CAPACITY];
#  else
  // As with the valence kernels, not having a definition of split force accumulation implies that
  // 64-bit signed integer accumulation is active in single-precision calculation mode.
  __shared__ llint sh_xfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_yfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_zfrc[VALENCE_ATOM_CAPACITY];
#  endif

  // Allocate space for storing the reference displacements for SETTLE group working frames.  Such
  // information is best stored in a dedicated array to avoid polluting other atomic coordinate
  // arrays, which may (and, often are) repurposed by other processes.
  __shared__ llint stash_xref[INTEG_KERNEL_THREAD_COUNT >> 2];
  __shared__ llint stash_yref[INTEG_KERNEL_THREAD_COUNT >> 2];
  __shared__ llint stash_zref[INTEG_KERNEL_THREAD_COUNT >> 2];
#  ifdef TCALC_IS_SINGLE
  // In single-precision mode, the valence kernel (which can include the core of this code) will
  // allocate arrays for particle positions in its __shared__ partition of L1.  In double-precision
  // mode, use the cache thread block resources.  These positions will become the reference as
  // the force arrays (which are held in __shared__ no matter the valence kernel configuration) are
  // used to iterate and refine the developing positions.
  __shared__ llint sh_xcrd[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_ycrd[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_zcrd[VALENCE_ATOM_CAPACITY];
#  else
  __shared__ int stash_xref_ovrf[INTEG_KERNEL_THREAD_COUNT >> 2];
  __shared__ int stash_yref_ovrf[INTEG_KERNEL_THREAD_COUNT >> 2];
  __shared__ int stash_zref_ovrf[INTEG_KERNEL_THREAD_COUNT >> 2];
  __shared__ int sh_atom_ljidx[VALENCE_ATOM_CAPACITY];
#  endif
  __shared__ int2 vwu_map[vwu_abstract_length];
  __shared__ int vwu_task_count[vwu_abstract_length];
  __shared__ int vwu_padded_task_count[vwu_abstract_length];
  __shared__ volatile int vwu_idx;
  __shared__ volatile TCALC rtoldt;
  
  // Each block takes its first valence work unit based on its block index.
  if (threadIdx.x == 0) {
    vwu_idx = blockIdx.x;
  }
  __syncthreads();
  while (vwu_idx < poly_vk.nvwu) {

    // The instruction set map is read and stored in __shared__ for convenience, and to ensure
    // that it never leaves cache.  The instructions themselves are "streamed," which for purposes
    // of this documentation means read from global, used once, and not cached.  Each block must be
    // at least vwu_abstract_length in size.
    if (threadIdx.x < vwu_abstract_length) {
      vwu_map[threadIdx.x] = __ldcv(&poly_vk.vwu_abstracts[(vwu_idx * vwu_abstract_length) +
                                                           threadIdx.x]);
      vwu_task_count[threadIdx.x] = vwu_map[threadIdx.x].y - vwu_map[threadIdx.x].x;
      vwu_padded_task_count[threadIdx.x] = devcRoundUp(vwu_task_count[threadIdx.x], warp_size_int);
    }
    __syncthreads();
    
    // Import the updated atomic coordinates.  This employs all threads of the block, breaking up
    // each set of information at the warp level.  Once the position update has occurred, forces
    // are no longer relevant and the arrays can thus be repurposed.  If the core of this kernel is
    // called in the context of a valence work unit kernel, the naive positions update will have
    // been stored in the most proximate arrays for the particle positions to act as a reference
    // while the force arrays are used to iterate those positions.  Replicate that setup for the
    // standalone kernel as the coordinates are taken in from global memory.
    const int impt_llim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].x;
    const int impt_hlim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].y;
    const int impt_count  = impt_hlim - impt_llim;
    const int impt_stride = devcRoundUp(impt_hlim - impt_llim, warp_size_int);
    int pos = threadIdx.x;
    while (pos < impt_stride) {
      if (pos < impt_count) {
        const size_t global_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + pos]);
        const size_t write_idx = pos + EXCL_GMEM_OFFSET;
        const llint x_update = __ldcv(&poly_psw.xalt[global_read_idx]);
#  ifdef TCALC_IS_SINGLE
        sh_xcrd[pos]     = __ldcv(&poly_psw.xcrd[global_read_idx]);
#    ifdef SPLIT_FORCE_ACCUMULATION
        const int2 x_tmp = longlongToInt63(x_update);
        sh_xfrc[pos]          = x_tmp.x;
        sh_xfrc_overflow[pos] = x_tmp.y;
#    else
        sh_xfrc[pos]          = __ldca(&poly_psw.xalt[global_read_idx]);
#    endif
        // Re-initialize the atomic Lennard-Jones parameter array to zero to prepare for marking
        // whether each particle was subjected to constraints.  This will become relevant for the
        // final particle velocity adjustment.  Once particles are moved, there is no need to know
        // their Lennard-Jones parameters anymore.
        __stwb(&gmem_r.lj_idx[write_idx], 0);
#  else
        const int x_update_ovrf = __ldcv(&poly_psw.xalt_ovrf[global_read_idx]);
        __stwb(&gmem_r.xcrd[write_idx], __ldcv(&poly_psw.xcrd[global_read_idx]));
        __stwb(&gmem_r.xcrd_ovrf[write_idx], __ldcv(&poly_psw.xcrd_ovrf[global_read_idx]));
        sh_xfrc[pos]          = x_update;
        sh_xfrc_overflow[pos] = x_update_ovrf;

        // In double-precision mode, the atomic Lennard-Jones parameter array is held in __shared__
        // memory to best utilize the space (a 64kB allocation must be made for other __shared__
        // memory arrays and cannot be used for other L1 caching, so putting this array in
        // __shared__ is essentially free).
        sh_atom_ljidx[pos] = 0;
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 2 * impt_stride) {
      const int rel_pos = pos - impt_stride;
      if (rel_pos < impt_count) {
        const size_t global_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + rel_pos]);
        const llint y_update = __ldcv(&poly_psw.yalt[global_read_idx]);
#  ifdef TCALC_IS_SINGLE
        sh_ycrd[rel_pos] = __ldcv(&poly_psw.ycrd[global_read_idx]);
#    ifdef SPLIT_FORCE_ACCUMULATION
        const int2 y_tmp = longlongToInt63(y_update);
        sh_yfrc[rel_pos]          = y_tmp.x;
        sh_yfrc_overflow[rel_pos] = y_tmp.y;
#    else
        sh_yfrc[rel_pos]          = __ldca(&poly_psw.yalt[global_read_idx]);
#    endif
#  else
        const size_t write_idx = rel_pos + EXCL_GMEM_OFFSET;
        const int y_update_ovrf = __ldcv(&poly_psw.yalt_ovrf[global_read_idx]);
        __stwb(&gmem_r.ycrd[write_idx], __ldcv(&poly_psw.ycrd[global_read_idx]));
        __stwb(&gmem_r.ycrd_ovrf[write_idx], __ldcv(&poly_psw.ycrd_ovrf[global_read_idx]));
        sh_yfrc[rel_pos]          = y_update;
        sh_yfrc_overflow[rel_pos] = y_update_ovrf;
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 3 * impt_stride) {
      const int rel_pos = pos - (2 * impt_stride);
      if (rel_pos < impt_count) {
        const size_t global_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + rel_pos]);
        const llint z_update = __ldcv(&poly_psw.zalt[global_read_idx]);
#  ifdef TCALC_IS_SINGLE
        sh_zcrd[rel_pos] = __ldcv(&poly_psw.zcrd[global_read_idx]);
#    ifdef SPLIT_FORCE_ACCUMULATION
        const int2 z_tmp = longlongToInt63(z_update);
        sh_zfrc[rel_pos]          = z_tmp.x;
        sh_zfrc_overflow[rel_pos] = z_tmp.y;
#    else
        sh_zfrc[rel_pos]          = __ldca(&poly_psw.zalt[global_read_idx]);
#    endif
#  else
        const size_t write_idx = rel_pos + EXCL_GMEM_OFFSET;
        const int z_update_ovrf = __ldcv(&poly_psw.zalt_ovrf[global_read_idx]);
        __stwb(&gmem_r.zcrd[write_idx], __ldcv(&poly_psw.zcrd[global_read_idx]));
        __stwb(&gmem_r.zcrd_ovrf[write_idx], __ldcv(&poly_psw.zcrd_ovrf[global_read_idx]));
        sh_zfrc[rel_pos]          = z_update;
        sh_zfrc_overflow[rel_pos] = z_update_ovrf;
#  endif
      }
      pos += blockDim.x;
    }
    __syncthreads();
#else  // CONSTRAINT_STANDALONE

    // The positions update will have placed the developing particle locations in the force arrays
    // in __shared__ memory, where they are most accessible for iterative refinement.  Thread
    // synchronization will have already been applied.  However, one more organizational step is
    // critical: the Lennard-Jones index array, which is no longer needed by the time particles
    // have been moved, must be repurposed to record whether particles are part of a constraint of
    // any sort.  To prepare for this, its elements for each atom were re-initialized to zero.  The
    // elements will be set to one if the corresponding atoms are part of a constraint.

#endif // CONSTRAINT_STANDALONE
    
    pos = ((threadIdx.x >> warp_bits) << warp_bits);
    const int cgrp_lane_idx = (threadIdx.x & warp_bits_mask_int);
#ifdef CONSTRAINT_STANDALONE
    int vterm_limit = vwu_padded_task_count[(size_t)(VwuAbstractMap::CGROUP)];
#else
    vterm_limit = vwu_padded_task_count[(size_t)(VwuAbstractMap::CGROUP)];
#endif
    while (pos < vterm_limit) {
      const int task_offset = vwu_map[(size_t)(VwuAbstractMap::CGROUP)].x;
      uint2 tinsr;
      if (pos + cgrp_lane_idx < vwu_task_count[(size_t)(VwuAbstractMap::CGROUP)]) {
        tinsr = __ldcv(&poly_auk.cnst_insr[task_offset + pos + cgrp_lane_idx]);
      }
      else {
        tinsr = { 0U, 0U };
      }
      
      // The warp must remain coalesced in order to shuffle changes to the central atom position
      // between threads working on the same group.
      TCALC dx_ref, dy_ref, dz_ref;
      int central_atom = (tinsr.x & 0x3ff);
      int peripheral_atom = ((tinsr.x >> 10) & 0x3ff);
      if (central_atom != 0 || peripheral_atom != 0) {
#ifdef TCALC_IS_SINGLE
        dx_ref = (TCALC)(sh_xcrd[peripheral_atom] - sh_xcrd[central_atom]) *
                 poly_psw.inv_gpos_scale_f;
        dy_ref = (TCALC)(sh_ycrd[peripheral_atom] - sh_ycrd[central_atom]) *
                 poly_psw.inv_gpos_scale_f;
        dz_ref = (TCALC)(sh_zcrd[peripheral_atom] - sh_zcrd[central_atom]) *
                 poly_psw.inv_gpos_scale_f;
#else  // TCALC_IS_SINGLE
        central_atom += EXCL_GMEM_OFFSET;
        peripheral_atom += EXCL_GMEM_OFFSET;
        const int95_t idx_ref = int95Subtract(__ldca(&gmem_r.xcrd[peripheral_atom]),
                                              __ldca(&gmem_r.xcrd_ovrf[peripheral_atom]),
                                              __ldca(&gmem_r.xcrd[central_atom]),
                                              __ldca(&gmem_r.xcrd_ovrf[central_atom]));
        const int95_t idy_ref = int95Subtract(__ldca(&gmem_r.ycrd[peripheral_atom]),
                                              __ldca(&gmem_r.ycrd_ovrf[peripheral_atom]),
                                              __ldca(&gmem_r.ycrd[central_atom]),
                                              __ldca(&gmem_r.ycrd_ovrf[central_atom]));
        const int95_t idz_ref = int95Subtract(__ldca(&gmem_r.zcrd[peripheral_atom]),
                                              __ldca(&gmem_r.zcrd_ovrf[peripheral_atom]),
                                              __ldca(&gmem_r.zcrd[central_atom]),
                                              __ldca(&gmem_r.zcrd_ovrf[central_atom]));
        central_atom -= EXCL_GMEM_OFFSET;
        peripheral_atom -= EXCL_GMEM_OFFSET;
        dx_ref = splitFPToReal(idx_ref) * poly_psw.inv_gpos_scale_f;
        dy_ref = splitFPToReal(idy_ref) * poly_psw.inv_gpos_scale_f;
        dz_ref = splitFPToReal(idz_ref) * poly_psw.inv_gpos_scale_f;
#endif // TCALC_IS_SINGLE

        // Having computed the reference positional delta, the "old" positions of particles are no
        // longer relevant.  Store the positions after the naive update in the "reference"
        // positions array, whether in __shared__ or the thread-block specific cache space.
#ifdef TCALC_IS_SINGLE
        __stwb(&gmem_r.lj_idx[peripheral_atom + EXCL_GMEM_OFFSET], 1);
#  ifdef SPLIT_FORCE_ACCUMULATION
        const llint ph_xcurr_tmp = int63ToLongLong(sh_xfrc[peripheral_atom],
                                                   sh_xfrc_overflow[peripheral_atom]);
        const llint ph_ycurr_tmp = int63ToLongLong(sh_yfrc[peripheral_atom],
                                                   sh_yfrc_overflow[peripheral_atom]);
        const llint ph_zcurr_tmp = int63ToLongLong(sh_zfrc[peripheral_atom],
                                                   sh_zfrc_overflow[peripheral_atom]);
        sh_xcrd[peripheral_atom] = ph_xcurr_tmp;
        sh_ycrd[peripheral_atom] = ph_ycurr_tmp;
        sh_zcrd[peripheral_atom] = ph_zcurr_tmp;
#  else
        sh_xcrd[peripheral_atom] = sh_xfrc[peripheral_atom];
        sh_ycrd[peripheral_atom] = sh_yfrc[peripheral_atom];
        sh_zcrd[peripheral_atom] = sh_zfrc[peripheral_atom];
#  endif
#else
        sh_atom_ljidx[peripheral_atom] = 1;
        const size_t ph_write_idx = peripheral_atom + EXCL_GMEM_OFFSET;
        __stwb(&gmem_r.xcrd[ph_write_idx], sh_xfrc[peripheral_atom]);
        __stwb(&gmem_r.ycrd[ph_write_idx], sh_yfrc[peripheral_atom]);
        __stwb(&gmem_r.zcrd[ph_write_idx], sh_zfrc[peripheral_atom]);
        __stwb(&gmem_r.xcrd_ovrf[ph_write_idx], sh_xfrc_overflow[peripheral_atom]);
        __stwb(&gmem_r.ycrd_ovrf[ph_write_idx], sh_yfrc_overflow[peripheral_atom]);
        __stwb(&gmem_r.zcrd_ovrf[ph_write_idx], sh_zfrc_overflow[peripheral_atom]);
#endif
        // The lead thread in each group will log the central atom's position
        if ((threadIdx.x & warp_bits_mask_int) == ((tinsr.x >> 20) & 0xff)) {
#ifdef TCALC_IS_SINGLE
          __stwb(&gmem_r.lj_idx[central_atom + EXCL_GMEM_OFFSET], 1);
#  ifdef SPLIT_FORCE_ACCUMULATION
          const llint ca_xcurr_tmp = int63ToLongLong(sh_xfrc[central_atom],
                                                     sh_xfrc_overflow[central_atom]);
          const llint ca_ycurr_tmp = int63ToLongLong(sh_yfrc[central_atom],
                                                     sh_yfrc_overflow[central_atom]);
          const llint ca_zcurr_tmp = int63ToLongLong(sh_zfrc[central_atom],
                                                     sh_zfrc_overflow[central_atom]);
          sh_xcrd[central_atom] = ca_xcurr_tmp;
          sh_ycrd[central_atom] = ca_ycurr_tmp;
          sh_zcrd[central_atom] = ca_zcurr_tmp;
#  else
          sh_xcrd[central_atom] = sh_xfrc[central_atom];
          sh_ycrd[central_atom] = sh_yfrc[central_atom];
          sh_zcrd[central_atom] = sh_zfrc[central_atom];
#  endif
#else
          sh_atom_ljidx[central_atom] = 1;
          const size_t ca_write_idx = central_atom + EXCL_GMEM_OFFSET;
          __stwb(&gmem_r.xcrd[ca_write_idx], sh_xfrc[central_atom]);
          __stwb(&gmem_r.ycrd[ca_write_idx], sh_yfrc[central_atom]);
          __stwb(&gmem_r.zcrd[ca_write_idx], sh_zfrc[central_atom]);
          __stwb(&gmem_r.xcrd_ovrf[ca_write_idx], sh_xfrc_overflow[central_atom]);
          __stwb(&gmem_r.ycrd_ovrf[ca_write_idx], sh_yfrc_overflow[central_atom]);
          __stwb(&gmem_r.zcrd_ovrf[ca_write_idx], sh_zfrc_overflow[central_atom]);
#endif
        }
      }
      else {
        dx_ref = (TCALC)(0.0);
        dy_ref = (TCALC)(0.0);
        dz_ref = (TCALC)(0.0);
      }
      const size_t ca_gbl_idx = __ldca(&poly_vk.vwu_imports[impt_llim + central_atom]);
      const size_t ph_gbl_idx = __ldca(&poly_vk.vwu_imports[impt_llim + peripheral_atom]);
      const TCALC ca_invmass = poly_auk.inv_masses[ca_gbl_idx] * poly_psw.gpos_scale_f;
      const TCALC ph_invmass = poly_auk.inv_masses[ph_gbl_idx] * poly_psw.gpos_scale_f;
      const TCALC cbm_factor = (TCALC)(0.6) / poly_auk.cnst_grp_params[tinsr.y].y;

      // "Blank" instructions will be counted as naturally converged, as the positional delta
      // between atoms 0 and 0 will be zero.
      bool converged = false;
      int iter = 0;
      while ((! converged) && iter < tstw.rattle_iter) {
        converged = true;
#ifdef SPLIT_FORCE_ACCUMULATION
#  ifdef TCALC_IS_SINGLE
        const int2 idx = int63Subtract(sh_xfrc[peripheral_atom], sh_xfrc_overflow[peripheral_atom],
                                       sh_xfrc[central_atom], sh_xfrc_overflow[central_atom]);
        const int2 idy = int63Subtract(sh_yfrc[peripheral_atom], sh_yfrc_overflow[peripheral_atom],
                                       sh_yfrc[central_atom], sh_yfrc_overflow[central_atom]);
        const int2 idz = int63Subtract(sh_zfrc[peripheral_atom], sh_zfrc_overflow[peripheral_atom],
                                       sh_zfrc[central_atom], sh_zfrc_overflow[central_atom]);
#  else
        const int95_t idx = int95Subtract(sh_xfrc[peripheral_atom],
                                          sh_xfrc_overflow[peripheral_atom],
                                          sh_xfrc[central_atom], sh_xfrc_overflow[central_atom]);
        const int95_t idy = int95Subtract(sh_yfrc[peripheral_atom],
                                          sh_yfrc_overflow[peripheral_atom],
                                          sh_yfrc[central_atom], sh_yfrc_overflow[central_atom]);
        const int95_t idz = int95Subtract(sh_zfrc[peripheral_atom],
                                          sh_zfrc_overflow[peripheral_atom],
                                          sh_zfrc[central_atom], sh_zfrc_overflow[central_atom]);
#  endif
        const TCALC dx = splitFPToReal(idx) * poly_psw.inv_gpos_scale_f;
        const TCALC dy = splitFPToReal(idy) * poly_psw.inv_gpos_scale_f;
        const TCALC dz = splitFPToReal(idz) * poly_psw.inv_gpos_scale_f;
#else  // SPLIT_FORCE_ACCUMULATION
        const TCALC dx = (TCALC)(sh_xfrc[peripheral_atom] - sh_xfrc[central_atom]) *
                         poly_psw.inv_gpos_scale_f;
        const TCALC dy = (TCALC)(sh_yfrc[peripheral_atom] - sh_yfrc[central_atom]) *
                         poly_psw.inv_gpos_scale_f;
        const TCALC dz = (TCALC)(sh_zfrc[peripheral_atom] - sh_zfrc[central_atom]) *
                         poly_psw.inv_gpos_scale_f;
#endif // SPLIT_FORCE_ACCUMULATION
        const TCALC r2 = (dx * dx) + (dy * dy) + (dz * dz);
        const TCALC delta = poly_auk.cnst_grp_params[tinsr.y].x - r2;
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
        int2 cax_update, cay_update, caz_update;
#  else
        llint cax_update, cay_update, caz_update;
#  endif
#else
        int95_t cax_update, cay_update, caz_update;
#endif
        if ((central_atom != 0 || peripheral_atom != 0) && ABS_FUNC(delta) > tstw.rattle_tol) {
          converged = false;
          const TCALC dot_disp = (dx * dx_ref) + (dy * dy_ref) + (dz * dz_ref);
          const TCALC term = cbm_factor * delta / dot_disp;
#ifdef SPLIT_FORCE_ACCUMULATION
#  ifdef TCALC_IS_SINGLE
          // The position scaling factor was folded into the inverse mass quantity (see above)
          const int2 phx_update = floatToInt63(dx_ref * term * ph_invmass);
          const int2 phy_update = floatToInt63(dy_ref * term * ph_invmass);
          const int2 phz_update = floatToInt63(dz_ref * term * ph_invmass);
          const int2 nph_x = splitFPSum(phx_update, sh_xfrc[peripheral_atom],
                                        sh_xfrc_overflow[peripheral_atom]);
          const int2 nph_y = splitFPSum(phy_update, sh_yfrc[peripheral_atom],
                                        sh_yfrc_overflow[peripheral_atom]);
          const int2 nph_z = splitFPSum(phz_update, sh_zfrc[peripheral_atom],
                                        sh_zfrc_overflow[peripheral_atom]);
          cax_update = floatToInt63(-dx_ref * term * ca_invmass);
          cay_update = floatToInt63(-dy_ref * term * ca_invmass);
          caz_update = floatToInt63(-dz_ref * term * ca_invmass);
#  else
          const int95_t phx_update = doubleToInt95(dx_ref * term * ph_invmass);
          const int95_t phy_update = doubleToInt95(dy_ref * term * ph_invmass);
          const int95_t phz_update = doubleToInt95(dz_ref * term * ph_invmass);
          const int95_t nph_x = splitFPSum(phx_update, sh_xfrc[peripheral_atom],
                                           sh_xfrc_overflow[peripheral_atom]);
          const int95_t nph_y = splitFPSum(phy_update, sh_yfrc[peripheral_atom],
                                           sh_yfrc_overflow[peripheral_atom]);
          const int95_t nph_z = splitFPSum(phz_update, sh_zfrc[peripheral_atom],
                                           sh_zfrc_overflow[peripheral_atom]);
          cax_update = doubleToInt95(-dx_ref * term * ca_invmass);
          cay_update = doubleToInt95(-dy_ref * term * ca_invmass);
          caz_update = doubleToInt95(-dz_ref * term * ca_invmass);
#  endif
          sh_xfrc[peripheral_atom] = nph_x.x;
          sh_yfrc[peripheral_atom] = nph_y.x;
          sh_zfrc[peripheral_atom] = nph_z.x;
          sh_xfrc_overflow[peripheral_atom] = nph_x.y;
          sh_yfrc_overflow[peripheral_atom] = nph_y.y;
          sh_zfrc_overflow[peripheral_atom] = nph_z.y;
#else
          sh_xfrc[peripheral_atom] += LLCONV_FUNC(dx_ref * term * ph_invmass);
          sh_yfrc[peripheral_atom] += LLCONV_FUNC(dy_ref * term * ph_invmass);
          sh_zfrc[peripheral_atom] += LLCONV_FUNC(dz_ref * term * ph_invmass);
          cax_update = LLCONV_FUNC(-dx_ref * term * ca_invmass);
          cay_update = LLCONV_FUNC(-dy_ref * term * ca_invmass);
          caz_update = LLCONV_FUNC(-dz_ref * term * ca_invmass);
#endif
        }
        else {
#ifdef SPLIT_FORCE_ACCUMULATION
#  ifdef TCALC_IS_SINGLE
          cax_update = { 0, 0 };
          cay_update = { 0, 0 };
          caz_update = { 0, 0 };
#  else
          cax_update = { 0LL, 0 };
          cay_update = { 0LL, 0 };
          caz_update = { 0LL, 0 };
#  endif
#else
          cax_update = 0LL;
          cay_update = 0LL;
          caz_update = 0LL;
#endif
        }
        
        // The peripheral atom positions may be left as they are, if the calculation has converged.
        // Reduce the moves on the central atoms.
        const bool leader_lane = (cgrp_lane_idx == ((tinsr.x >> 20) & 0xff));
        const int spoke_count = (tinsr.x >> 28);
        for (int i = 1; i < poly_auk.largest_group; i++) {
#ifdef SPLIT_FORCE_ACCUMULATION
#  ifdef TCALC_IS_SINGLE
          int2 ncax, ncay, ncaz;
#  else
          int95_t ncax, ncay, ncaz;
#  endif
          ncax.x = SHFL(cax_update.x, cgrp_lane_idx + i);
          ncax.y = SHFL(cax_update.y, cgrp_lane_idx + i);
          ncay.x = SHFL(cay_update.x, cgrp_lane_idx + i);
          ncay.y = SHFL(cay_update.y, cgrp_lane_idx + i);
          ncaz.x = SHFL(caz_update.x, cgrp_lane_idx + i);
          ncaz.y = SHFL(caz_update.y, cgrp_lane_idx + i);
#else
          const llint ncax = SHFL(cax_update, cgrp_lane_idx + i);
          const llint ncay = SHFL(cay_update, cgrp_lane_idx + i);
          const llint ncaz = SHFL(caz_update, cgrp_lane_idx + i);
#endif
          if (leader_lane && i < spoke_count) {
#ifdef SPLIT_FORCE_ACCUMULATION
            cax_update = splitFPSum(cax_update, ncax);
            cay_update = splitFPSum(cay_update, ncay);
            caz_update = splitFPSum(caz_update, ncaz);
#else
            cax_update += ncax;
            cay_update += ncay;
            caz_update += ncaz;
#endif
          }
        }

        // Check again for blank instructions
        if (central_atom != 0 || peripheral_atom != 0) {

          // The leader lane will update the central atom's position.
          if (leader_lane) {
#ifdef SPLIT_FORCE_ACCUMULATION
            cax_update = splitFPSum(cax_update,
                                    sh_xfrc[central_atom], sh_xfrc_overflow[central_atom]);
            cay_update = splitFPSum(cay_update,
                                    sh_yfrc[central_atom], sh_yfrc_overflow[central_atom]);
            caz_update = splitFPSum(caz_update,
                                    sh_zfrc[central_atom], sh_zfrc_overflow[central_atom]);
            sh_xfrc[central_atom] = cax_update.x;
            sh_yfrc[central_atom] = cay_update.x;
            sh_zfrc[central_atom] = caz_update.x;
            sh_xfrc_overflow[central_atom] = cax_update.y;
            sh_yfrc_overflow[central_atom] = cay_update.y;
            sh_zfrc_overflow[central_atom] = caz_update.y;
#else
            sh_xfrc[central_atom] += cax_update;
            sh_yfrc[central_atom] += cay_update;
            sh_zfrc[central_atom] += caz_update;
#endif
          }
        }

        // Test convergence across the whole warp.  The implicit warp synchronization in this
        // call will also ensure that the __shared__ memory array updates are ready if convergence
        // is not complete.
        converged = (BALLOT(! converged) == 0x0);
        iter++;
      }
      pos += blockDim.x;
    }

    // Implement SETTLE position constraints
//#ifdef PME_COMPATIBLE
    vterm_limit += (vwu_padded_task_count[(size_t)(VwuAbstractMap::SETTLE)] << 2);
    while (pos < vterm_limit) {
      const int task_offset = vwu_map[(size_t)(VwuAbstractMap::SETTLE)].x;
      uint2 tinsr;
      const int rel_pos = ((pos + cgrp_lane_idx -
                            vwu_padded_task_count[(size_t)(VwuAbstractMap::CGROUP)]) >> 2);

      // The warp must remain coalesced in order to carry out shuffle instructions later on.
      if (rel_pos < vwu_task_count[(size_t)(VwuAbstractMap::SETTLE)]) {
        tinsr = __ldcv(&poly_auk.sett_insr[task_offset + rel_pos]);
      }
      else {
        tinsr = { 0U, 0xffffffffU };
      }

      // The developer is referred to the diagram in settleGroupPosition() of the template
      // implementation file src/Structure/settle.tpp.  There are several critical groups of
      // three-element vectors to store, including the local representations of the heavy atom
      // (oxy) and two light atoms (hd1 and hd2) in a coordinate system where the three atoms'
      // center of mass is the origin, the reference oxy => hd1 and oxy => hd2 displacements,
      // the primed coordinate axes Px, Py, and Pz shown in the diagram, and the canonical
      // coordinates.  The adjusted coordinates finally emerge from a combination of the local
      // coordinates and the canonical coordinates, then get put back in the original coordinate
      // system by rotating with the transpose of the primed axes and translating with the center
      // of mass, which can be recalculated at the end of the computation by referencing the
      // original and updated coordinates which remain in various L1 arrays throughout the SETTLE
      // implementation.  This creates a set of eleven three-element vectors to distribute amongst
      // the four threads in each group carrying out a SETTLE instruction:
      //
      // Thread 0  [ oxy local X Y Z ] [ prime i^ X Y Z ] [ oxy canonical X Y Z ]
      // Thread 1  [ hd1 local X Y Z ] [ prime j^ X Y Z ] [ hd1 canonical X Y Z ]
      // Thread 2  [ hd2 local X Y Z ] [ prime k^ X Y Z ] [ hd2 canonical X Y Z ]
      // Thread 3  [ ref oxy => hd1 X Y Z ] [ ref oxy => hd2 X Y Z ]
      //
      // Midway through the calculation, the fourth thread will be instrumental in computing the
      // the alpha, beta, and gamma angles and then sines and cosines of the three Euler angles
      // phi, psi, and theta used to compute the adjusted coordinates.
      int my_choice = (cgrp_lane_idx & 0x3);
      my_choice -= 3 * (my_choice == 3);
      const int atm_idx = ((tinsr.x >> (10 * my_choice)) & 0x3ff);
      const int oxy_idx = (tinsr.x & 0x3ff);
#ifdef TCALC_IS_SINGLE

      // Mark the atom as having been constrained, so that its velocity may be updated as
      // appropriate along with SHAKE-constrained atoms.
      if (tinsr.y != 0xffffffffU) {
        __stwb(&gmem_r.lj_idx[atm_idx + EXCL_GMEM_OFFSET], 1);
      }
#  ifdef SPLIT_FORCE_ACCUMULATION
      TCALC3 lpos = {
        (TCALC)(int63ToLongLong(sh_xfrc[atm_idx], sh_xfrc_overflow[atm_idx]) - sh_xcrd[oxy_idx]),
        (TCALC)(int63ToLongLong(sh_yfrc[atm_idx], sh_yfrc_overflow[atm_idx]) - sh_ycrd[oxy_idx]),
        (TCALC)(int63ToLongLong(sh_zfrc[atm_idx], sh_zfrc_overflow[atm_idx]) - sh_zcrd[oxy_idx])
      };
#  else
      TCALC3 lpos = { (TCALC)(sh_xfrc[atm_idx] - sh_xcrd[oxy_idx]),
                      (TCALC)(sh_yfrc[atm_idx] - sh_ycrd[oxy_idx]),
                      (TCALC)(sh_zfrc[atm_idx] - sh_zcrd[oxy_idx]) };      
#  endif
#else // TCALC_IS_SINGLE
      if (tinsr.y != 0xffffffffU) {
        sh_atom_ljidx[atm_idx] = 1;
      }
      TCALC3 lpos;
#endif // TCALC_IS_SINGLE
      const int base_lane = ((cgrp_lane_idx >> 2) << 2);
      TCALC frac_mo, frac_mh;
      if (tinsr.y != 0xffffffffU) {
        const TCALC4 smass = poly_auk.settle_mass[tinsr.y];
        frac_mo = smass.z;
        frac_mh = smass.w;
      }
      else {
        frac_mo = (TCALC)(1.0);
        frac_mh = (TCALC)(1.0);
      }
#ifdef TCALC_IS_SINGLE
      TCALC3 lcom = { (SHFL(lpos.x, base_lane) * frac_mo) +
                      ((SHFL(lpos.x, base_lane + 1) + SHFL(lpos.x, base_lane + 2)) * frac_mh),
                      (SHFL(lpos.y, base_lane) * frac_mo) +
                      ((SHFL(lpos.y, base_lane + 1) + SHFL(lpos.y, base_lane + 2)) * frac_mh),
                      (SHFL(lpos.z, base_lane) * frac_mo) +
                      ((SHFL(lpos.z, base_lane + 1) + SHFL(lpos.z, base_lane + 2)) * frac_mh) };
#else // TCALC_IS_SINGLE
      TCALC3 lcom;
#endif // TCALC_IS_SINGLE

      // Move the local coordinates to the center of mass.  Let the first lane add the center of
      // mass to the oxy atom's coordinates so that it may be implicitly accounted for when
      // returning the system to its original place.  The above shuffle operations will ensure that
      // prior references to the original oxy atom's position by other threads in the warp are
      // complete.  SETTLE and other constraint operations cannot have combined effects on the same
      // atoms.  The local coordinates, and hence the center of mass, have not been scaled down
      // from the fixed-precision scaling so that the center of mass can be added directly to the
      // original oxy atom position.
      lpos.x -= lcom.x;
      lpos.y -= lcom.y;
      lpos.z -= lcom.z;

      // The fourth thread holds the oxygen atom's local coordinates.  While it is a "different
      // take" on the same displacement, any deviation between the values calculated by the first
      // and fourth threads is a well-bounded error.  Compute the oxy => hd2 displacement and
      // stash it in the fourth thread's cpos (canonical position) tuple.  Compute the oxy => hd1
      // displacement and stash it in the fourth thread's lpos tuple.
      TCALC3 prime_v, cpos;
      if (cgrp_lane_idx == base_lane + 3) {
        const int hd1_idx = ((tinsr.x >> 10) & 0x3ff);
        const int hd2_idx = ((tinsr.x >> 20) & 0x3ff);
#ifdef TCALC_IS_SINGLE
        lpos  = { (TCALC)(sh_xcrd[hd1_idx] - sh_xcrd[oxy_idx]),
                  (TCALC)(sh_ycrd[hd1_idx] - sh_ycrd[oxy_idx]),
                  (TCALC)(sh_zcrd[hd1_idx] - sh_zcrd[oxy_idx]) };
        cpos  = { (TCALC)(sh_xcrd[hd2_idx] - sh_xcrd[oxy_idx]) * poly_psw.inv_gpos_scale_f,
                  (TCALC)(sh_ycrd[hd2_idx] - sh_ycrd[oxy_idx]) * poly_psw.inv_gpos_scale_f,
                  (TCALC)(sh_zcrd[hd2_idx] - sh_zcrd[oxy_idx]) * poly_psw.inv_gpos_scale_f };
#else
        const int local_oxy_idx = oxy_idx + EXCL_GMEM_OFFSET;
        const int local_hd1_idx = hd1_idx + EXCL_GMEM_OFFSET;
        const int local_hd2_idx = hd2_idx + EXCL_GMEM_OFFSET;
        lpos  = { splitFPToReal(int95Subtract(__ldca(&gmem_r.xcrd[local_hd1_idx]),
                                              __ldca(&gmem_r.xcrd_ovrf[local_hd1_idx]),
                                              __ldca(&gmem_r.xcrd[local_oxy_idx]),
                                              __ldca(&gmem_r.xcrd_ovrf[local_oxy_idx]))),
                  splitFPToReal(int95Subtract(__ldca(&gmem_r.ycrd[local_hd1_idx]),
                                              __ldca(&gmem_r.ycrd_ovrf[local_hd1_idx]),
                                              __ldca(&gmem_r.ycrd[local_oxy_idx]),
                                              __ldca(&gmem_r.ycrd_ovrf[local_oxy_idx]))),
                  splitFPToReal(int95Subtract(__ldca(&gmem_r.zcrd[local_hd1_idx]),
                                              __ldca(&gmem_r.zcrd_ovrf[local_hd1_idx]),
                                              __ldca(&gmem_r.zcrd[local_oxy_idx]),
                                              __ldca(&gmem_r.zcrd_ovrf[local_oxy_idx]))) };
        cpos  = { splitFPToReal(int95Subtract(__ldca(&gmem_r.xcrd[local_hd2_idx]),
                                              __ldca(&gmem_r.xcrd_ovrf[local_hd2_idx]),
                                              __ldca(&gmem_r.xcrd[local_oxy_idx]),
                                              __ldca(&gmem_r.xcrd_ovrf[local_oxy_idx]))) *
                  poly_psw.inv_gpos_scale_f,
                  splitFPToReal(int95Subtract(__ldca(&gmem_r.ycrd[local_hd2_idx]),
                                              __ldca(&gmem_r.ycrd_ovrf[local_hd2_idx]),
                                              __ldca(&gmem_r.ycrd[local_oxy_idx]),
                                              __ldca(&gmem_r.ycrd_ovrf[local_oxy_idx]))) *
                  poly_psw.inv_gpos_scale_f,
                  splitFPToReal(int95Subtract(__ldca(&gmem_r.zcrd[local_hd2_idx]),
                                              __ldca(&gmem_r.zcrd_ovrf[local_hd2_idx]),
                                              __ldca(&gmem_r.zcrd[local_oxy_idx]),
                                              __ldca(&gmem_r.zcrd_ovrf[local_oxy_idx]))) *
                  poly_psw.inv_gpos_scale_f };
#endif
      }
      
      // Synchronize to protect the local coordinate accesses before the oxygen atom's original
      // coordinates are updated with the center of mass.
      SYNCWARP;
      if (cgrp_lane_idx == base_lane && tinsr.y != 0xffffffffU) {
#ifdef TCALC_IS_SINGLE
        stash_xref[threadIdx.x >> 2] = sh_xcrd[atm_idx] + LLCONV_FUNC(lcom.x);
        stash_yref[threadIdx.x >> 2] = sh_ycrd[atm_idx] + LLCONV_FUNC(lcom.y);
        stash_zref[threadIdx.x >> 2] = sh_zcrd[atm_idx] + LLCONV_FUNC(lcom.z);
#else // TCALC_IS_SINGLE
#endif // TCALC_IS_SINGLE
      }

      // In the same manner as was done for the hub-and-spoke constraints, substitute the current
      // (unconstrained) particle locations into the "reference" coordinates so that, after a
      // block-wide synchronization, these atoms (which have been marked as part of a constrained
      // group) can have their velocities updated in accord with the extra coordinate movement.
      if (cgrp_lane_idx < base_lane + 3 && tinsr.y != 0xffffffffU) {
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
        const llint xcurr_tmp = int63ToLongLong(sh_xfrc[atm_idx], sh_xfrc_overflow[atm_idx]);
        const llint ycurr_tmp = int63ToLongLong(sh_yfrc[atm_idx], sh_yfrc_overflow[atm_idx]);
        const llint zcurr_tmp = int63ToLongLong(sh_zfrc[atm_idx], sh_zfrc_overflow[atm_idx]);
        sh_xcrd[atm_idx] = xcurr_tmp;
        sh_ycrd[atm_idx] = ycurr_tmp;
        sh_zcrd[atm_idx] = zcurr_tmp;
#  else
        sh_xcrd[atm_idx] = sh_xfrc[atm_idx];
        sh_ycrd[atm_idx] = sh_yfrc[atm_idx];
        sh_zcrd[atm_idx] = sh_zfrc[atm_idx];
#  endif
#else // TCALC_IS_SINGLE
#endif // TCALC_IS_SINGLE
      }

      // Synchronize again to protect the stashed reference frame displacement before other
      // threads will access it at the end of the workflow for the SETTLE instruction.
      SYNCWARP;
      lpos.x *= poly_psw.inv_gpos_scale_f;
      lpos.y *= poly_psw.inv_gpos_scale_f;
      lpos.z *= poly_psw.inv_gpos_scale_f;
      prime_v = crossProduct(lpos, cpos);

      // The prime k^ vector is now in the fourth lane.  Get it to the third lane with shuffle
      // operations.  Doing so, in fact, takes it to all other threads in the group, including
      // the first, which is then ready to compute prime i^.
      prime_v.x = SHFL(prime_v.x, base_lane + 3);
      prime_v.y = SHFL(prime_v.y, base_lane + 3);
      prime_v.z = SHFL(prime_v.z, base_lane + 3);
      if (cgrp_lane_idx == base_lane) {
        prime_v = crossProduct(lpos, prime_v);
      }

      // Communicate the prime i^ vector across the threads in the group.  The second thread in
      // the group has already received prime k^, so placing prime i^ in the placeholder vector
      // ph_vec (necessary to protect prior work in the first and third threads) provides an
      // opportunity to take one final cross product that will create prime j^.
      TCALC3 ph_vec = { SHFL(prime_v.x, base_lane), SHFL(prime_v.y, base_lane),
                        SHFL(prime_v.z, base_lane) };
      if (cgrp_lane_idx == base_lane + 1) {
        prime_v = crossProduct(prime_v, ph_vec);
      }

      // The primed coordinate system vectors must now be normalized.
      const TCALC mag_pv = SQRT_FUNC((prime_v.x * prime_v.x) + (prime_v.y * prime_v.y) +
                                     (prime_v.z * prime_v.z));
      prime_v.x /= mag_pv;
      prime_v.y /= mag_pv;
      prime_v.z /= mag_pv;

      // Rotate the local coordinates into the primed coordinate system.  Each of the first three
      // lanes has one of the atoms' local coordinates and, alongside it, one of the prime
      // coordinate system's vectors.  The fourth thread contains two vectors that need to be
      // rotated into the primed coordinate system, one of which is in the place of the canonical
      // coordinate vector.  Stash this result in the fourth thread's prime coordinate vector as
      // it accumulates.
      ph_vec.x = SHFL(prime_v.x, base_lane);
      ph_vec.y = SHFL(prime_v.y, base_lane);
      ph_vec.z = SHFL(prime_v.z, base_lane);
      const TCALC tmp_px = dot(ph_vec, lpos);
      if (cgrp_lane_idx == base_lane + 3) {
        prime_v.x = dot(ph_vec, cpos);
      }
      ph_vec.x = SHFL(prime_v.x, base_lane + 1);
      ph_vec.y = SHFL(prime_v.y, base_lane + 1);
      ph_vec.z = SHFL(prime_v.z, base_lane + 1);
      const TCALC tmp_py = dot(ph_vec, lpos);
      if (cgrp_lane_idx == base_lane + 3) {
        prime_v.y = dot(ph_vec, cpos);
      }
      ph_vec.x = SHFL(prime_v.x, base_lane + 2);
      ph_vec.y = SHFL(prime_v.y, base_lane + 2);
      ph_vec.z = SHFL(prime_v.z, base_lane + 2);
      lpos.z = dot(ph_vec, lpos);
      lpos.x = tmp_px;
      lpos.y = tmp_py;
      if (cgrp_lane_idx == base_lane + 3) {
        cpos.z = dot(ph_vec, cpos);
        cpos.x = prime_v.x;
        cpos.y = prime_v.y;
      }
      
      // Stash the primed coordinates in the __shared__ memory force arrays for the three atoms.
      // The information in them has already been offloaded (if guaranteed by a warp
      // synchronization), and transferring the material to __shared__ will be helpful for the
      // transposition.
      SYNCWARP;
      TCALC4 sgeom;
      if (tinsr.y != 0xffffffffU) {
        if (cgrp_lane_idx < base_lane + 3) { 
#ifdef TCALC_IS_SINGLE
          sh_xfrc[atm_idx] = __float_as_int(prime_v.x);
          sh_yfrc[atm_idx] = __float_as_int(prime_v.y);
          sh_zfrc[atm_idx] = __float_as_int(prime_v.z);
#else
          sh_xfrc[atm_idx] = __double_as_longlong(prime_v.x);
          sh_yfrc[atm_idx] = __double_as_longlong(prime_v.y);
          sh_zfrc[atm_idx] = __double_as_longlong(prime_v.z);
#endif
        }
        sgeom = poly_auk.settle_geom[tinsr.y];
      }
      
      // Compute the sines and cosines of the first two Euler angles.
      const TCALC ra = sgeom.x;
      const TCALC rb = sgeom.y;
      const TCALC rc = sgeom.z;
      const TCALC sinphi = SHFL(lpos.z, base_lane) / ra;
      const TCALC cosphi = SQRT_FUNC((TCALC)(1.0) - (sinphi * sinphi));
      const TCALC sinpsi = (SHFL(lpos.z, base_lane + 1) - SHFL(lpos.z, base_lane + 2)) /
                           ((TCALC)(2.0) * rc * cosphi);
      const TCALC cospsi = SQRT_FUNC((TCALC)(1.0) - (sinpsi * sinpsi));
      if (cgrp_lane_idx == base_lane) {
        cpos.x = (TCALC)(0.0);
        cpos.y = ra * cosphi;
        cpos.z = ra * sinphi;
      }
      else if (cgrp_lane_idx == base_lane + 1) {
        cpos.x = -rc * cospsi;
        cpos.y = -((rc * sinpsi * sinphi) + (rb * cosphi));
        cpos.z = (rc * sinpsi * cosphi) - (rb * sinphi);
      }
      else if (cgrp_lane_idx == base_lane + 2) {
        cpos.x = rc * cosphi;
        cpos.y = (rc * sinpsi * sinphi) - (rb * cosphi);
        cpos.z = -((rc * sinpsi * cosphi) + (rb * sinphi));
      }
      const TCALC cnn_hd1_x = SHFL(cpos.x, base_lane + 1);
      const TCALC cnn_hd1_y = SHFL(cpos.y, base_lane + 1);
      const TCALC cnn_hd2_y = SHFL(cpos.y, base_lane + 2);
      const TCALC loc_hd1_x = SHFL(lpos.x, base_lane + 1);
      const TCALC loc_hd1_y = SHFL(lpos.y, base_lane + 1);
      const TCALC loc_hd2_x = SHFL(lpos.x, base_lane + 2);
      const TCALC loc_hd2_y = SHFL(lpos.y, base_lane + 2);
      
      // The fourth thread still holds the oxy => hd1 displacement as lpos and the oxy => hd2
      // displacement as cpos.
      TCALC sintheta;
      if (cgrp_lane_idx == base_lane + 3) {
        const TCALC alpha = (cnn_hd1_x * (lpos.x - cpos.x)) +
                            (cnn_hd1_y * lpos.y) + (cnn_hd2_y * cpos.y);
        const TCALC beta  = (cnn_hd1_x * (cpos.y - lpos.y)) +
                            (cnn_hd1_y * lpos.x) + (cnn_hd2_y * cpos.x);
        const TCALC gamma = (lpos.x * loc_hd1_y) - (loc_hd1_x * lpos.y) +
                            (cpos.x * loc_hd2_y) - (loc_hd2_x * cpos.y);
        const TCALC a2b2  = (alpha * alpha) + (beta * beta);
        sintheta = ((alpha * gamma) - (beta * SQRT_FUNC(a2b2 - (gamma * gamma)))) / a2b2;
      }
      sintheta = SHFL(sintheta, base_lane + 3);
      const TCALC costheta = SQRT_FUNC((TCALC)(1.0) - (sintheta * sintheta));
      
      // Create the adjusted coordinates
      TCALC3 apos;
      if (cgrp_lane_idx == base_lane) {
        apos.x = -cpos.y * sintheta;
        apos.y = cpos.y * costheta;
      }
      else if (cgrp_lane_idx == base_lane + 1) {
        apos.x = (cpos.x * costheta) - (cpos.y * sintheta);
        apos.y = (cpos.x * sintheta) + (cpos.y * costheta);
      }
      else if (cgrp_lane_idx == base_lane + 2) {
        apos.x = -(cnn_hd1_x * costheta) - (cpos.y * sintheta);
        apos.y = -(cnn_hd1_x * sintheta) + (cpos.y * costheta);
      }
      apos.z = lpos.z;
      
      // Create the un-primed coordinate vectors using the transpose of the above.
      const int oxy_atm_idx = SHFL(atm_idx, base_lane);
      const int hd1_atm_idx = SHFL(atm_idx, base_lane + 1);
      const int hd2_atm_idx = SHFL(atm_idx, base_lane + 2);
#ifdef TCALC_IS_SINGLE
      const TCALC3 unprime_i = { __int_as_float(sh_xfrc[oxy_atm_idx]),
                                 __int_as_float(sh_xfrc[hd1_atm_idx]),
                                 __int_as_float(sh_xfrc[hd2_atm_idx]) };
#else
      const TCALC3 unprime_i = { __longlong_as_double(sh_xfrc[oxy_atm_idx]),
                                 __longlong_as_double(sh_xfrc[hd1_atm_idx]),
                                 __longlong_as_double(sh_xfrc[hd2_atm_idx]) };
#endif
      const TCALC tmp_apx = dot(apos, unprime_i);
#ifdef TCALC_IS_SINGLE
      const TCALC3 unprime_j = { __int_as_float(sh_yfrc[oxy_atm_idx]),
                                 __int_as_float(sh_yfrc[hd1_atm_idx]),
                                 __int_as_float(sh_yfrc[hd2_atm_idx]) };
#else
      const TCALC3 unprime_j = { __longlong_as_double(sh_yfrc[oxy_atm_idx]),
                                 __longlong_as_double(sh_yfrc[hd1_atm_idx]),
                                 __longlong_as_double(sh_yfrc[hd2_atm_idx]) };
#endif
      const TCALC tmp_apy = dot(apos, unprime_j);
#ifdef TCALC_IS_SINGLE
      const TCALC3 unprime_k = { __int_as_float(sh_zfrc[oxy_atm_idx]),
                                 __int_as_float(sh_zfrc[hd1_atm_idx]),
                                 __int_as_float(sh_zfrc[hd2_atm_idx]) };
#else
      const TCALC3 unprime_k = { __longlong_as_double(sh_zfrc[oxy_atm_idx]),
                                 __longlong_as_double(sh_zfrc[hd1_atm_idx]),
                                 __longlong_as_double(sh_zfrc[hd2_atm_idx]) };
#endif
      apos.z = dot(apos, unprime_k) * poly_psw.gpos_scale_f;
      apos.x = tmp_apx * poly_psw.gpos_scale_f;
      apos.y = tmp_apy * poly_psw.gpos_scale_f;

      // Replace the developing coordinates with the constrained system, if the SETTLE group is
      // valid.
      if (tinsr.y != 0xffffffffU && cgrp_lane_idx < base_lane + 3) {
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
        const int2 cns_pos_x = longlongToInt63(stash_xref[threadIdx.x >> 2] + LLCONV_FUNC(apos.x));
        const int2 cns_pos_y = longlongToInt63(stash_yref[threadIdx.x >> 2] + LLCONV_FUNC(apos.y));
        const int2 cns_pos_z = longlongToInt63(stash_zref[threadIdx.x >> 2] + LLCONV_FUNC(apos.z));
        sh_xfrc[atm_idx] = cns_pos_x.x;
        sh_yfrc[atm_idx] = cns_pos_y.x;
        sh_zfrc[atm_idx] = cns_pos_z.x;
        sh_xfrc_overflow[atm_idx] = cns_pos_x.y;
        sh_yfrc_overflow[atm_idx] = cns_pos_y.y;
        sh_zfrc_overflow[atm_idx] = cns_pos_z.y;
#  else
        sh_xfrc[atm_idx] = sh_xcrd[oxy_atm_idx] + LLCONV_FUNC(apos.x);
        sh_yfrc[atm_idx] = sh_ycrd[oxy_atm_idx] + LLCONV_FUNC(apos.y);
        sh_zfrc[atm_idx] = sh_zcrd[oxy_atm_idx] + LLCONV_FUNC(apos.z);
#  endif
#else // TCALC_IS_SINGLE
#endif // TCALC_IS_SINGLE
      }
      
      // Increment the instruction counter by a quarter of the block width, to reflect the fact
      // that four threads are working on each SETTLE instruction.
      pos += blockDim.x;
    }
//#endif // PME_COMPATIBLE
    
    // The updated positions are now in the force arrays.  In the context of a more extensive
    // valence kernel, the positions are in the proper memory space to guide virtual site
    // placement, if requested, and ultimately to be flushed back to the global positions arrays.
    // In the context of a standalone kernel, the positions should be returned to arrays in the
    // coordinate synthesis now.
    __syncthreads();
    pos = threadIdx.x;
    const TCALC cfac = poly_psw.inv_gpos_scale_f * poly_psw.vel_scale_f / tstw.dt;        
    while (pos < impt_count) {
      const int2 mask_limits =
        poly_vk.vwu_abstracts[(vwu_idx * vwu_abstract_length) +
                              static_cast<int>(VwuAbstractMap::MANIPULATE)];
      const int pos_updt_elem = (pos >> 5);
      const int pos_updt_bit  = pos - (pos_updt_elem << 5);
      const uint2 permit = __ldca(&poly_auk.vwu_manip[mask_limits.x + pos_updt_elem]);
      if ((permit.y >> pos_updt_bit) & 0x1) {
        const size_t global_idx = __ldca(&poly_vk.vwu_imports[impt_llim + pos]);

        // Write the new positions to the appropriate arrays, if this is a stand-alone kernel.
        // Otherwise, wait for possible virtual site placements and write positions at the end
        // of the valence work unit kernel.
#ifdef CONSTRAINT_STANDALONE
#  ifdef TCALC_IS_SINGLE
#    ifdef SPLIT_FORCE_ACCUMULATION
        const llint x_tmp = int63ToLongLong(sh_xfrc[pos], sh_xfrc_overflow[pos]);
        const llint y_tmp = int63ToLongLong(sh_yfrc[pos], sh_yfrc_overflow[pos]);
        const llint z_tmp = int63ToLongLong(sh_zfrc[pos], sh_zfrc_overflow[pos]);
        __stwb(&poly_psw.xalt[global_idx], x_tmp);
        __stwb(&poly_psw.yalt[global_idx], y_tmp);
        __stwb(&poly_psw.zalt[global_idx], z_tmp);
#    else
        __stwb(&poly_psw.xalt[global_idx], sh_xfrc[pos]);
        __stwb(&poly_psw.yalt[global_idx], sh_yfrc[pos]);
        __stwb(&poly_psw.zalt[global_idx], sh_zfrc[pos]);
#    endif
#  else
        __stwb(&poly_psw.xalt[global_idx], sh_xfrc[pos]);
        __stwb(&poly_psw.yalt[global_idx], sh_yfrc[pos]);
        __stwb(&poly_psw.zalt[global_idx], sh_zfrc[pos]);
        __stwb(&poly_psw.xalt_ovrf[global_idx], sh_xfrc_overflow[pos]);
        __stwb(&poly_psw.yalt_ovrf[global_idx], sh_yfrc_overflow[pos]);
        __stwb(&poly_psw.zalt_ovrf[global_idx], sh_zfrc_overflow[pos]);
#  endif
#endif
        // Compute the velocity adjustments due to geometric constraints.  The Lennard-Jones
        // parameter array has been refilled with information on whether constraints acted on
        // any given atom.
        const size_t local_idx = pos + EXCL_GMEM_OFFSET;
#ifdef TCALC_IS_SINGLE
        const bool particle_was_constrained = (__ldca(&gmem_r.lj_idx[local_idx]) == 1);
#else
        const bool particle_was_constrained = (sh_atom_ljidx[pos] == 1);
#endif
        TCALC gc_dx, gc_dy, gc_dz;
        if (particle_was_constrained) {
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
#    ifndef CONSTRAINT_STANDALONE
          const llint x_tmp = int63ToLongLong(sh_xfrc[pos], sh_xfrc_overflow[pos]);
          const llint y_tmp = int63ToLongLong(sh_yfrc[pos], sh_yfrc_overflow[pos]);
          const llint z_tmp = int63ToLongLong(sh_zfrc[pos], sh_zfrc_overflow[pos]);
#    endif
          gc_dx = (TCALC)(x_tmp - sh_xcrd[pos]) * cfac;
          gc_dy = (TCALC)(y_tmp - sh_ycrd[pos]) * cfac;
          gc_dz = (TCALC)(z_tmp - sh_zcrd[pos]) * cfac;
#  else
          gc_dx = (TCALC)(sh_xfrc[pos] - sh_xcrd[pos]) * cfac;
          gc_dy = (TCALC)(sh_yfrc[pos] - sh_ycrd[pos]) * cfac;
          gc_dz = (TCALC)(sh_zfrc[pos] - sh_zcrd[pos]) * cfac;
#  endif
#else  // TCALC_IS_SINGLE
          gc_dx = int95SubtractToDouble(sh_xfrc[pos], sh_xfrc_overflow[pos],
                                        __ldca(&gmem_r.xcrd[local_idx]),
                                        __ldca(&gmem_r.xcrd_ovrf[local_idx])) * cfac;
          gc_dy = int95SubtractToDouble(sh_yfrc[pos], sh_yfrc_overflow[pos],
                                        __ldca(&gmem_r.ycrd[local_idx]),
                                        __ldca(&gmem_r.ycrd_ovrf[local_idx])) * cfac;
          gc_dz = int95SubtractToDouble(sh_zfrc[pos], sh_zfrc_overflow[pos],
                                        __ldca(&gmem_r.zcrd[local_idx]),
                                        __ldca(&gmem_r.zcrd_ovrf[local_idx])) * cfac;
#endif // TCALC_IS_SINGLE
        }
        else {
          gc_dx = (TCALC)(0.0);
          gc_dy = (TCALC)(0.0);
          gc_dz = (TCALC)(0.0);
        }

        // The adjustments to velocities have been computed, but a standalone kernel will draw
        // the velocities themselves from global arrays in the developmental half of the
        // coordinate synthesis, whereas if the code is embedded within a valence work unit the
        // velocities will have been developed from the beginning of integration process and
        // stored in the thread block's private, local resources.  Once adjusted, the velocities
        // will be written back to the global arrays in the coordinate synthesis.
#ifdef TCALC_IS_SINGLE
#  ifdef CONSTRAINT_STANDALONE
        const llint adj_xvel = __ldlu(&poly_psw.vxalt[global_idx]) + LLCONV_FUNC(gc_dx);
        const llint adj_yvel = __ldlu(&poly_psw.vyalt[global_idx]) + LLCONV_FUNC(gc_dy);
        const llint adj_zvel = __ldlu(&poly_psw.vzalt[global_idx]) + LLCONV_FUNC(gc_dz);
#  else
        const llint adj_xvel = __ldca(&gmem_r.xvel[local_idx]) + LLCONV_FUNC(gc_dx);
        const llint adj_yvel = __ldca(&gmem_r.yvel[local_idx]) + LLCONV_FUNC(gc_dy);
        const llint adj_zvel = __ldca(&gmem_r.zvel[local_idx]) + LLCONV_FUNC(gc_dz);
#  endif
        __stwt(&poly_psw.vxalt[global_idx], adj_xvel);
        __stwt(&poly_psw.vyalt[global_idx], adj_yvel);
        __stwt(&poly_psw.vzalt[global_idx], adj_zvel);
#else  // TCALC_IS_SINGLE
#  ifdef CONSTRAINT_STANDALONE
        const int95_t adj_xvel = int95Sum(__ldcv(&poly_psw.vxalt[global_idx]),
                                          __ldcv(&poly_psw.vxalt_ovrf[global_idx]), gc_dx);
        const int95_t adj_yvel = int95Sum(__ldcv(&poly_psw.vyalt[global_idx]),
                                          __ldcv(&poly_psw.vyalt_ovrf[global_idx]), gc_dy);
        const int95_t adj_zvel = int95Sum(__ldcv(&poly_psw.vzalt[global_idx]),
                                          __ldcv(&poly_psw.vzalt_ovrf[global_idx]), gc_dz);
#  else
        const int95_t adj_xvel = int95Sum(__ldcv(&gmem_r.xvel[local_idx]),
                                          __ldcv(&gmem_r.xvel_ovrf[local_idx]), gc_dx);
        const int95_t adj_yvel = int95Sum(__ldcv(&gmem_r.yvel[local_idx]),
                                          __ldcv(&gmem_r.yvel_ovrf[local_idx]), gc_dy);
        const int95_t adj_zvel = int95Sum(__ldcv(&gmem_r.zvel[local_idx]),
                                          __ldcv(&gmem_r.zvel_ovrf[local_idx]), gc_dz);
#  endif
        __stwb(&poly_psw.vxalt[global_idx], adj_xvel.x);
        __stwb(&poly_psw.vyalt[global_idx], adj_yvel.x);
        __stwb(&poly_psw.vzalt[global_idx], adj_zvel.x);
        __stwb(&poly_psw.vxalt_ovrf[global_idx], adj_xvel.y);
        __stwb(&poly_psw.vyalt_ovrf[global_idx], adj_yvel.y);
        __stwb(&poly_psw.vzalt_ovrf[global_idx], adj_zvel.y);
#endif // TCALC_IS_SINGLE
      }
      pos += blockDim.x;
    }
#ifdef CONSTRAINT_STANDALONE
    // Proceed to the next valence work unit.
    __syncthreads();
    if (threadIdx.x == 0) {
      const size_t prog_counter_idx = (ctrl.step & twice_warp_bits_mask_int);
      vwu_idx = atomicAdd(&ctrl.gcns_progress[prog_counter_idx], 1);
    }
    __syncthreads();
  } // Close the loop over all valence work units

  // Set the block counters for future iterations of this kernel
  if (blockIdx.x == 0 && threadIdx.x < warp_size_int) {
    const int step_modulus = (ctrl.step & twice_warp_bits_mask_int);
    if (step_modulus == 0) {
      ctrl.gcns_progress[threadIdx.x + warp_size_int] = gridDim.x;
    }
    if (step_modulus == warp_size_int) {
      ctrl.gcns_progress[threadIdx.x] = gridDim.x;
    }
  }
}

// Clear definitions of the valence atom capacity and the offset into the thread-block exclusive
// cache space.
#  undef VALENCE_ATOM_CAPACITY
#  undef EXCL_GMEM_OFFSET
#endif // CONSTRAINT_STANDALONE
