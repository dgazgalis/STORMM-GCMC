// -*-c++-*-
#include "copyright.h"

// Define the block's atom capacity, if this code is not included in another block where it is
// already defined.  See Structure/valence_potential.cui for more details.
#ifdef VERLET_STANDALONE
#  define EXCL_GMEM_OFFSET  (blockIdx.x * gmem_r.max_atoms)
#  ifdef TCALC_IS_SINGLE
#    if INTEG_KERNEL_THREAD_COUNT == 64
#      define VALENCE_ATOM_CAPACITY eighth_valence_work_unit_atoms
#    elif INTEG_KERNEL_THREAD_COUNT <= 128
#      define VALENCE_ATOM_CAPACITY quarter_valence_work_unit_atoms
#    elif INTEG_KERNEL_THREAD_COUNT <= 256
#      define VALENCE_ATOM_CAPACITY half_valence_work_unit_atoms
#    else
#      define VALENCE_ATOM_CAPACITY maximum_valence_work_unit_atoms
#    endif
#  else
#    define VALENCE_ATOM_CAPACITY maximum_valence_work_unit_atoms
#  endif

/// \brief In standalone form, the second verlet update kernel must import forces (stored by the
///        standalone form of the first verlet update kernel, for consolidation as well as
///        inclusion of any Langevin update), as well as atomic positions.  Velocities will be
///        taken in as needed.  The kernel will update the velocities a second time, then update
///        positions and store them as appropriate based on whether it is a standalone, partially
///        fused, or fully fused kernel.
__global__ void __launch_bounds__(INTEG_KERNEL_THREAD_COUNT, INTEG_BLOCK_MULTIPLICITY)
KERNEL_NAME(const SyValenceKit<TCALC> poly_vk, MMControlKit<TCALC> ctrl,
            PsSynthesisWriter poly_psw, const SyAtomUpdateKit<TCALC, TCALC2, TCALC4> poly_auk,
            const ThermostatWriter<TCALC> tstw, CacheResourceKit<TCALC> gmem_r) {

  // Arrays named sh_xfrc and the like will be used to assemble the total force on each particle
  // due to contributions from all sources.
#  ifdef SPLIT_FORCE_ACCUMULATION
#    ifdef TCALC_IS_SINGLE
  __shared__ int sh_xfrc[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_yfrc[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_zfrc[VALENCE_ATOM_CAPACITY];
#    else
  __shared__ llint sh_xfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_yfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_zfrc[VALENCE_ATOM_CAPACITY];
#    endif
  __shared__ int sh_xfrc_overflow[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_yfrc_overflow[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_zfrc_overflow[VALENCE_ATOM_CAPACITY];
#  else
  // As with the valence kernels, not having a definition of split force accumulation implies that
  // 64-bit signed integer accumulation is active in single-precision calculation mode.
  __shared__ llint sh_xfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_yfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_zfrc[VALENCE_ATOM_CAPACITY];
#  endif
#ifdef TCALC_IS_SINGLE
  __shared__ llint sh_xcrd[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_ycrd[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_zcrd[VALENCE_ATOM_CAPACITY];
#endif
  // In standalone mode, the valence work unit still needs to be collected.
  __shared__ int2 vwu_map[vwu_abstract_length];
  __shared__ int vwu_task_count[vwu_abstract_length];
  __shared__ int vwu_padded_task_count[vwu_abstract_length];
  __shared__ volatile int vwu_idx;
  __shared__ volatile TCALC rtoldt;
  __shared__ volatile bool constraint_work;

  // Each block takes its first valence work unit based on its block index.
  if (threadIdx.x == 0) {
    vwu_idx = blockIdx.x;
    rtoldt = tstw.rattle_tol * poly_psw.vel_scale / tstw.dt;
  }
  __syncthreads();
  while (vwu_idx < poly_vk.nvwu) {

    // The instruction set map is read and stored in __shared__ for convenience, and to ensure
    // that it never leaves cache.  The instructions themselves are "streamed," which for purposes
    // of this documentation means read from global, used once, and not cached.  Each block must be
    // at least vwu_abstract_length in size.
    if (threadIdx.x < vwu_abstract_length) {
      vwu_map[threadIdx.x] = __ldcv(&poly_vk.vwu_abstracts[(vwu_idx * vwu_abstract_length) +
                                                           threadIdx.x]);
      vwu_task_count[threadIdx.x] = vwu_map[threadIdx.x].y - vwu_map[threadIdx.x].x;
      vwu_padded_task_count[threadIdx.x] = devcRoundUp(vwu_task_count[threadIdx.x], warp_size_int);
    }
    __syncthreads();
    
    // Recover the current forces on each particle from __global__ arrays.  The complete forces,
    // including any Langevin bumps, were stored there in standalone the Verlet I step.
    const int impt_llim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].x;
    const int impt_hlim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].y;
    const int impt_count  = impt_hlim - impt_llim;
    const int impt_stride = devcRoundUp(impt_hlim - impt_llim, warp_size_int);
    if (threadIdx.x == 0) {
      constraint_work = (vwu_task_count[(size_t)(VwuAbstractMap::CGROUP)] +
                         vwu_task_count[(size_t)(VwuAbstractMap::SETTLE)] > 0 &&
                         tstw.cnst_geom);
    }
    int pos = threadIdx.x;
    while (pos < impt_stride) {
      if (pos < impt_count) {
        const size_t global_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + pos]);
        const size_t local_write_idx = pos + EXCL_GMEM_OFFSET;
#  ifdef TCALC_IS_SINGLE
#    ifdef SPLIT_FORCE_ACCUMULATION
        const int2 tmp_xfrc = longlongToInt63(__ldcv(&poly_psw.fxalt[global_read_idx]));
        sh_xfrc[pos] = tmp_xfrc.x;
        sh_xfrc_overflow[pos] = tmp_xfrc.y;
#    else
        sh_xfrc[pos] = __ldcv(&poly_psw.fxalt[global_read_idx]);
#    endif
        sh_xcrd[pos] = __ldcv(&poly_psw.xcrd[global_read_idx]);
        __stwb(&gmem_r.xvel[local_write_idx], __ldcv(&poly_psw.vxalt[global_read_idx]));
#  else
        sh_xfrc[pos] = __ldcv(&poly_psw.fxalt[global_read_idx]);
        sh_xfrc_overflow[pos] = __ldcv(&poly_psw.fxalt_ovrf[global_read_idx]);
        __stwb(&gmem_r.xcrd[local_write_idx], __ldcv(&poly_psw.xcrd[global_read_idx]));
        __stwb(&gmem_r.xcrd_ovrf[local_write_idx], __ldcv(&poly_psw.xcrd_ovrf[global_read_idx]));
        __stwb(&gmem_r.xvel[local_write_idx], __ldcv(&poly_psw.vxalt[global_read_idx]));
        __stwb(&gmem_r.xvel_ovrf[local_write_idx], __ldcv(&poly_psw.vxalt_ovrf[global_read_idx]));
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 2 * impt_stride) {
      const int rel_pos = pos - impt_stride;
      if (rel_pos < impt_count) {
        const size_t global_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + rel_pos]);
        const size_t local_write_idx = rel_pos + EXCL_GMEM_OFFSET;
#  ifdef TCALC_IS_SINGLE
#    ifdef SPLIT_FORCE_ACCUMULATION
        const int2 tmp_yfrc = longlongToInt63(__ldcv(&poly_psw.fyalt[global_read_idx]));
        sh_yfrc[rel_pos] = tmp_yfrc.x;
        sh_yfrc_overflow[rel_pos] = tmp_yfrc.y;
#    else
        sh_yfrc[rel_pos] = __ldcv(&poly_psw.fyalt[global_read_idx]);
#    endif
        sh_ycrd[rel_pos] = __ldcv(&poly_psw.ycrd[global_read_idx]);
        __stwb(&gmem_r.yvel[local_write_idx], __ldcv(&poly_psw.vyalt[global_read_idx]));
#  else
        sh_yfrc[rel_pos] = __ldcv(&poly_psw.fyalt[global_read_idx]);
        sh_yfrc_overflow[rel_pos] = __ldcv(&poly_psw.fyalt_ovrf[global_read_idx]);
        __stwb(&gmem_r.ycrd[local_write_idx], __ldcv(&poly_psw.ycrd[global_read_idx]));
        __stwb(&gmem_r.ycrd_ovrf[local_write_idx], __ldcv(&poly_psw.ycrd_ovrf[global_read_idx]));
        __stwb(&gmem_r.yvel[local_write_idx], __ldcv(&poly_psw.vyalt[global_read_idx]));
        __stwb(&gmem_r.yvel_ovrf[local_write_idx], __ldcv(&poly_psw.vyalt_ovrf[global_read_idx]));
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 3 * impt_stride) {
      const int rel_pos = pos - (2 * impt_stride);
      if (rel_pos < impt_count) {
        const size_t global_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + rel_pos]);
        const size_t local_write_idx = rel_pos + EXCL_GMEM_OFFSET;
#  ifdef TCALC_IS_SINGLE
#    ifdef SPLIT_FORCE_ACCUMULATION
        const int2 tmp_zfrc = longlongToInt63(__ldcv(&poly_psw.fzalt[global_read_idx]));
        sh_zfrc[rel_pos] = tmp_zfrc.x;
        sh_zfrc_overflow[rel_pos] = tmp_zfrc.y;
#    else
        sh_zfrc[rel_pos] = __ldcv(&poly_psw.fzalt[global_read_idx]);
#    endif
        sh_zcrd[rel_pos] = __ldcv(&poly_psw.zcrd[global_read_idx]);
        __stwb(&gmem_r.zvel[local_write_idx], __ldcv(&poly_psw.vzalt[global_read_idx]));
#  else
        sh_zfrc[rel_pos] = __ldcv(&poly_psw.fzalt[global_read_idx]);
        sh_zfrc_overflow[rel_pos] = __ldcv(&poly_psw.fzalt_ovrf[global_read_idx]);
        __stwb(&gmem_r.zcrd[local_write_idx], __ldcv(&poly_psw.zcrd[global_read_idx]));
        __stwb(&gmem_r.zcrd_ovrf[local_write_idx], __ldcv(&poly_psw.zcrd_ovrf[global_read_idx]));
        __stwb(&gmem_r.zvel[local_write_idx], __ldcv(&poly_psw.vzalt[global_read_idx]));
        __stwb(&gmem_r.zvel_ovrf[local_write_idx], __ldcv(&poly_psw.vzalt_ovrf[global_read_idx]));
#  endif
      }
      pos += blockDim.x;
    }
    __syncthreads();
#endif // VERLET_STANDALONE
    
    // Apply the second Velocity Verlet step: the second half of the velocity update, followed by
    // computation of new positions.  Store the updated positions locally if there will be
    // positional constraints to apply.  Otherwise, store the updated positions directly to the
    // global arrays.
    pos = threadIdx.x;
    while (pos < impt_stride) {
      const int manip_llim    = vwu_map[(size_t)(VwuAbstractMap::MANIPULATE)].x;
      const int manip_segment = (pos >> 5);
      const int manip_bitpos  = (pos & 0x1f);
      const uint2 manip_mask = __ldca(&poly_auk.vwu_manip[manip_llim + manip_segment]);
      if (pos < impt_count && ((manip_mask.x >> manip_bitpos) & 0x1)) {
        const size_t lcl_idx = EXCL_GMEM_OFFSET + pos;
#ifdef SPLIT_FORCE_ACCUMULATION
#  ifdef TCALC_IS_SINGLE
        const TCALC rxfrc = int63ToFloat(sh_xfrc[pos], sh_xfrc_overflow[pos]);
        const TCALC ryfrc = int63ToFloat(sh_yfrc[pos], sh_yfrc_overflow[pos]);
        const TCALC rzfrc = int63ToFloat(sh_zfrc[pos], sh_zfrc_overflow[pos]);
#  else
	const TCALC rxfrc = int95ToDouble(sh_xfrc[pos], sh_xfrc_overflow[pos]);
        const TCALC ryfrc = int95ToDouble(sh_yfrc[pos], sh_yfrc_overflow[pos]);
        const TCALC rzfrc = int95ToDouble(sh_zfrc[pos], sh_zfrc_overflow[pos]);
#  endif
#else // SPLIT_FORCE_ACCUMULATION
        const TCALC rxfrc = (TCALC)(sh_xfrc[pos]);
        const TCALC ryfrc = (TCALC)(sh_yfrc[pos]);
        const TCALC rzfrc = (TCALC)(sh_zfrc[pos]);
#endif // SPLIT_FORCE_ACCUMULATION
        const size_t gbl_idx = __ldca(&poly_vk.vwu_imports[impt_llim + pos]);

        // Carry out the second Velocity-Verlet velocity update, then store the velocity back into
        // its global accumulator if that is the responsibility of this work unit.
        const TCALC t_mass = __ldca(&poly_auk.masses[gbl_idx]);
        const TCALC hmdt = (t_mass > (TCALC)(1.0e-8)) ?
                           ((TCALC)(0.5) * tstw.dt / t_mass) * kcal_to_gafs_f *
                           poly_psw.vel_scale * poly_psw.inv_frc_scale : (TCALC)(0.0);
        TCALC vx_update, vy_update, vz_update;
        switch (tstw.kind) {
        case ThermostatKind::LANGEVIN:
          {
            // The Langevin bump force has already been added the the accumulated forces, and the
            // velocity must be multiplied by a real-valued factor so read it from the local cache
            // and convert it directly to a real number.
#ifdef TCALC_IS_SINGLE
            vx_update = (TCALC)(__ldca(&gmem_r.xvel[lcl_idx]));
            vy_update = (TCALC)(__ldca(&gmem_r.yvel[lcl_idx]));
            vz_update = (TCALC)(__ldca(&gmem_r.zvel[lcl_idx]));
#else // TCALC_IS_SINGLE
            vx_update = int95ToDouble(__ldca(&gmem_r.xvel[lcl_idx]),
                                      __ldca(&gmem_r.xvel_ovrf[lcl_idx]));
            vy_update = int95ToDouble(__ldca(&gmem_r.yvel[lcl_idx]),
                                      __ldca(&gmem_r.yvel_ovrf[lcl_idx]));
            vz_update = int95ToDouble(__ldca(&gmem_r.zvel[lcl_idx]),
                                      __ldca(&gmem_r.zvel_ovrf[lcl_idx]));
#endif // TCALC_IS_SINGLE
            vx_update = (vx_update * tstw.ln_explicit) + (hmdt * rxfrc);
            vy_update = (vy_update * tstw.ln_explicit) + (hmdt * ryfrc);
            vz_update = (vz_update * tstw.ln_explicit) + (hmdt * rzfrc);
          }
          break;
        case ThermostatKind::NONE:
        case ThermostatKind::ANDERSEN:
        case ThermostatKind::BERENDSEN:
          {
            // For all thermostats but Langevin, including the case of no thermostat when energy
            // conservation is scrutinized, the velocity update is additive.  Convert the small
            // delta to the fixed-precision format for best conservation of information.
            const TCALC vx_delta = hmdt * rxfrc;
            const TCALC vy_delta = hmdt * ryfrc;
            const TCALC vz_delta = hmdt * rzfrc;
#ifdef TCALC_IS_SINGLE
            const llint ivx_delta = LLCONV_FUNC(vx_delta);
            const llint ivy_delta = LLCONV_FUNC(vy_delta);
            const llint ivz_delta = LLCONV_FUNC(vz_delta);
            const llint ivx_update = __ldca(&gmem_r.xvel[lcl_idx]) + ivx_delta;
            const llint ivy_update = __ldca(&gmem_r.yvel[lcl_idx]) + ivy_delta;
            const llint ivz_update = __ldca(&gmem_r.zvel[lcl_idx]) + ivz_delta;
            vx_update = (TCALC)(ivx_update);
            vy_update = (TCALC)(ivy_update);
            vz_update = (TCALC)(ivz_update);
#else // TCALC_IS_SINGLE
            const int95_t ivx_delta  = doubleToInt95(vx_delta);
            const int95_t ivy_delta  = doubleToInt95(vy_delta);
            const int95_t ivz_delta  = doubleToInt95(vz_delta);
            const int95_t ivx_update = splitFPSum(ivx_delta, __ldca(&gmem_r.xvel[lcl_idx]),
                                                  __ldca(&gmem_r.xvel_ovrf[lcl_idx]));
            const int95_t ivy_update = splitFPSum(ivy_delta, __ldca(&gmem_r.yvel[lcl_idx]),
                                                  __ldca(&gmem_r.yvel_ovrf[lcl_idx]));
            const int95_t ivz_update = splitFPSum(ivz_delta, __ldca(&gmem_r.zvel[lcl_idx]),
                                                  __ldca(&gmem_r.zvel_ovrf[lcl_idx]));
            vx_update = int95ToDouble(ivx_update.x, ivx_update.y);
            vy_update = int95ToDouble(ivy_update.x, ivy_update.y);
            vz_update = int95ToDouble(ivz_update.x, ivz_update.y);
#endif // TCALC_IS_SINGLE
          }
          break;
        }

        // If there are no constraints to apply, the velocities are closed and complete.  Store
        // the updated velocity to the global arrays, if this work unit is responsible for moving
        // the atom.  If there are contraints to apply, stowe the velocities once more in the
        // thread block's private cache resources.  This requires no synchronization as each
        // thread is reading and then writing exclusively to its own designated index each array.
#ifndef VERLET_STANDALONE
        if (constraint_work) {
#  ifdef TCALC_IS_SINGLE
          __stwb(&gmem_r.xvel[lcl_idx], LLCONV_FUNC(vx_update));
          __stwb(&gmem_r.yvel[lcl_idx], LLCONV_FUNC(vy_update));
          __stwb(&gmem_r.zvel[lcl_idx], LLCONV_FUNC(vz_update));
#  else // TCALC_IS_SINGLE
          const int95_t ivx_update = doubleToInt95(vx_update);
          const int95_t ivy_update = doubleToInt95(vy_update);
          const int95_t ivz_update = doubleToInt95(vz_update);
          __stwb(&gmem_r.xvel[lcl_idx], ivx_update.x);
          __stwb(&gmem_r.yvel[lcl_idx], ivy_update.x);
          __stwb(&gmem_r.zvel[lcl_idx], ivz_update.x);
          __stwb(&gmem_r.xvel_ovrf[lcl_idx], ivx_update.y);
          __stwb(&gmem_r.yvel_ovrf[lcl_idx], ivy_update.y);
          __stwb(&gmem_r.zvel_ovrf[lcl_idx], ivz_update.y);
#  endif // TCALC_IS_SINGLE
        }
        else {
#endif // VERLET_STANDALONE is undefined

          // While it was checked that this atom's bit in manip_mask.x was set to 1, the bit in
          // manip_mask.y must be checked to ensure that the work unit is not only expected to
          // move the atom but also to update its position in the global arrays.
          if ((manip_mask.y >> manip_bitpos) & 0x1) {
#ifdef TCALC_IS_SINGLE
            __stwt(&poly_psw.vxalt[gbl_idx], LLCONV_FUNC(vx_update));
            __stwt(&poly_psw.vyalt[gbl_idx], LLCONV_FUNC(vy_update));
            __stwt(&poly_psw.vzalt[gbl_idx], LLCONV_FUNC(vz_update));
#else // TCALC_IS_SINGLE
            const int95_t ivx_update = doubleToInt95(vx_update);
            const int95_t ivy_update = doubleToInt95(vy_update);
            const int95_t ivz_update = doubleToInt95(vz_update);
            __stwt(&poly_psw.vxalt[gbl_idx], ivx_update.x);
            __stwt(&poly_psw.vyalt[gbl_idx], ivy_update.x);
            __stwt(&poly_psw.vzalt[gbl_idx], ivz_update.x);
            __stwt(&poly_psw.vxalt_ovrf[gbl_idx], ivx_update.y);
            __stwt(&poly_psw.vyalt_ovrf[gbl_idx], ivy_update.y);
            __stwt(&poly_psw.vzalt_ovrf[gbl_idx], ivz_update.y);
#endif // TCALC_IS_SINGLE
          }
#ifndef VERLET_STANDALONE
        }
#endif // VERLET_STANDALONE is undefined

        // The updated velocity is currently inflated by the fixed-precision scaling.  Remove that,
        // apply the positional scaling inflation factor and multiply by the time step to get the
        // positional delta, then update the locally cached positions and velocities for this
        // particle.  If there are no positional constraints to apply, write results directly to
        // the global arrays.  Otherwise write back to the positions of locally cached particles.
        // In standalone compilations only the presence of virtual sites matters, as constraints
        // will be applied by a subsequent kernel.
        const TCALC xpos_delta = vx_update * poly_psw.inv_vel_scale * poly_psw.gpos_scale *
                                 tstw.dt;
        const TCALC ypos_delta = vy_update * poly_psw.inv_vel_scale * poly_psw.gpos_scale *
                                 tstw.dt;
        const TCALC zpos_delta = vz_update * poly_psw.inv_vel_scale * poly_psw.gpos_scale *
                                 tstw.dt;
#ifdef TCALC_IS_SINGLE
        const llint ixpos_delta = LLCONV_FUNC(xpos_delta);
        const llint iypos_delta = LLCONV_FUNC(ypos_delta);
        const llint izpos_delta = LLCONV_FUNC(zpos_delta);
        const llint ixpos_update = sh_xcrd[pos] + ixpos_delta;
        const llint iypos_update = sh_ycrd[pos] + iypos_delta;
        const llint izpos_update = sh_zcrd[pos] + izpos_delta;
#  ifdef VERLET_STANDALONE
        if (constraint_work == false && vwu_task_count[(size_t)VwuAbstractMap::VSITE] > 0) {
#  else
        if (constraint_work || vwu_task_count[(size_t)VwuAbstractMap::VSITE] > 0) {
#  endif
#  ifdef SPLIT_FORCE_ACCUMULATION
          const int2 nx_loc = longlongToInt63(ixpos_update);
          const int2 ny_loc = longlongToInt63(iypos_update);
          const int2 nz_loc = longlongToInt63(izpos_update);
          sh_xfrc[pos] = nx_loc.x;
          sh_yfrc[pos] = ny_loc.x;
          sh_zfrc[pos] = nz_loc.x;
          sh_xfrc_overflow[pos] = nx_loc.y;
          sh_yfrc_overflow[pos] = ny_loc.y;
          sh_zfrc_overflow[pos] = nz_loc.y;
#  else
          sh_xfrc[pos] = ixpos_update;
          sh_yfrc[pos] = iypos_update;
          sh_zfrc[pos] = izpos_update;
#  endif
#  ifdef VERLET_STANDALONE
        }
#  else
        }
#  endif
        else {

          // See above on the distinction between checks on the bit setting in manip_mask.x versus
          // that in manip_mask.y.
          if ((manip_mask.y >> manip_bitpos) & 0x1) {
            __stwt(&poly_psw.xalt[gbl_idx], ixpos_update);
            __stwt(&poly_psw.yalt[gbl_idx], iypos_update);
            __stwt(&poly_psw.zalt[gbl_idx], izpos_update);
          }
        }
#else // TCALC_IS_SINGLE
        const int95_t ixpos_delta = doubleToInt95(xpos_delta);
        const int95_t iypos_delta = doubleToInt95(ypos_delta);
        const int95_t izpos_delta = doubleToInt95(zpos_delta);
        const int95_t ixpos_update = splitFPSum(ixpos_delta, __ldcv(&gmem_r.xcrd[lcl_idx]),
                                                __ldcv(&gmem_r.xcrd_ovrf[lcl_idx]));
        const int95_t iypos_update = splitFPSum(iypos_delta, __ldcv(&gmem_r.ycrd[lcl_idx]),
                                                __ldcv(&gmem_r.ycrd_ovrf[lcl_idx]));
        const int95_t izpos_update = splitFPSum(izpos_delta, __ldcv(&gmem_r.zcrd[lcl_idx]),
                                                __ldcv(&gmem_r.zcrd_ovrf[lcl_idx]));
#  ifdef VERLET_STANDALONE
        if (constraint_work == false && vwu_task_count[(size_t)VwuAbstractMap::VSITE] > 0) {
#  else
        if (constraint_work || vwu_task_count[(size_t)VwuAbstractMap::VSITE] > 0) {
#  endif
          sh_xfrc[pos] = ixpos_update.x;
          sh_yfrc[pos] = iypos_update.x;
          sh_zfrc[pos] = izpos_update.x;
          sh_xfrc_overflow[pos] = ixpos_update.y;
          sh_yfrc_overflow[pos] = iypos_update.y;
          sh_zfrc_overflow[pos] = izpos_update.y;
#  ifdef VERLET_STANDALONE
        }
#  else
        }
#  endif
        else {

          // See above on the distinction between checks on the bit setting in manip_mask.x versus
          // that in manip_mask.y.
          if ((manip_mask.y >> manip_bitpos) & 0x1) {
            __stwt(&poly_psw.xalt[gbl_idx],      ixpos_update.x);
            __stwt(&poly_psw.yalt[gbl_idx],      iypos_update.x);
            __stwt(&poly_psw.zalt[gbl_idx],      izpos_update.x);
            __stwt(&poly_psw.xalt_ovrf[gbl_idx], ixpos_update.y);
            __stwt(&poly_psw.yalt_ovrf[gbl_idx], iypos_update.y);
            __stwt(&poly_psw.zalt_ovrf[gbl_idx], izpos_update.y);
          }
        }
#endif // TCALC_IS_SINGLE
      }
      pos += blockDim.x;
    }

    // As with other integration standalone code, no additional synchronization occurs here, as
    // anywhere this code is included will be expected to have a __syncthreads() call immediately
    // following it.
#ifdef VERLET_STANDALONE

    // If there are no constraints to apply, the virtual site placement still needs to be done.
    // That can happen in the context of this kernel, to mirror what happens with force
    // transmission in the Verlet I step.  If there are constraints to apply, hold on virtual site
    // placement (it will occur as part of the geometry constraints kernel).
    if (constraint_work == false && vwu_task_count[(size_t)(VwuAbstractMap::VSITE)] > 0) {
      __syncthreads();
      int vterm_limit;
#  include "Structure/virtual_site_placement.cui"
      __syncthreads();
      pos = threadIdx.x;
      while (pos < impt_stride) {
        const int manip_llim    = vwu_map[(size_t)(VwuAbstractMap::MANIPULATE)].x;
        const int manip_segment = (pos >> 5);
        const int manip_bitpos  = (pos & 0x1f);
        const uint2 manip_mask = __ldca(&poly_auk.vwu_manip[manip_llim + manip_segment]);
        if (pos < impt_count && ((manip_mask.y >> manip_bitpos) & 0x1)) {
          const size_t gbl_idx = __ldca(&poly_vk.vwu_imports[impt_llim + pos]);
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
          const llint ixpos_update = int63ToLongLong(sh_xfrc[pos], sh_xfrc_overflow[pos]);
          const llint iypos_update = int63ToLongLong(sh_yfrc[pos], sh_yfrc_overflow[pos]);
          const llint izpos_update = int63ToLongLong(sh_zfrc[pos], sh_zfrc_overflow[pos]);
          __stwt(&poly_psw.xalt[gbl_idx], ixpos_update);
          __stwt(&poly_psw.yalt[gbl_idx], iypos_update);
          __stwt(&poly_psw.zalt[gbl_idx], izpos_update);
#  else
          __stwt(&poly_psw.xalt[gbl_idx], sh_xfrc[pos]);
          __stwt(&poly_psw.yalt[gbl_idx], sh_yfrc[pos]);
          __stwt(&poly_psw.zalt[gbl_idx], sh_zfrc[pos]);
#  endif
#else // TCALC_IS_SINGLE
          __stwt(&poly_psw.xalt[gbl_idx], sh_xfrc[pos]);
          __stwt(&poly_psw.yalt[gbl_idx], sh_yfrc[pos]);
          __stwt(&poly_psw.zalt[gbl_idx], sh_zfrc[pos]);
          __stwt(&poly_psw.xalt_ovrf[gbl_idx], sh_xfrc_overflow[pos]);
          __stwt(&poly_psw.yalt_ovrf[gbl_idx], sh_yfrc_overflow[pos]);
          __stwt(&poly_psw.zalt_ovrf[gbl_idx], sh_zfrc_overflow[pos]);
#endif // TCALC_IS_SINGLE
        }
        pos += blockDim.x;
      }
    }

    // Proceed to the next valence work unit.
    __syncthreads();
    if (threadIdx.x == 0) {
      const size_t prog_counter_idx = (ctrl.step & twice_warp_bits_mask_int);
      vwu_idx = atomicAdd(&ctrl.pupt_progress[prog_counter_idx], 1);
    }
    __syncthreads();
  } // Close the loop over all valence work units

  // Set the block counters for future iterations of this kernel
  if (blockIdx.x == 0 && threadIdx.x < warp_size_int) {
    const int step_modulus = (ctrl.step & twice_warp_bits_mask_int);
    if (step_modulus == 0) {
      ctrl.pupt_progress[threadIdx.x + warp_size_int] = gridDim.x;
    }
    if (step_modulus == warp_size_int) {
      ctrl.pupt_progress[threadIdx.x] = gridDim.x;
    }
  }
}

// Clear definitions of the valence atom capacity and the offset into the thread-block exclusive
// cache space.
#  undef VALENCE_ATOM_CAPACITY
#  undef EXCL_GMEM_OFFSET
#endif // VERLET_STANDALONE

