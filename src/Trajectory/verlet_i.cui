// -*-c++-*-
#include "copyright.h"

// Define the block's atom capacity, if this code is not included in another block where it is
// already defined.  See Structure/valence_potential.cui for more details.
#ifdef VERLET_STANDALONE
#  define EXCL_GMEM_OFFSET  (blockIdx.x * gmem_r.max_atoms)
#  ifdef TCALC_IS_SINGLE
#    if INTEG_KERNEL_THREAD_COUNT == 64
#      define VALENCE_ATOM_CAPACITY eighth_valence_work_unit_atoms
#    elif INTEG_KERNEL_THREAD_COUNT <= 128
#      define VALENCE_ATOM_CAPACITY quarter_valence_work_unit_atoms
#    elif INTEG_KERNEL_THREAD_COUNT <= 256
#      define VALENCE_ATOM_CAPACITY half_valence_work_unit_atoms
#    else
#      define VALENCE_ATOM_CAPACITY maximum_valence_work_unit_atoms
#    endif
#  else
#    define VALENCE_ATOM_CAPACITY maximum_valence_work_unit_atoms
#  endif

/// \brief In standalone form, the first verlet update kernel has nothing to import--the forces
///        stored by the prior valence computations will be held in PhaseSpaceSynthesis, while
///        forces due to non-bonded interactions will also be folded into accumulators in the
///        PhaseSpaceSynthesis (for implicit solvent calculations in isolated boundary conditions)
///        or in CellGrid objects (for calculations in periodic boundary conditions).  The
///        local force accumulators must therefore be initialized for the standalone kernel.  Both
///        the standalone kernel and includable code will then take in the current particle
///        velocities, sum the current forces, update particle velocities, and write the results
///        into the arrays of developing particle velocities.
__global__ void __launch_bounds__(INTEG_KERNEL_THREAD_COUNT, INTEG_BLOCK_MULTIPLICITY)
KERNEL_NAME(const SyValenceKit<TCALC> poly_vk, MMControlKit<TCALC> ctrl,
            PsSynthesisWriter poly_psw,
#  ifdef PME_COMPATIBLE
#    ifdef DUAL_GRIDS
            const CellGridReader<TCOORD, TACC, TCOORD, TCOORD4> cgr_qq,
            const CellGridReader<TCOORD, TACC, TCOORD, TCOORD4> cgr_lj,
#    else
            const CellGridReader<TCOORD, TACC, TCOORD, TCOORD4> cgr,
#    endif
#  endif
            const SyAtomUpdateKit<TCALC, TCALC2, TCALC4> poly_auk,
            const ThermostatWriter<TCALC> tstw, CacheResourceKit<TCALC> gmem_r) {

  // Arrays named sh_xfrc and the like will be used to assemble the total force on each particle
  // due to contributions from all sources.
#  ifdef SPLIT_FORCE_ACCUMULATION
#    ifdef TCALC_IS_SINGLE
  __shared__ int sh_xfrc[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_yfrc[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_zfrc[VALENCE_ATOM_CAPACITY];
#    else
  __shared__ llint sh_xfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_yfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_zfrc[VALENCE_ATOM_CAPACITY];
#    endif
  __shared__ int sh_xfrc_overflow[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_yfrc_overflow[VALENCE_ATOM_CAPACITY];
  __shared__ int sh_zfrc_overflow[VALENCE_ATOM_CAPACITY];
#  else
  // As with the valence kernels, not having a definition of split force accumulation implies that
  // 64-bit signed integer accumulation is active in single-precision calculation mode.
  __shared__ llint sh_xfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_yfrc[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_zfrc[VALENCE_ATOM_CAPACITY];
#  endif
#ifdef TCALC_IS_SINGLE
  __shared__ llint sh_xcrd[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_ycrd[VALENCE_ATOM_CAPACITY];
  __shared__ llint sh_zcrd[VALENCE_ATOM_CAPACITY];
#endif
  
  // In standalone mode, the valence work unit still needs to be collected.
  __shared__ int2 vwu_map[vwu_abstract_length];
  __shared__ int vwu_task_count[vwu_abstract_length], vwu_padded_task_count[vwu_abstract_length];
  __shared__ volatile int vwu_idx;
  __shared__ volatile TCALC rtoldt;

  // Each block takes its first valence work unit based on its block index.
  if (threadIdx.x == 0) {
    vwu_idx = blockIdx.x;
    rtoldt = tstw.rattle_tol * poly_psw.vel_scale / tstw.dt;
  }
  __syncthreads();
  while (vwu_idx < poly_vk.nvwu) {

    // The instruction set map is read and stored in __shared__ for convenience, and to ensure
    // that it never leaves cache.  The instructions themselves are "streamed," which for purposes
    // of this documentation means read from global, used once, and not cached.  Each block must be
    // at least vwu_abstract_length in size.
    if (threadIdx.x < vwu_abstract_length) {
      vwu_map[threadIdx.x] = __ldcv(&poly_vk.vwu_abstracts[(vwu_idx * vwu_abstract_length) +
                                                           threadIdx.x]);
      vwu_task_count[threadIdx.x] = vwu_map[threadIdx.x].y - vwu_map[threadIdx.x].x;
      vwu_padded_task_count[threadIdx.x] = devcRoundUp(vwu_task_count[threadIdx.x], warp_size_int);
    }
    __syncthreads();

    // Initialize the current forces on each particle so that the total force from all prior
    // contributions can be assembled.
    const int impt_llim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].x;
    const int impt_hlim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].y;
    const int impt_count  = impt_hlim - impt_llim;
    const int impt_stride = devcRoundUp(impt_hlim - impt_llim, warp_size_int);
    const bool has_virtual_sites = (vwu_task_count[(size_t)(VwuAbstractMap::VSITE)] > 0);
    int pos = threadIdx.x;
    while (pos < impt_stride) {
      if (pos < impt_count) {
        const size_t gbl_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + pos]);
#  ifdef TCALC_IS_SINGLE
#    ifdef SPLIT_FORCE_ACCUMULATION
        const int2 tmp_fx = longlongToInt63(__ldcv(&poly_psw.xfrc[gbl_read_idx]));
        sh_xfrc[pos] = tmp_fx.x;
        sh_xfrc_overflow[pos] = tmp_fx.y;
#    else
        sh_xfrc[pos] = __ldcv(&poly_psw.xfrc[gbl_read_idx]);
#    endif
        if (has_virtual_sites) {
          sh_xcrd[pos] = __ldcv(&poly_psw.xcrd[gbl_read_idx]);
        }
#  else
        sh_xfrc[pos] = __ldcv(&poly_psw.xfrc[gbl_read_idx]);
        sh_xfrc_overflow[pos] = __ldcv(&poly_psw.xfrc_ovrf[gbl_read_idx]);
        if (has_virtual_sites) {
          const size_t lcl_write_idx = pos + EXCL_GMEM_OFFSET;
          __stwb(&gmem_r.xcrd[lcl_write_idx], __ldcv(&poly_psw.xcrd[gbl_read_idx]));
          __stwb(&gmem_r.xcrd_ovrf[lcl_write_idx], __ldcv(&poly_psw.xcrd_ovrf[gbl_read_idx]));
        }
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 2 * impt_stride) {
      const int rel_pos = pos - impt_stride;
      if (rel_pos < impt_count) {
        const size_t gbl_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + rel_pos]);
#  ifdef TCALC_IS_SINGLE
#    ifdef SPLIT_FORCE_ACCUMULATION
        const int2 tmp_fy = longlongToInt63(__ldcv(&poly_psw.yfrc[gbl_read_idx]));
        sh_yfrc[rel_pos] = tmp_fy.x;
        sh_yfrc_overflow[rel_pos] = tmp_fy.y;
#    else
        sh_yfrc[rel_pos] = __ldcv(&poly_psw.yfrc[gbl_read_idx]);
#    endif
        if (has_virtual_sites) {
          sh_ycrd[rel_pos] = __ldcv(&poly_psw.ycrd[gbl_read_idx]);
        }
#  else
        sh_yfrc[rel_pos] = __ldcv(&poly_psw.yfrc[gbl_read_idx]);
        sh_yfrc_overflow[rel_pos] = __ldcv(&poly_psw.yfrc_ovrf[gbl_read_idx]);
        if (has_virtual_sites) {
          const size_t lcl_write_idx = rel_pos + EXCL_GMEM_OFFSET;
          __stwb(&gmem_r.ycrd[lcl_write_idx], __ldcv(&poly_psw.ycrd[gbl_read_idx]));
          __stwb(&gmem_r.ycrd_ovrf[lcl_write_idx], __ldcv(&poly_psw.ycrd_ovrf[gbl_read_idx]));
        }
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 3 * impt_stride) {
      const int rel_pos = pos - (2 * impt_stride);
      if (rel_pos < impt_count) {
        const size_t gbl_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + rel_pos]);
#  ifdef TCALC_IS_SINGLE
#    ifdef SPLIT_FORCE_ACCUMULATION
        const int2 tmp_fz = longlongToInt63(__ldcv(&poly_psw.zfrc[gbl_read_idx]));
        sh_zfrc[rel_pos] = tmp_fz.x;
        sh_zfrc_overflow[rel_pos] = tmp_fz.y;
#    else
        sh_zfrc[rel_pos] = __ldcv(&poly_psw.zfrc[gbl_read_idx]);
#    endif
        if (has_virtual_sites) {
          sh_zcrd[rel_pos] = __ldcv(&poly_psw.zcrd[gbl_read_idx]);
        }
#  else
        sh_zfrc[rel_pos] = __ldcv(&poly_psw.zfrc[gbl_read_idx]);
        sh_zfrc_overflow[rel_pos] = __ldcv(&poly_psw.zfrc_ovrf[gbl_read_idx]);
        if (has_virtual_sites) {
          const size_t lcl_write_idx = rel_pos + EXCL_GMEM_OFFSET;
          __stwb(&gmem_r.zcrd[lcl_write_idx], __ldcv(&poly_psw.zcrd[gbl_read_idx]));
          __stwb(&gmem_r.zcrd_ovrf[lcl_write_idx], __ldcv(&poly_psw.zcrd_ovrf[gbl_read_idx]));
        }
#  endif
      }
      pos += blockDim.x;
    }

    // Forces loaded from the synthesis must be situated before additional contributions from
    // neighbor lists can be accepted.  The standalone kernel therefore must synchronize all
    // threads' progress here.
    __syncthreads();
#endif // VERLET_STANDALONE

    // Import forces from prior computations, currently held in the global arrays.  These forces
    // will then be "burned" as the atom update is happening here.
    pos = threadIdx.x;
    while (pos < impt_stride) {
      if (pos < impt_count) {
        const size_t gbl_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + pos]);
#ifdef SPLIT_FORCE_ACCUMULATION
#  ifdef TCALC_IS_SINGLE
        int2 other_xfrc = { 0, 0 };
        int2 other_yfrc = { 0, 0 };
        int2 other_zfrc = { 0, 0 };
#  else
        int95_t other_xfrc = { 0LL, 0 };
        int95_t other_yfrc = { 0LL, 0 };
        int95_t other_zfrc = { 0LL, 0 };
#  endif
#else
        llint other_xfrc = 0LL;
        llint other_yfrc = 0LL;
        llint other_zfrc = 0LL;
#endif
        // Non-bonded forces may arise due to interactions computed between atoms indexed by the
        // topological arrangement.  If the Verlet integration is done in standalone form, these
        // forces will have already been taken in as part of the initialization.  If the Verlet
        // integration is fused to valence interaction computations, take in the extra forces now.
#ifndef VERLET_STANDALONE
        if (tstw.load_synthesis_forces || poly_psw.unit_cell == UnitCellType::NONE) {
#  ifdef TCALC_IS_SINGLE
#    ifdef SPLIT_FORCE_ACCUMULATION
          other_xfrc = longlongToInt63(__ldcv(&poly_psw.xfrc[gbl_read_idx]));
          other_yfrc = longlongToInt63(__ldcv(&poly_psw.yfrc[gbl_read_idx]));
          other_zfrc = longlongToInt63(__ldcv(&poly_psw.zfrc[gbl_read_idx]));
#    else
          other_xfrc = __ldcv(&poly_psw.xfrc[gbl_read_idx]);
          other_yfrc = __ldcv(&poly_psw.yfrc[gbl_read_idx]);
          other_zfrc = __ldcv(&poly_psw.zfrc[gbl_read_idx]);
#    endif
#  else // TCALC_IS_SINGLE
          other_xfrc = { __ldcv(&poly_psw.xfrc[gbl_read_idx]),
                         __ldcv(&poly_psw.xfrc_ovrf[gbl_read_idx]) };
          other_yfrc = { __ldcv(&poly_psw.yfrc[gbl_read_idx]),
                         __ldcv(&poly_psw.yfrc_ovrf[gbl_read_idx]) };
          other_zfrc = { __ldcv(&poly_psw.zfrc[gbl_read_idx]),
                         __ldcv(&poly_psw.zfrc_ovrf[gbl_read_idx]) };
#  endif // TCALC_IS_SINGLE
        }
#endif
        // Take in non-bonded forces from the neighbor list(s).
#ifdef PME_COMPATIBLE
#  ifdef TCALC_IS_SINGLE
#    ifdef SPLIT_FORCE_ACCUMULATION
#      ifdef DUAL_GRIDS
        const size_t qq_img_idx = __ldg(&cgr_qq.img_atom_idx[gbl_read_idx]);
        const size_t lj_img_idx = __ldg(&cgr_lj.img_atom_idx[gbl_read_idx]);
#        ifdef TCOORD_IS_LONG
        const int2 qq_xfrc = longlongToInt63(__ldg(&cgr_qq.xfrc[qq_img_idx]));
        const int2 qq_yfrc = longlongToInt63(__ldg(&cgr_qq.yfrc[qq_img_idx]));
        const int2 qq_zfrc = longlongToInt63(__ldg(&cgr_qq.zfrc[qq_img_idx]));
        const int2 lj_xfrc = longlongToInt63(__ldg(&cgr_lj.xfrc[lj_img_idx]));
        const int2 lj_yfrc = longlongToInt63(__ldg(&cgr_lj.yfrc[lj_img_idx]));
        const int2 lj_zfrc = longlongToInt63(__ldg(&cgr_lj.zfrc[lj_img_idx]));
        other_xfrc = splitFPSum(other_xfrc, qq_xfrc);
        other_yfrc = splitFPSum(other_yfrc, qq_yfrc);
        other_zfrc = splitFPSum(other_zfrc, qq_zfrc);
        other_xfrc = splitFPSum(other_xfrc, lj_xfrc);
        other_yfrc = splitFPSum(other_yfrc, lj_yfrc);
        other_zfrc = splitFPSum(other_zfrc, lj_zfrc);
#        else // TCOORD_IS_LONG
        other_xfrc = splitFPSum(other_xfrc, __ldg(&cgr_qq.xfrc[qq_img_idx]),
                                __ldg(&cgr_qq.xfrc_ovrf[qq_img_idx]));
        other_yfrc = splitFPSum(other_yfrc, __ldg(&cgr_qq.yfrc[qq_img_idx]),
                                __ldg(&cgr_qq.yfrc_ovrf[qq_img_idx]));
        other_zfrc = splitFPSum(other_zfrc, __ldg(&cgr_qq.zfrc[qq_img_idx]),
                                __ldg(&cgr_qq.zfrc_ovrf[qq_img_idx]));
        other_xfrc = splitFPSum(other_xfrc, __ldg(&cgr_lj.xfrc[lj_img_idx]),
                                 __ldg(&cgr_lj.xfrc_ovrf[lj_img_idx]));
        other_yfrc = splitFPSum(other_yfrc, __ldg(&cgr_lj.yfrc[lj_img_idx]),
                                __ldg(&cgr_lj.yfrc_ovrf[lj_img_idx]));
        other_zfrc = splitFPSum(other_zfrc, __ldg(&cgr_lj.zfrc[lj_img_idx]),
                                __ldg(&cgr_lj.zfrc_ovrf[lj_img_idx]));
#        endif // TCOORD_IS_LONG
#      else // DUAL_GRIDS
        const size_t img_idx = __ldg(&cgr.img_atom_idx[gbl_read_idx]);
#        ifdef TCOORD_IS_LONG
        const int2 cg_xfrc = longlongToInt63(__ldg(&cgr.xfrc[img_idx]));
        const int2 cg_yfrc = longlongToInt63(__ldg(&cgr.yfrc[img_idx]));
        const int2 cg_zfrc = longlongToInt63(__ldg(&cgr.zfrc[img_idx]));
#        else
        const int2 cg_xfrc = { __ldg(&cgr.xfrc[img_idx]), __ldg(&cgr.xfrc_ovrf[img_idx]) };
        const int2 cg_yfrc = { __ldg(&cgr.yfrc[img_idx]), __ldg(&cgr.yfrc_ovrf[img_idx]) };
        const int2 cg_zfrc = { __ldg(&cgr.zfrc[img_idx]), __ldg(&cgr.zfrc_ovrf[img_idx]) };
#        endif
        other_xfrc = splitFPSum(other_xfrc, cg_xfrc);
        other_yfrc = splitFPSum(other_yfrc, cg_yfrc);
        other_zfrc = splitFPSum(other_zfrc, cg_zfrc);
#      endif // DUAL_GRIDS
#    else // SPLIT_FORCE_ACCUMULATION
#      ifdef DUAL_GRIDS
        const size_t qq_img_idx = __ldg(&cgr_qq.img_atom_idx[gbl_read_idx]);
        const size_t lj_img_idx = __ldg(&cgr_lj.img_atom_idx[gbl_read_idx]);
#        ifdef TCOORD_IS_LONG
        other_xfrc += __ldg(&cgr_qq.xfrc[qq_img_idx]);
        other_yfrc += __ldg(&cgr_qq.yfrc[qq_img_idx]);
        other_zfrc += __ldg(&cgr_qq.zfrc[qq_img_idx]);
        other_xfrc += __ldg(&cgr_lj.xfrc[lj_img_idx]);
        other_yfrc += __ldg(&cgr_lj.yfrc[lj_img_idx]);
        other_zfrc += __ldg(&cgr_lj.zfrc[lj_img_idx]);
#        else // TCOORD_IS_LONG
        other_xfrc += int63ToLongLong(__ldg(&cgr_qq.xfrc[qq_img_idx]),
                                      __ldg(&cgr_qq.xfrc_ovrf[qq_img_idx]));
        other_yfrc += int63ToLongLong(__ldg(&cgr_qq.yfrc[qq_img_idx]),
                                      __ldg(&cgr_qq.yfrc_ovrf[qq_img_idx]));
        other_zfrc += int63ToLongLong(__ldg(&cgr_qq.zfrc[qq_img_idx]),
                                      __ldg(&cgr_qq.zfrc_ovrf[qq_img_idx]));
        other_xfrc += int63ToLongLong(__ldg(&cgr_lj.xfrc[lj_img_idx]),
                                      __ldg(&cgr_lj.xfrc_ovrf[lj_img_idx]));
        other_yfrc += int63ToLongLong(__ldg(&cgr_lj.yfrc[lj_img_idx]),
                                      __ldg(&cgr_lj.yfrc_ovrf[lj_img_idx]));
        other_zfrc += int63ToLongLong(__ldg(&cgr_lj.zfrc[lj_img_idx]),
                                      __ldg(&cgr_lj.zfrc_ovrf[lj_img_idx]));
#        endif // TCOORD_IS_LONG
#      else // DUAL_GRIDS
        const size_t img_idx = __ldg(&cgr.img_atom_idx[gbl_read_idx]);
#        ifdef TCOORD_IS_LONG
        other_xfrc += __ldg(&cgr.xfrc[img_idx]);
        other_yfrc += __ldg(&cgr.yfrc[img_idx]);
        other_zfrc += __ldg(&cgr.zfrc[img_idx]);
#        else // TCOORD_IS_LONG
        other_xfrc += int63ToLongLong(__ldg(&cgr.xfrc[img_idx]),
                                      __ldg(&cgr.xfrc_ovrf[img_idx]));
        other_yfrc += int63ToLongLong(__ldg(&cgr.yfrc[img_idx]),
                                      __ldg(&cgr.yfrc_ovrf[img_idx]));
        other_zfrc += int63ToLongLong(__ldg(&cgr.zfrc[img_idx]),
                                      __ldg(&cgr.zfrc_ovrf[img_idx]));
#        endif // TCOORD_IS_LONG
#      endif // DUAL_GRIDS
#    endif // SPLIT_FORCE_ACCUMULATION
#  else // TCALC_IS_SINGLE
#    ifdef DUAL_GRIDS
        const size_t qq_img_idx = __ldg(&cgr_qq.img_atom_idx[gbl_read_idx]);
        const size_t lj_img_idx = __ldg(&cgr_lj.img_atom_idx[gbl_read_idx]);
#      ifdef TCOORD_IS_LONG
        other_xfrc = splitFPSum(other_xfrc, __ldg(&cgr_qq.xfrc[qq_img_idx]),
                                __ldg(&cgr_qq.xfrc_ovrf[qq_img_idx]));
        other_yfrc = splitFPSum(other_yfrc, __ldg(&cgr_qq.yfrc[qq_img_idx]),
                                __ldg(&cgr_qq.yfrc_ovrf[qq_img_idx]));
        other_zfrc = splitFPSum(other_zfrc, __ldg(&cgr_qq.zfrc[qq_img_idx]),
                                __ldg(&cgr_qq.zfrc_ovrf[qq_img_idx]));
        other_xfrc = splitFPSum(other_xfrc, __ldg(&cgr_lj.xfrc[lj_img_idx]),
                                __ldg(&cgr_lj.xfrc_ovrf[lj_img_idx]));
        other_yfrc = splitFPSum(other_yfrc, __ldg(&cgr_lj.yfrc[lj_img_idx]),
                                __ldg(&cgr_lj.yfrc_ovrf[lj_img_idx]));
        other_zfrc = splitFPSum(other_zfrc, __ldg(&cgr_lj.zfrc[lj_img_idx]),
                                __ldg(&cgr_lj.zfrc_ovrf[lj_img_idx]));
#      else // TCOORD_IS_LONG
        const llint qq_xfrc = int63ToLongLong(__ldg(&cgr_qq.xfrc[qq_img_idx]),
                                              __ldg(&cgr_qq.xfrc_ovrf[qq_img_idx]));
        const llint qq_yfrc = int63ToLongLong(__ldg(&cgr_qq.yfrc[qq_img_idx]),
                                              __ldg(&cgr_qq.yfrc_ovrf[qq_img_idx]));
        const llint qq_zfrc = int63ToLongLong(__ldg(&cgr_qq.zfrc[qq_img_idx]),
                                              __ldg(&cgr_qq.zfrc_ovrf[qq_img_idx]));
        const llint lj_xfrc = int63ToLongLong(__ldg(&cgr_lj.xfrc[lj_img_idx]),
                                              __ldg(&cgr_lj.xfrc_ovrf[lj_img_idx]));
        const llint lj_yfrc = int63ToLongLong(__ldg(&cgr_lj.yfrc[lj_img_idx]),
                                              __ldg(&cgr_lj.yfrc_ovrf[lj_img_idx]));
        const llint lj_zfrc = int63ToLongLong(__ldg(&cgr_lj.zfrc[lj_img_idx]),
                                              __ldg(&cgr_lj.zfrc_ovrf[lj_img_idx]));
        other_xfrc = splitFPSum(other_xfrc, qq_xfrc + lj_xfrc, 0);
        other_yfrc = splitFPSum(other_yfrc, qq_yfrc + lj_xfrc, 0);
        other_zfrc = splitFPSum(other_zfrc, qq_zfrc + lj_xfrc, 0);
#      endif // TCOORD_IS_LONG
#    else // DUAL_GRIDS
        const size_t img_idx = __ldg(&cgr.img_atom_idx[gbl_read_idx]);
#      ifdef TCOORD_IS_LONG
        other_xfrc = splitFPSum(other_xfrc, __ldg(&cgr.xfrc[img_idx]),
                                __ldg(&cgr.xfrc_ovrf[img_idx]));
        other_yfrc = splitFPSum(other_xfrc, __ldg(&cgr.yfrc[img_idx]),
                                __ldg(&cgr.yfrc_ovrf[img_idx]));
        other_zfrc = splitFPSum(other_xfrc, __ldg(&cgr.zfrc[img_idx]),
                                __ldg(&cgr.zfrc_ovrf[img_idx]));
#      else // TCOORD_IS_LONG
        other_xfrc = splitFPSum(other_xfrc, int63ToLongLong(__ldg(&cgr.xfrc[img_idx]),
                                                            __ldg(&cgr.xfrc_ovrf[img_idx])), 0);
        other_yfrc = splitFPSum(other_yfrc, int63ToLongLong(__ldg(&cgr.yfrc[img_idx]),
                                                            __ldg(&cgr.yfrc_ovrf[img_idx])), 0);
        other_zfrc = splitFPSum(other_zfrc, int63ToLongLong(__ldg(&cgr.zfrc[img_idx]),
                                                            __ldg(&cgr.zfrc_ovrf[img_idx])), 0);
#      endif // TCOORD_IS_LONG
#    endif // DUAL_GRIDS
#  endif // TCALC_IS_SINGLE
#endif // PME_COMPATIBLE

        // Add the non-bonded force contributions.
#ifdef SPLIT_FORCE_ACCUMULATION
#  ifdef TCALC_IS_SINGLE
        const int2 total_xfrc = splitFPSum(other_xfrc, sh_xfrc[pos], sh_xfrc_overflow[pos]);
        const int2 total_yfrc = splitFPSum(other_yfrc, sh_yfrc[pos], sh_yfrc_overflow[pos]);
        const int2 total_zfrc = splitFPSum(other_zfrc, sh_zfrc[pos], sh_zfrc_overflow[pos]);
#  else // TCALC_IS_SINGLE
        const int95_t total_xfrc = splitFPSum(other_xfrc, sh_xfrc[pos], sh_xfrc_overflow[pos]);
        const int95_t total_yfrc = splitFPSum(other_yfrc, sh_yfrc[pos], sh_yfrc_overflow[pos]);
        const int95_t total_zfrc = splitFPSum(other_zfrc, sh_zfrc[pos], sh_zfrc_overflow[pos]);
#  endif // TCALC_IS_SINGLE
        sh_xfrc[pos] = total_xfrc.x;
        sh_yfrc[pos] = total_yfrc.x;
        sh_zfrc[pos] = total_zfrc.x;
        sh_xfrc_overflow[pos] = total_xfrc.y;
        sh_yfrc_overflow[pos] = total_yfrc.y;
        sh_zfrc_overflow[pos] = total_zfrc.y;
#else // SPLIT_FORCE_ACCUMULATION
        sh_xfrc[pos] += other_xfrc;
        sh_yfrc[pos] += other_yfrc;
        sh_zfrc[pos] += other_zfrc;
#endif // SPLIT_FORCE_ACCUMULATION
      }
      pos += blockDim.x;
    }
    
    // Transfer forces from virtual particles to their frame atoms, which have mass.  This is done
    // in preparation for the integration process.
    if (vwu_task_count[(size_t)(VwuAbstractMap::VSITE)] > 0) {
      __syncthreads();
#ifdef VERLET_STANDALONE
      int vterm_limit;
#endif
#include "Structure/virtual_site_transmission.cui"
    }

    // Synchronization is needed for all code pathways to ensure that updates to forces in the are
    // block-specific __global__ or __shared__ accumulators are complete.
    __syncthreads();
    
    // Carry out the first Velocity-Verlet velocity update.  The "inverse half mass time step"
    // factor (hmdt) is pre-scaled by the velocity scaling factor for convenience.  Virtual sites
    // have no mass, but their inverse masses have been preset to zero so motion of virtual sites
    // at this stage is nullified.
    pos = threadIdx.x;
    while (pos < impt_stride) {
      const size_t gbl_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + pos]);
      if (pos < impt_count) {
        const TCALC t_mass = __ldca(&poly_auk.masses[gbl_read_idx]);
        const TCALC hmdt = (t_mass > (TCALC)(1.0e-8)) ?
                           ((TCALC)(0.5) * tstw.dt / t_mass) * kcal_to_gafs_f *
                           poly_psw.vel_scale * poly_psw.inv_frc_scale : (TCALC)(0.0);
#ifdef TCALC_IS_SINGLE
        llint vx_update, vy_update, vz_update;
#else
        int95_t vx_update, vy_update, vz_update;
#endif
        switch (tstw.kind) {
        case ThermostatKind::LANGEVIN:
          {
            // Calculate the Langevin parameters for the current temperature.  Contribute the
            // Langevin bumps directly to the forces acting on each atom, so that they do not
            // need to be recalculated for the second Velocity-Verlet velocity update.
            TCALC current_temp;
            switch (tstw.layout) {
            case ThermostatPartition::COMMON:
              if (tstw.step <= tstw.init_evolution) {
                current_temp = tstw.init_temperature;
              }
              else if (tstw.step < tstw.end_evolution) {
                const TCALC progfac = (TCALC)(tstw.step - tstw.init_evolution) /
                                      (TCALC)(tstw.end_evolution - tstw.init_evolution);
                current_temp = (((TCALC)(1.0) - progfac) * tstw.init_temperature) +
                               (progfac * tstw.final_temperature);
              }
              else {
                current_temp = tstw.final_temperature;
              }
              break;
            case ThermostatPartition::SYSTEMS:
              {
                const int sys_idx = vwu_map[(size_t)(VwuAbstractMap::SYSTEM_ID)].x;
                if (tstw.step <= tstw.init_evolution) {
                  current_temp = __ldca(&tstw.init_temperatures[sys_idx]);
                }
                else if (tstw.step < tstw.end_evolution) {
                  const TCALC progfac = (TCALC)(tstw.step - tstw.init_evolution) /
                                        (TCALC)(tstw.end_evolution - tstw.init_evolution);
                  current_temp = (((TCALC)(1.0) - progfac) *
                                  __ldca(&tstw.init_temperatures[sys_idx])) +
                                 (progfac * __ldca(&tstw.final_temperatures[sys_idx]));
                }
                else {
                  current_temp = __ldca(&tstw.final_temperatures[sys_idx]);
                }
              }
              break;
            case ThermostatPartition::ATOMS:
              if (tstw.step <= tstw.init_evolution) {
                current_temp = __ldca(&tstw.init_temperatures[gbl_read_idx]);
              }
              else if (tstw.step < tstw.end_evolution) {
                const TCALC progfac = (TCALC)(tstw.step - tstw.init_evolution) /
                                      (TCALC)(tstw.end_evolution - tstw.init_evolution);
                current_temp = (((TCALC)(1.0) - progfac) *
                                __ldca(&tstw.init_temperatures[gbl_read_idx])) +
                               (progfac * __ldca(&tstw.final_temperatures[gbl_read_idx]));
              }
              else {
                current_temp = __ldca(&tstw.final_temperatures[gbl_read_idx]);
              }
              break;
            }
            const TCALC sdfac = (ABS_FUNC(t_mass) > (TCALC)(1.0e-8)) ?
                                SQRT_FUNC(tstw.gamma_ln * boltzmann_constant_f * current_temp *
                                          t_mass / (kcal_to_gafs_f * (TCALC)(0.5) * tstw.dt)) *
                                poly_psw.frc_scale : (TCALC)(0.0);
            const size_t cache_idx = ((size_t)(3 * (tstw.step % tstw.depth)) *
                                      (size_t)(tstw.padded_natom)) + gbl_read_idx;
            TCALC bump_xfrc, bump_yfrc, bump_zfrc;
            switch (tstw.rng_mode) {
            case PrecisionModel::DOUBLE:
              bump_xfrc = sdfac * __ldcv(&tstw.cache[cache_idx]);
              bump_yfrc = sdfac * __ldcv(&tstw.cache[cache_idx + tstw.padded_natom]);
              bump_zfrc = sdfac * __ldcv(&tstw.cache[cache_idx + (2 * tstw.padded_natom)]);
              break;
            case PrecisionModel::SINGLE:
              bump_xfrc = sdfac * __ldcv(&tstw.sp_cache[cache_idx]);
              bump_yfrc = sdfac * __ldcv(&tstw.sp_cache[cache_idx + tstw.padded_natom]);
              bump_zfrc = sdfac * __ldcv(&tstw.sp_cache[cache_idx + (2 * tstw.padded_natom)]);
              break;
            }
#ifdef SPLIT_FORCE_ACCUMULATION
#  ifdef TCALC_IS_SINGLE
            const int2 ibump_xfrc  = floatToInt63(bump_xfrc);
            const int2 ibump_yfrc  = floatToInt63(bump_yfrc);
            const int2 ibump_zfrc  = floatToInt63(bump_zfrc);
            const int2 xfrc_update = splitFPSum(ibump_xfrc, sh_xfrc[pos],
                                                sh_xfrc_overflow[pos]);
            const int2 yfrc_update = splitFPSum(ibump_yfrc, sh_yfrc[pos],
                                                sh_yfrc_overflow[pos]);
            const int2 zfrc_update = splitFPSum(ibump_zfrc, sh_zfrc[pos],
                                                sh_zfrc_overflow[pos]);
#  else
            const int95_t ibump_xfrc  = doubleToInt95(bump_xfrc);
            const int95_t ibump_yfrc  = doubleToInt95(bump_yfrc);
            const int95_t ibump_zfrc  = doubleToInt95(bump_zfrc);
            const int95_t xfrc_update = splitFPSum(ibump_xfrc, sh_xfrc[pos],
                                                   sh_xfrc_overflow[pos]);
            const int95_t yfrc_update = splitFPSum(ibump_yfrc, sh_yfrc[pos],
                                                   sh_yfrc_overflow[pos]);
            const int95_t zfrc_update = splitFPSum(ibump_zfrc, sh_zfrc[pos],
                                                   sh_zfrc_overflow[pos]);
#  endif
            sh_xfrc[pos]          = xfrc_update.x;
            sh_yfrc[pos]          = yfrc_update.x;
            sh_zfrc[pos]          = zfrc_update.x;
            sh_xfrc_overflow[pos] = xfrc_update.y;
            sh_yfrc_overflow[pos] = yfrc_update.y;
            sh_zfrc_overflow[pos] = zfrc_update.y;
#else
            const llint ibump_xfrc = LLCONV_FUNC(bump_xfrc);
            const llint ibump_yfrc = LLCONV_FUNC(bump_yfrc);
            const llint ibump_zfrc = LLCONV_FUNC(bump_zfrc);
            sh_xfrc[pos] += ibump_xfrc;
            sh_yfrc[pos] += ibump_yfrc;
            sh_zfrc[pos] += ibump_zfrc;
#endif
            // Update the velocities by the implicit Langevin step
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
            const llint vx_delta = LLCONV_FUNC(hmdt *
                                               int63ToFloat(sh_xfrc[pos], sh_xfrc_overflow[pos]));
            const llint vy_delta = LLCONV_FUNC(hmdt *
                                               int63ToFloat(sh_yfrc[pos], sh_yfrc_overflow[pos]));
            const llint vz_delta = LLCONV_FUNC(hmdt *
                                               int63ToFloat(sh_zfrc[pos], sh_zfrc_overflow[pos]));
#  else
            const llint vx_delta = LLCONV_FUNC(hmdt * (TCALC)(sh_xfrc[pos]));
            const llint vy_delta = LLCONV_FUNC(hmdt * (TCALC)(sh_yfrc[pos]));
            const llint vz_delta = LLCONV_FUNC(hmdt * (TCALC)(sh_zfrc[pos]));
#  endif
            vx_update = LLCONV_FUNC((TCALC)(vx_delta + __ldcv(&poly_psw.xvel[gbl_read_idx])) *
                                    tstw.ln_implicit);
            vy_update = LLCONV_FUNC((TCALC)(vy_delta + __ldcv(&poly_psw.yvel[gbl_read_idx])) *
                                    tstw.ln_implicit);
            vz_update = LLCONV_FUNC((TCALC)(vz_delta + __ldcv(&poly_psw.zvel[gbl_read_idx])) *
                                    tstw.ln_implicit);
#else
            int95_t vx_delta = doubleToInt95(hmdt * int95ToDouble(sh_xfrc[pos],
                                                                  sh_xfrc_overflow[pos]));
            int95_t vy_delta = doubleToInt95(hmdt * int95ToDouble(sh_yfrc[pos],
                                                                  sh_yfrc_overflow[pos]));
            int95_t vz_delta = doubleToInt95(hmdt * int95ToDouble(sh_zfrc[pos],
                                                                  sh_zfrc_overflow[pos]));
            vx_delta = splitFPSum(vx_delta, __ldcv(&poly_psw.xvel[gbl_read_idx]),
                                  __ldcv(&poly_psw.xvel_ovrf[gbl_read_idx]));
            vy_delta = splitFPSum(vy_delta, __ldcv(&poly_psw.yvel[gbl_read_idx]),
                                  __ldcv(&poly_psw.yvel_ovrf[gbl_read_idx]));
            vz_delta = splitFPSum(vz_delta, __ldcv(&poly_psw.zvel[gbl_read_idx]),
                                  __ldcv(&poly_psw.zvel_ovrf[gbl_read_idx]));
            vx_update = doubleToInt95(tstw.ln_implicit * splitFPToReal(vx_delta));
            vy_update = doubleToInt95(tstw.ln_implicit * splitFPToReal(vy_delta));
            vz_update = doubleToInt95(tstw.ln_implicit * splitFPToReal(vz_delta));
#endif
          }
          break;
        case ThermostatKind::NONE:
        case ThermostatKind::ANDERSEN:
        case ThermostatKind::BERENDSEN:
          {
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
            const llint vx_delta = LLCONV_FUNC(hmdt *
                                               int63ToFloat(sh_xfrc[pos], sh_xfrc_overflow[pos]));
            const llint vy_delta = LLCONV_FUNC(hmdt *
                                               int63ToFloat(sh_yfrc[pos], sh_yfrc_overflow[pos]));
            const llint vz_delta = LLCONV_FUNC(hmdt *
                                               int63ToFloat(sh_zfrc[pos], sh_zfrc_overflow[pos]));
#  else
            const llint vx_delta = LLCONV_FUNC(hmdt * (TCALC)(sh_xfrc[pos]));
            const llint vy_delta = LLCONV_FUNC(hmdt * (TCALC)(sh_yfrc[pos]));
            const llint vz_delta = LLCONV_FUNC(hmdt * (TCALC)(sh_zfrc[pos]));
#  endif
            vx_update = vx_delta + __ldcv(&poly_psw.xvel[gbl_read_idx]);
            vy_update = vy_delta + __ldcv(&poly_psw.yvel[gbl_read_idx]);
            vz_update = vz_delta + __ldcv(&poly_psw.zvel[gbl_read_idx]);
#else
            const int95_t vx_delta = doubleToInt95(hmdt * int95ToDouble(sh_xfrc[pos],
                                                                        sh_xfrc_overflow[pos]));
            const int95_t vy_delta = doubleToInt95(hmdt * int95ToDouble(sh_yfrc[pos],
                                                                        sh_yfrc_overflow[pos]));
            const int95_t vz_delta = doubleToInt95(hmdt * int95ToDouble(sh_zfrc[pos],
                                                                        sh_zfrc_overflow[pos]));
            vx_update = splitFPSum(vx_delta, __ldcv(&poly_psw.xvel[gbl_read_idx]),
                                   __ldcv(&poly_psw.xvel_ovrf[gbl_read_idx]));
            vy_update = splitFPSum(vy_delta, __ldcv(&poly_psw.yvel[gbl_read_idx]),
                                   __ldcv(&poly_psw.yvel_ovrf[gbl_read_idx]));
            vz_update = splitFPSum(vz_delta, __ldcv(&poly_psw.zvel[gbl_read_idx]),
                                   __ldcv(&poly_psw.zvel_ovrf[gbl_read_idx]));
#endif
          }
          break;
        }

        // Store the updated velocities so that constraints may be applied or that kinetic energy
        // may be computed.  In the standalone form or a partially fused kernel, the velocities
        // will be written to persistent arrays in __global__ memory, while in a kernel that fuses
        // all of the integration work the velocities will be written back to local, block-specific
        // temporary storage.  If the kernel is not entirely fused, the compiled and complete
        // forces on each particle (after transmitting forces on virtual sites) will also be
        // written back to persistent arrays in __global__ memory.
        const size_t gbl_write_idx = __ldca(&poly_vk.vwu_imports[impt_llim + pos]);
#ifdef  VERLET_STANDALONE
#  ifdef TCALC_IS_SINGLE
        __stwt(&poly_psw.vxalt[gbl_write_idx], vx_update);
        __stwt(&poly_psw.vyalt[gbl_write_idx], vy_update);
        __stwt(&poly_psw.vzalt[gbl_write_idx], vz_update);
#  else
        __stwt(&poly_psw.vxalt[gbl_write_idx], vx_update.x);
        __stwt(&poly_psw.vyalt[gbl_write_idx], vy_update.x);
        __stwt(&poly_psw.vzalt[gbl_write_idx], vz_update.x);
        __stwt(&poly_psw.vxalt_ovrf[gbl_write_idx], vx_update.y);
        __stwt(&poly_psw.vyalt_ovrf[gbl_write_idx], vy_update.y);
        __stwt(&poly_psw.vzalt_ovrf[gbl_write_idx], vz_update.y);
#  endif
#else // VERLET_STANDALONE
        const size_t lcl_write_idx = EXCL_GMEM_OFFSET + pos;
#  ifdef TCALC_IS_SINGLE
        __stwb(&gmem_r.xvel[lcl_write_idx], vx_update);
        __stwb(&gmem_r.yvel[lcl_write_idx], vy_update);
        __stwb(&gmem_r.zvel[lcl_write_idx], vz_update);
#  else
        __stwb(&gmem_r.xvel[lcl_write_idx],      vx_update.x);
        __stwb(&gmem_r.yvel[lcl_write_idx],      vy_update.x);
        __stwb(&gmem_r.zvel[lcl_write_idx],      vz_update.x);
        __stwb(&gmem_r.xvel_ovrf[lcl_write_idx], vx_update.y);
        __stwb(&gmem_r.yvel_ovrf[lcl_write_idx], vy_update.y);
        __stwb(&gmem_r.zvel_ovrf[lcl_write_idx], vz_update.y);
#  endif
#endif // VERLET_STANDALONE

        // Store the forces in the event that they have been modified and will need to be accessed
        // by a separate kernel.  The subsequent Verlet velocity and position update will assume
        // that all forces are contained in the synthesis.  This can be skipped if neither periodic
        // boundary conditions nor a Langevin thermostat are in effect.  To avoid writing the same
        // array that is being read (and potentially creating a race condition, if two or more work
        // units access the same atoms), the "alternate" force arrays must be used.  In situations
        // where forces are being stored for a second kernel to access them, a third kernel will
        // need to be invoked to clear the alternate force arrays in preparation for the next time
        // step.  Otherwise, in a fused kernel that performs all of the particle movement, the
        // alternate force arrays can be initialized now.
        const int manip_llim    = vwu_map[(size_t)(VwuAbstractMap::MANIPULATE)].x;
        const int manip_segment = (pos >> 5);
        const int manip_bitpos  = (pos & 0x1f);
        const uint2 manip_mask = __ldca(&poly_auk.vwu_manip[manip_llim + manip_segment]);
#ifdef VERLET_STANDALONE
#  ifndef PME_COMPATIBLE
        switch (tstw.kind) {
        case ThermostatKind::LANGEVIN:
#  endif
          if ((manip_mask.y >> manip_bitpos) & 0x1) {
#  ifdef TCALC_IS_SINGLE
#    ifdef SPLIT_FORCE_ACCUMULATION
            __stwt(&poly_psw.fxalt[gbl_write_idx],
                   int63ToLongLong(sh_xfrc[pos], sh_xfrc_overflow[pos]));
            __stwt(&poly_psw.fyalt[gbl_write_idx],
                   int63ToLongLong(sh_yfrc[pos], sh_yfrc_overflow[pos]));
            __stwt(&poly_psw.fzalt[gbl_write_idx],
                   int63ToLongLong(sh_zfrc[pos], sh_zfrc_overflow[pos]));
#    else
            __stwt(&poly_psw.fxalt[gbl_write_idx], sh_xfrc[pos]);
            __stwt(&poly_psw.fyalt[gbl_write_idx], sh_yfrc[pos]);
            __stwt(&poly_psw.fzalt[gbl_write_idx], sh_zfrc[pos]);
#    endif
#  else // SPLIT_FORCE_ACCUMULATION
            __stwt(&poly_psw.fxalt[gbl_write_idx], sh_xfrc[pos]);
            __stwt(&poly_psw.fyalt[gbl_write_idx], sh_yfrc[pos]);
            __stwt(&poly_psw.fzalt[gbl_write_idx], sh_zfrc[pos]);
            __stwt(&poly_psw.fxalt_ovrf[gbl_write_idx], sh_xfrc_overflow[pos]);
            __stwt(&poly_psw.fyalt_ovrf[gbl_write_idx], sh_yfrc_overflow[pos]);
            __stwt(&poly_psw.fzalt_ovrf[gbl_write_idx], sh_zfrc_overflow[pos]);
#  endif // SPLIT_FORCE_ACCUMULATION
            }
#  ifndef PME_COMPATIBLE
          break;
        case ThermostatKind::NONE:
        case ThermostatKind::ANDERSEN:
        case ThermostatKind::BERENDSEN:
          break;
        }
#  endif
#else // VERLET_STANDALONE
        if ((manip_mask.y >> manip_bitpos) & 0x1) {
          __stwt(&poly_psw.fxalt[gbl_write_idx], 0LL);
          __stwt(&poly_psw.fyalt[gbl_write_idx], 0LL);
          __stwt(&poly_psw.fzalt[gbl_write_idx], 0LL);
#  ifndef TCALC_IS_SINGLE
          __stwt(&poly_psw.fxalt_ovrf[gbl_write_idx], 0LL);
          __stwt(&poly_psw.fyalt_ovrf[gbl_write_idx], 0LL);
          __stwt(&poly_psw.fzalt_ovrf[gbl_write_idx], 0LL);
#  endif
        }
#endif // VERLET_STANDALONE
      }
      pos += blockDim.x;
    }

    // No additional synchronization occurs for the core of the code, as anywhere this code is
    // included will be expected to have a __syncthreads() call immediately following it.
#ifdef VERLET_STANDALONE
    // Proceed to the next valence work unit.
    __syncthreads();
    if (threadIdx.x == 0) {
      const size_t prog_counter_idx = (ctrl.step & twice_warp_bits_mask_int);
      vwu_idx = atomicAdd(&ctrl.vupt_progress[prog_counter_idx], 1);
    }
    __syncthreads();
  } // Close the loop over all valence work units

  // Set the block counters for future iterations of this kernel
  if (blockIdx.x == 0 && threadIdx.x < warp_size_int) {
    const int step_modulus = (ctrl.step & twice_warp_bits_mask_int);
    if (step_modulus == 0) {
      ctrl.vupt_progress[threadIdx.x + warp_size_int] = gridDim.x;
    }
    if (step_modulus == warp_size_int) {
      ctrl.vupt_progress[threadIdx.x] = gridDim.x;
    }
  }
}

// Clear definitions of the valence atom capacity and the offset into the thread-block exclusive
// cache space.
#  undef VALENCE_ATOM_CAPACITY
#  undef EXCL_GMEM_OFFSET
#endif // VERLET_STANDALONE    
  
