// -*-c++-*-
#include "copyright.h"

// Define the block's atom capacity, if this code is not included in another block where it is
// already defined.  See Structure/valence_potential.cui for more details.
#ifdef VERLET_STANDALONE
#  define EXCL_GMEM_OFFSET  (blockIdx.x * gmem_r.max_atoms)
#  ifdef TCALC_IS_SINGLE
#    if INTEG_KERNEL_THREAD_COUNT == 64
#      define VALENCE_ATOM_CAPACITY eighth_valence_work_unit_atoms
#    elif INTEG_KERNEL_THREAD_COUNT <= 128
#      define VALENCE_ATOM_CAPACITY quarter_valence_work_unit_atoms
#    elif INTEG_KERNEL_THREAD_COUNT <= 256
#      define VALENCE_ATOM_CAPACITY half_valence_work_unit_atoms
#    else
#      define VALENCE_ATOM_CAPACITY maximum_valence_work_unit_atoms
#    endif
#  else
#    define VALENCE_ATOM_CAPACITY maximum_valence_work_unit_atoms
#  endif
#endif

/// \brief In standalone form, the kinetic energy computation will read directly from the
///        developing velocities.  If the code is included as part of a fused kernel, the relevant
///        velocities are expected to reside in block-specific arrays.  The result will be
///        accumulated in __shared__ memory, then added to an accumulator for the correct system
///        in __global__ memory, which is expected to have been initialized prior to calling the
///        kernel.
#ifdef VERLET_STANDALONE
__global__ void __launch_bounds__(INTEG_KERNEL_THREAD_COUNT, INTEG_BLOCK_MULTIPLICITY)
KERNEL_NAME(const SyValenceKit<TCALC> poly_vk, MMControlKit<TCALC> ctrl,
            PsSynthesisWriter poly_psw, const SyAtomUpdateKit<TCALC, TCALC2, TCALC4> poly_auk,
            ScoreCardWriter scw) {

  // In standalone mode, the valence work unit still needs to be collected.
  __shared__ int2 vwu_map[vwu_abstract_length];
#  ifdef PME_COMPATIBLE
  __shared__ int vwu_task_count[vwu_abstract_length];
#  endif
  __shared__ llint sh_knrg_acc[INTEG_KERNEL_THREAD_COUNT >> warp_bits];
  __shared__ volatile int vwu_idx;

  // Each block takes its first valence work unit based on its block index.  In the standalone
  // form of the kernel, the calculation is so short that asynchronous scheduling is likely to be
  // unhelpful.
  if (threadIdx.x == 0) {
    vwu_idx = blockIdx.x;
  }
  __syncthreads();
  while (vwu_idx < poly_vk.nvwu) {

    // The instruction set map is read and stored in __shared__ for convenience, and to ensure
    // that it never leaves cache.  The instructions themselves are "streamed," which for purposes
    // of this documentation means read from global, used once, and not cached.  Each block must be
    // at least vwu_abstract_length in size.
    if (threadIdx.x < vwu_abstract_length) {
      vwu_map[threadIdx.x] = __ldcv(&poly_vk.vwu_abstracts[(vwu_idx * vwu_abstract_length) +
                                                           threadIdx.x]);
    }
    __syncthreads();

    // The calculation of kinetic energy requires no further initialization.
    const int impt_llim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].x;
    const int impt_hlim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].y;
    const int impt_count  = impt_hlim - impt_llim;
    const int impt_stride = devcRoundUp(impt_hlim - impt_llim, warp_size_int);
    int pos = threadIdx.x;
#else // VERLET_STANDALONE
    pos = threadIdx.x;
#endif // VERLET_STANDALONE
    // The work unit will record the kinetic energy of any atoms it is responsible for updating.
    llint knrg_acc = 0LL;
    while (pos < impt_stride) {
      const int manip_llim    = vwu_map[(size_t)(VwuAbstractMap::MANIPULATE)].x;
      const int manip_segment = (pos >> 5);
      const int manip_bitpos  = (pos & 0x1f);
      const uint2 manip_mask = poly_auk.vwu_manip[manip_llim + manip_segment];
      if (pos < impt_count && ((manip_mask.y >> manip_bitpos) & 0x1)) {
#ifdef VERLET_STANDALONE
        const size_t gbl_read_idx = __ldcv(&poly_vk.vwu_imports[impt_llim + pos]);
#else
        const size_t gbl_read_idx = __ldca(&poly_vk.vwu_imports[impt_llim + pos]);
#endif
        const TCALC t_mass = __ldca(&poly_auk.masses[gbl_read_idx]);
#ifdef VERLET_STANDALONE
#  ifdef TCALC_IS_SINGLE
        const TCALC vx = (TCALC)(poly_psw.vxalt[gbl_read_idx]) * poly_psw.inv_vel_scale;
        const TCALC vy = (TCALC)(poly_psw.vyalt[gbl_read_idx]) * poly_psw.inv_vel_scale;
        const TCALC vz = (TCALC)(poly_psw.vzalt[gbl_read_idx]) * poly_psw.inv_vel_scale;
#  else // TCALC_IS_SINGLE
        const TCALC vx = int95ToDouble(poly_psw.vxalt[gbl_read_idx],
                                       poly_psw.vxalt_ovrf[gbl_read_idx]) *
                         poly_psw.inv_vel_scale;
        const TCALC vy = int95ToDouble(poly_psw.vyalt[gbl_read_idx],
                                       poly_psw.vyalt_ovrf[gbl_read_idx]) *
                         poly_psw.inv_vel_scale;
        const TCALC vz = int95ToDouble(poly_psw.vzalt[gbl_read_idx],
                                       poly_psw.vzalt_ovrf[gbl_read_idx]) *
                         poly_psw.inv_vel_scale;
#  endif // TCALC_IS_SINGLE
#else // VERLET_STANDALONE
        const size_t lcl_read_idx = EXCL_GMEM_OFFSET + pos;
#  ifdef TCALC_IS_SINGLE
        const TCALC vx = (TCALC)(gmem_r.xvel[lcl_read_idx]) * poly_psw.inv_vel_scale;
        const TCALC vy = (TCALC)(gmem_r.yvel[lcl_read_idx]) * poly_psw.inv_vel_scale;
        const TCALC vz = (TCALC)(gmem_r.zvel[lcl_read_idx]) * poly_psw.inv_vel_scale;
#  else // TCALC_IS_SINGLE
        const TCALC vx = int95ToDouble(gmem_r.xvel[lcl_read_idx],
                                       gmem_r.xvel_ovrf[lcl_read_idx]) * poly_psw.inv_vel_scale;
        const TCALC vy = int95ToDouble(gmem_r.yvel[lcl_read_idx],
                                       gmem_r.yvel_ovrf[lcl_read_idx]) * poly_psw.inv_vel_scale;
        const TCALC vz = int95ToDouble(gmem_r.zvel[lcl_read_idx],
                                       gmem_r.zvel_ovrf[lcl_read_idx]) * poly_psw.inv_vel_scale;
#  endif // TCALC_IS_SINGLE
#endif // VERLET_STANDALONE
        const TCALC knrg_contrib = (TCALC)(0.5) * ((vx * vx) + (vy * vy) + (vz * vz)) * t_mass;
        knrg_acc += LLCONV_FUNC(knrg_contrib * gafs_to_kcal_f * scw.nrg_scale_f);
      }
      pos += blockDim.x;
    }
    WARP_REDUCE_DOWN(knrg_acc);
    if ((threadIdx.x & warp_bits_mask_int) == 0) {
      sh_knrg_acc[(threadIdx.x >> warp_bits)] = knrg_acc;
    }

    // The synchronization needed to ensure that further updates to the velocities do not
    // contaminate the kinetic energy result serves as a means of collecting the warp sums.  This
    // aspect of the energy is not accumulated at the same time as others, because the others are
    // computed whether atoms are updated or not.
    __syncthreads();
    if (threadIdx.x < warp_size_int) {
      llint sum_knrg_acc = (threadIdx.x < (blockDim.x >> warp_bits)) ?
                           sh_knrg_acc[threadIdx.x] : 0LL;
      WARP_REDUCE_DOWN(sum_knrg_acc);
      if (threadIdx.x == 0) {
        const size_t sv_pos = (int)(StateVariable::KINETIC) +
                              (vwu_map[(size_t)(VwuAbstractMap::SYSTEM_ID)].x * scw.data_stride);
        atomicAdd((ullint*)&scw.instantaneous_accumulators[sv_pos], (ullint)(sum_knrg_acc));
      }
    }
#ifdef VERLET_STANDALONE
    // Proceed to the next valence work unit.
    __syncthreads();
    if (threadIdx.x == 0) {
      vwu_idx += gridDim.x;
    }
    __syncthreads();
  }
}
#endif // VERLET_STANDALONE
