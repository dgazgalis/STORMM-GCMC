// -*-c++-*-
#include "copyright.h"

#define EXCL_GMEM_OFFSET  (blockIdx.x * gmem_r.max_atoms)

// The "standalone" pre-processor directive constructs a separate kernel to implement the virtual
// site placement.  Without the standalone directive, the code still assumes the availability of
// critical variables, including a phase space synthesis writeable abstract named poly_psw, a
// const valence kit (which includes valence work units and imported atom maps) named poly_vk,
// and a cache resource kit named gmem_r.
#ifdef VSITE_STANDALONE
__global__ void __launch_bounds__(small_block_size, 4)
KERNEL_NAME(PsSynthesisWriter poly_psw, const SyValenceKit<TCALC> poly_vk,
            const SyAtomUpdateKit<TCALC2, TCALC4> poly_auk, CacheResourceKit<TCALC> gmem_r) {
  __shared__ llint sh_xfrc[maximum_valence_work_unit_atoms];
  __shared__ llint sh_yfrc[maximum_valence_work_unit_atoms];
  __shared__ llint sh_zfrc[maximum_valence_work_unit_atoms];
#  ifndef TCALC_IS_SINGLE
  __shared__ int sh_xfrc_overflow[maximum_valence_work_unit_atoms];
  __shared__ int sh_yfrc_overflow[maximum_valence_work_unit_atoms];
  __shared__ int sh_zfrc_overflow[maximum_valence_work_unit_atoms];
#  endif
  __shared__ int2 vwu_map[vwu_abstract_length];
  __shared__ int vwu_task_count[vwu_abstract_length];
  __shared__ int vwu_padded_task_count[vwu_abstract_length];
  __shared__ int vwu_idx;

  if (threadIdx.x == 0) {
    vwu_idx = blockIdx.x;
  }
  __syncthreads();
  while (vwu_idx < poly_vk.nvwu) {

    // The instruction set map is read and stored in __shared__ for convenience, and to ensure
    // that it never leaves cache.  The instructions themselves are "streamed," which for purposes
    // of this documentation means read from global, used once, and not cached.  Each block must be
    // at least vwu_abstract_length + warp_size_int in size.  Currently, that is 96 on NVIDIA GPUs
    // 128 on commodity AMD GPUs, and only 64 on Intel GPUs.
    if (threadIdx.x < vwu_abstract_length) {
      vwu_map[threadIdx.x] = __ldcv(&poly_vk.vwu_abstracts[(vwu_idx * vwu_abstract_length) +
                                                           threadIdx.x]);
      vwu_task_count[threadIdx.x] = vwu_map[threadIdx.x].y - vwu_map[threadIdx.x].x;
      vwu_padded_task_count[threadIdx.x] = devcRoundUp(vwu_task_count[threadIdx.x], warp_size_int);
    }
    __syncthreads();

    // Import atomic coordinates, properties, and (if appropriate) velocities.  This employs all
    // threads of the block, breaking up each set of information at the warp level.
    const int import_llim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].x;
    const int import_hlim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].y;
    const int import_count  = import_hlim - import_llim;
    const int import_stride = devcRoundUp(import_hlim - import_llim, warp_size_int);
    int pos = threadIdx.x;
    while (pos < import_stride) {
      if (pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + pos;
        __stwb(&gmem_r.xcrd[write_idx], __ldcs(&poly_psw.xcrd[read_idx]));
        sh_xfrc[pos] = __ldcs(&poly_psw.xfrc[read_idx]);
#  ifndef TCALC_IS_SINGLE
        __stwb(&gmem_r.xcrd_ovrf[write_idx], __ldcs(&poly_psw.xcrd_ovrf[read_idx]));
        sh_xfrc_overflow[pos] = __ldcs(&poly_psw.xfrc_ovrf[read_idx]);
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 2 * import_stride) {
      const int rel_pos = pos - import_stride;
      if (rel_pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.ycrd[write_idx], __ldcs(&poly_psw.ycrd[read_idx]));
        sh_yfrc[pos] = __ldcs(&poly_psw.yfrc[read_idx]);
#  ifndef TCALC_IS_SINGLE
        __stwb(&gmem_r.ycrd_ovrf[write_idx], __ldcs(&poly_psw.ycrd_ovrf[read_idx]));
        sh_yfrc_overflow[pos] = __ldcs(&poly_psw.yfrc_ovrf[read_idx]);
#  endif
      }
      pos += blockDim.x;
    }
    while (pos < 3 * import_stride) {
      const int rel_pos = pos - (2 * import_stride);
      if (rel_pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.zcrd[write_idx], __ldcs(&poly_psw.zcrd[read_idx]));
        sh_zfrc[pos] = __ldcs(&poly_psw.zfrc[read_idx]);
#  ifndef TCALC_IS_SINGLE
        __stwb(&gmem_r.zcrd_ovrf[write_idx], __ldcs(&poly_psw.zcrd_ovrf[read_idx]));
        sh_zfrc_overflow[pos] = __ldcs(&poly_psw.zfrc_ovrf[read_idx]);
#  endif
      }
      pos += blockDim.x;
    }
    __syncthreads();
    int vterm_limit;
#endif

    // Similar to the process with valence interaction terms, loop over all virtual site frames.
    pos = threadIdx.x;
    vterm_limit = vwu_padded_task_count[(size_t)(VwuAbstractMap::VSITE)];
    while (pos < vterm_limit) {
      if (pos < vwu_task_count[(size_t)(VwuAbstractMap::VSITE)]) {

        // Obtain the instruction
        const int task_offset = vwu_map[(size_t)(VwuAbstractMap::VSITE)].x;
        const uint2 tinsr = __ldcs(&poly_auk.vste_insr[task_offset + pos]);
        const int vs_atom = (tinsr.x & 0x3ff);
        int parent_atom = ((tinsr.x >> 10) & 0x3ff) + EXCL_GMEM_OFFSET;
        int frame2_atom = ((tinsr.x >> 20) & 0x3ff) + EXCL_GMEM_OFFSET;
        const int fr_param_idx = ((tinsr.y >> 20) & 0xfff);
        const TCALC4 fr_details = poly_auk.vs_params[fr_param_idx];
        const VirtualSiteKind kind = static_cast<VirtualSiteKind>(fr_details.w);

        // The parent (first) and second frame atoms' locations must be read for any frame
        const llint parent_x = __ldca(&gmem_r.xcrd[parent_atom]);
        const llint parent_y = __ldca(&gmem_r.ycrd[parent_atom]);
        const llint parent_z = __ldca(&gmem_r.zcrd[parent_atom]);
        const llint frame2_x = __ldca(&gmem_r.xcrd[frame2_atom]);
        const llint frame2_y = __ldca(&gmem_r.ycrd[frame2_atom]);
        const llint frame2_z = __ldca(&gmem_r.zcrd[frame2_atom]);
#ifndef TCALC_IS_SINGLE
        const int parent_x_ovrf =  __ldca(&gmem_r.xcrd_ovrf[parent_atom]);
        const int parent_y_ovrf =  __ldca(&gmem_r.ycrd_ovrf[parent_atom]);
        const int parent_z_ovrf =  __ldca(&gmem_r.zcrd_ovrf[parent_atom]);
        const int frame2_x_ovrf =  __ldca(&gmem_r.xcrd_ovrf[frame2_atom]);
        const int frame2_y_ovrf =  __ldca(&gmem_r.ycrd_ovrf[frame2_atom]);
        const int frame2_z_ovrf =  __ldca(&gmem_r.zcrd_ovrf[frame2_atom]);
#endif
        parent_atom -= EXCL_GMEM_OFFSET;
        frame2_atom -= EXCL_GMEM_OFFSET;
        switch (kind) {
        case VirtualSiteKind::FLEX_2:
          {
            const TCALC part_mult = (TCALC)(1.0) - fr_details.x;
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
            // When the code is included inside a valence interaction kernel and TCALC_IS_SINGLE,
            // SPLIT_FORCE_ACCUMULATION may or may not be defined.  If it is, work in the split
            // integer accumulation.
            const TCALC vs_x = int63ToFloat(sh_xfrc[vs_atom], sh_xfrc_overflow[vs_atom]);
            const TCALC vs_y = int63ToFloat(sh_yfrc[vs_atom], sh_yfrc_overflow[vs_atom]);
            const TCALC vs_z = int63ToFloat(sh_zfrc[vs_atom], sh_zfrc_overflow[vs_atom]);
            const int2 xpart = floatToInt63(part_mult * vs_x);
            const int2 ypart = floatToInt63(part_mult * vs_y);
            const int2 zpart = floatToInt63(part_mult * vs_z);
            atomicSplit(xpart, parent_atom, sh_xfrc, sh_xfrc_overflow);
            atomicSplit(ypart, parent_atom, sh_yfrc, sh_yfrc_overflow);
            atomicSplit(zpart, parent_atom, sh_zfrc, sh_zfrc_overflow);
            const int2 xrmdr = int63Sum(sh_xfrc[vs_atom], sh_xfrc_overflow[vs_atom], -xpart.x,
                                        -xpart.y);
            const int2 yrmdr = int63Sum(sh_yfrc[vs_atom], sh_yfrc_overflow[vs_atom], -ypart.x,
                                        -ypart.y);
            const int2 zrmdr = int63Sum(sh_zfrc[vs_atom], sh_zfrc_overflow[vs_atom], -zpart.x,
                                        -zpart.y);
            atomicSplit(xrmdr, frame2_atom, sh_xfrc, sh_xfrc_overflow);
            atomicSplit(yrmdr, frame2_atom, sh_yfrc, sh_yfrc_overflow);
            atomicSplit(zrmdr, frame2_atom, sh_zfrc, sh_zfrc_overflow);
#  else
            const llint xpart = LLCONV_FUNC(part_mult * (TCALC)(sh_xfrc[vs_atom]));
            const llint ypart = LLCONV_FUNC(part_mult * (TCALC)(sh_yfrc[vs_atom]));
            const llint zpart = LLCONV_FUNC(part_mult * (TCALC)(sh_zfrc[vs_atom]));
            atomicAdd((ullint*)&sh_xfrc[parent_atom], (ullint)(xpart));
            atomicAdd((ullint*)&sh_yfrc[parent_atom], (ullint)(ypart));
            atomicAdd((ullint*)&sh_zfrc[parent_atom], (ullint)(zpart));
            atomicAdd((ullint*)&sh_xfrc[frame2_atom], (ullint)(sh_xfrc[vs_atom] - xpart));
            atomicAdd((ullint*)&sh_yfrc[frame2_atom], (ullint)(sh_yfrc[vs_atom] - ypart));
            atomicAdd((ullint*)&sh_zfrc[frame2_atom], (ullint)(sh_zfrc[vs_atom] - zpart));
#  endif
#else
            const TCALC vs_x = int95ToDouble(sh_xfrc[vs_atom], sh_xfrc_overflow[vs_atom]);
            const TCALC vs_y = int95ToDouble(sh_yfrc[vs_atom], sh_yfrc_overflow[vs_atom]);
            const TCALC vs_z = int95ToDouble(sh_zfrc[vs_atom], sh_zfrc_overflow[vs_atom]);
            const int95_t xpart = doubleToInt95(part_mult * vs_x);
            const int95_t ypart = doubleToInt95(part_mult * vs_y);
            const int95_t zpart = doubleToInt95(part_mult * vs_z);
            atomicSplit(xpart, parent_atom, sh_xfrc, sh_xfrc_overflow);
            atomicSplit(ypart, parent_atom, sh_yfrc, sh_yfrc_overflow);
            atomicSplit(zpart, parent_atom, sh_zfrc, sh_zfrc_overflow);
            const int95_t xrmdr = int95Sum(sh_xfrc[vs_atom], sh_xfrc_overflow[vs_atom], -xpart.x,
                                           -xpart.y);
            const int95_t yrmdr = int95Sum(sh_yfrc[vs_atom], sh_yfrc_overflow[vs_atom], -ypart.x,
                                           -ypart.y);
            const int95_t zrmdr = int95Sum(sh_zfrc[vs_atom], sh_zfrc_overflow[vs_atom], -zpart.x,
                                           -zpart.y);
            atomicSplit(xrmdr, frame2_atom, sh_xfrc, sh_xfrc_overflow);
            atomicSplit(yrmdr, frame2_atom, sh_yfrc, sh_yfrc_overflow);
            atomicSplit(zrmdr, frame2_atom, sh_zfrc, sh_zfrc_overflow);
#endif                      
          }
          break;
        case VirtualSiteKind::FIXED_2:
          {
            const size_t vs_atom_cpos = vs_atom + EXCL_GMEM_OFFSET;
#ifdef TCALC_IS_SINGLE
            const TCALC3 p_f2 = { (TCALC)(frame2_x - parent_x), (TCALC)(frame2_y - parent_y),
                                  (TCALC)(frame2_z - parent_z) };
            const TCALC3 p_vs = { (TCALC)(__ldca(&gmem_r.xcrd[vs_atom_cpos]) - parent_x),
                                  (TCALC)(__ldca(&gmem_r.ycrd[vs_atom_cpos]) - parent_y),
                                  (TCALC)(__ldca(&gmem_r.zcrd[vs_atom_cpos]) - parent_z) };
#else
            const TCALC3 p_f2 = {
              int95SumToDouble(frame2_x, frame2_x_ovrf, -parent_x, -parent_x_ovrf),
              int95SumToDouble(frame2_y, frame2_y_ovrf, -parent_y, -parent_y_ovrf),
              int95SumToDouble(frame2_z, frame2_z_ovrf, -parent_z, -parent_z_ovrf)
            };
            const TCALC3 p_vs = {
              int95SumToDouble(__ldca(&gmem_r.xcrd[vs_atom_cpos]),
                               __ldca(&gmem_r.xcrd_ovrf[vs_atom_cpos]), -parent_x, -parent_x_ovrf),
              int95SumToDouble(__ldca(&gmem_r.ycrd[vs_atom_cpos]),
                               __ldca(&gmem_r.ycrd_ovrf[vs_atom_cpos]), -parent_y, -parent_y_ovrf),
              int95SumToDouble(__ldca(&gmem_r.zcrd[vs_atom_cpos]),
                               __ldca(&gmem_r.zcrd_ovrf[vs_atom_cpos]), -parent_z, -parent_z_ovrf)
            };
#endif
            const TCALC invr_p_f2 = (TCALC)(1.0) / SQRT_FUNC((p_f2.x * p_f2.x) +
                                                             (p_f2.y * p_f2.y) +
                                                             (p_f2.z * p_f2.z));
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
            const TCALC3 vs_frc = { int63ToFloat(sh_xfrc[vs_atom], sh_xfrc_overflow[vs_atom]),
                                    int63ToFloat(sh_yfrc[vs_atom], sh_yfrc_overflow[vs_atom]),
                                    int63ToFloat(sh_zfrc[vs_atom], sh_zfrc_overflow[vs_atom]) };
#  else
            const TCALC3 vs_frc = { (TCALC)(sh_xfrc[vs_atom]), (TCALC)(sh_yfrc[vs_atom]),
                                    (TCALC)(sh_zfrc[vs_atom]) };
#  endif
#else
            const TCALC3 vs_frc = { int95ToDouble(sh_xfrc[vs_atom], sh_xfrc_overflow[vs_atom]),
                                    int95ToDouble(sh_yfrc[vs_atom], sh_yfrc_overflow[vs_atom]),
                                    int95ToDouble(sh_zfrc[vs_atom], sh_zfrc_overflow[vs_atom]) };
#endif
            const TCALC3 vs_frc_proj = projection(vs_frc, p_vs);
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
            const int2 xpart = floatToInt63(invr_p_f2 * fr_details.x * (vs_frc.x - vs_frc_proj.x));
            const int2 ypart = floatToInt63(invr_p_f2 * fr_details.x * (vs_frc.y - vs_frc_proj.y));
            const int2 zpart = floatToInt63(invr_p_f2 * fr_details.x * (vs_frc.z - vs_frc_proj.z));
            const int2 xrmdr = int63Sum(sh_xfrc[vs_atom], sh_xfrc_overflow[vs_atom], -xpart.x,
                                        -xpart.y);
            const int2 yrmdr = int63Sum(sh_yfrc[vs_atom], sh_yfrc_overflow[vs_atom], -ypart.x,
                                        -ypart.y);
            const int2 zrmdr = int63Sum(sh_zfrc[vs_atom], sh_zfrc_overflow[vs_atom], -zpart.x,
                                        -zpart.y);
            atomicSplit(xrmdr, parent_atom, sh_xfrc, sh_xfrc_overflow);
            atomicSplit(yrmdr, parent_atom, sh_yfrc, sh_yfrc_overflow);
            atomicSplit(zrmdr, parent_atom, sh_zfrc, sh_zfrc_overflow);
            atomicSplit(xpart, frame2_atom, sh_xfrc, sh_xfrc_overflow);
            atomicSplit(ypart, frame2_atom, sh_yfrc, sh_yfrc_overflow);
            atomicSplit(zpart, frame2_atom, sh_zfrc, sh_zfrc_overflow);
#  else
            const llint xpart = LLCONV_FUNC(invr_p_f2 * fr_details.x * (vs_frc.x - vs_frc_proj.x));
            const llint ypart = LLCONV_FUNC(invr_p_f2 * fr_details.x * (vs_frc.y - vs_frc_proj.y));
            const llint zpart = LLCONV_FUNC(invr_p_f2 * fr_details.x * (vs_frc.z - vs_frc_proj.z));
            atomicAdd((ullint*)&sh_xfrc[parent_atom], (ullint)(sh_xfrc[vs_atom] - xpart));
            atomicAdd((ullint*)&sh_yfrc[parent_atom], (ullint)(sh_yfrc[vs_atom] - ypart));
            atomicAdd((ullint*)&sh_zfrc[parent_atom], (ullint)(sh_zfrc[vs_atom] - zpart));
            atomicAdd((ullint*)&sh_xfrc[frame2_atom], (ullint)(xpart));
            atomicAdd((ullint*)&sh_yfrc[frame2_atom], (ullint)(ypart));
            atomicAdd((ullint*)&sh_zfrc[frame2_atom], (ullint)(zpart));
#  endif
#else
            const TCALC mfac = invr_p_f2 * fr_details.x;
            const int95_t xpart = doubleToInt95(mfac * (vs_frc.x - vs_frc_proj.x));
            const int95_t ypart = doubleToInt95(mfac * (vs_frc.y - vs_frc_proj.y));
            const int95_t zpart = doubleToInt95(mfac * (vs_frc.z - vs_frc_proj.z));
            const int95_t xrmdr = int95Sum(sh_xfrc[vs_atom], sh_xfrc_overflow[vs_atom], -xpart.x,
                                           -xpart.y);
            const int95_t yrmdr = int95Sum(sh_yfrc[vs_atom], sh_yfrc_overflow[vs_atom], -ypart.x,
                                           -ypart.y);
            const int95_t zrmdr = int95Sum(sh_zfrc[vs_atom], sh_zfrc_overflow[vs_atom], -zpart.x,
                                           -zpart.y);
            atomicSplit(xrmdr, parent_atom, sh_xfrc, sh_xfrc_overflow);
            atomicSplit(yrmdr, parent_atom, sh_yfrc, sh_yfrc_overflow);
            atomicSplit(zrmdr, parent_atom, sh_zfrc, sh_zfrc_overflow);
            atomicSplit(xpart, frame2_atom, sh_xfrc, sh_xfrc_overflow);
            atomicSplit(ypart, frame2_atom, sh_yfrc, sh_yfrc_overflow);
            atomicSplit(zpart, frame2_atom, sh_zfrc, sh_zfrc_overflow);
#endif
          }
          break;
        case VirtualSiteKind::FLEX_3:
          {
          }
          break;
        case VirtualSiteKind::FIXED_3:
          {
          }
          break;
        case VirtualSiteKind::FAD_3:
          {
          }
          break;
        case VirtualSiteKind::OUT_3:
          {
          }
          break;
        case VirtualSiteKind::FIXED_4:
          {
          }
          break;
        case VirtualSiteKind::NONE:
          break;
        }

        // Zero the force on the virtual site
#ifdef TCALC_IS_SINGLE
#  ifdef SPLIT_FORCE_ACCUMULATION
        sh_xfrc[vs_atom] = 0;
        sh_yfrc[vs_atom] = 0;
        sh_zfrc[vs_atom] = 0;
        sh_xfrc_overflow[vs_atom] = 0;
        sh_yfrc_overflow[vs_atom] = 0;
        sh_zfrc_overflow[vs_atom] = 0;
#  else
        sh_xfrc[vs_atom] = 0LL;
        sh_yfrc[vs_atom] = 0LL;
        sh_zfrc[vs_atom] = 0LL;
#  endif
#else
        sh_xfrc[vs_atom] = 0LL;
        sh_yfrc[vs_atom] = 0LL;
        sh_zfrc[vs_atom] = 0LL;
        sh_xfrc_overflow[vs_atom] = 0;
        sh_yfrc_overflow[vs_atom] = 0;
        sh_zfrc_overflow[vs_atom] = 0;
#endif
      }
      pos += blockDim.x;
    }

    // Synchronize to ensure that all virtual sites' forces have been transmitted to their frame
    // atoms in the space of the block's __shared__ memory.
    __syncthreads();

#ifdef VSITE_STANDALONE
    // Write forces back to global memory accumulators.  No atomics are needed for the standalone
    // kernel, as the work units are assigned atoms for which they will be computing all required
    // interactions (the atomic operations were done on forces in __shared__ memory within the
    // thread block).  No further synchronization is needed, as nothing depends on the work unit
    // index until after the following synchronization.
    pos = threadIdx.x;
    const int manip_llim    = vwu_map[(size_t)(VwuAbstractMap::MANIPULATE)].x;
    const int import_count_ii  = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].y -
                                 vwu_map[(size_t)(VwuAbstractMap::IMPORT)].x;
    const int import_stride_ii = devcRoundUp(import_hlim - import_llim, warp_size_int);
    while (pos < import_stride_ii) {
      if (pos < import_count_ii) {
        const int manip_segment = (pos >> warp_bits);
        const int manip_bitpos  = (pos & warp_bits_mask_int);
        const uint2 manip_mask = poly_auk.vwu_manip[manip_llim + manip_segment];
        if ((manip_mask.y >> manip_bitpos) & 0x1) {
          const size_t write_idx = __ldca(&poly_vk.vwu_imports[import_llim + pos]);
          __stwb(&poly_psw.xfrc[write_idx], sh_xfrc[pos]);
#  ifndef TCALC_IS_SINGLE
          __stwb(&poly_psw.xfrc_ovrf[write_idx], sh_xfrc_overflow[pos]);
#  endif
        }
      }
      pos += blockDim.x;
    }
    while (pos < 2 * import_stride_ii) {
      const int rel_pos = pos - import_stride_ii;
      if (rel_pos < import_count_ii) {
        const int manip_segment = (rel_pos >> warp_bits);
        const int manip_bitpos  = (rel_pos & warp_bits_mask_int);
        const uint2 manip_mask = poly_auk.vwu_manip[manip_llim + manip_segment];
        if ((manip_mask.y >> manip_bitpos) & 0x1) {
          const size_t write_idx = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
          __stwb(&poly_psw.yfrc[write_idx], sh_yfrc[pos]);
#  ifndef TCALC_IS_SINGLE
          __stwb(&poly_psw.yfrc_ovrf[write_idx], sh_yfrc_overflow[pos]);
#  endif
        }
      }
      pos += blockDim.x;
    }
    while (pos < 3 * import_stride_ii) {
      const int rel_pos = pos - (2 * import_stride_ii);
      if (rel_pos < import_count_ii) {
        const int manip_segment = (rel_pos >> warp_bits);
        const int manip_bitpos  = (rel_pos & warp_bits_mask_int);
        const uint2 manip_mask = poly_auk.vwu_manip[manip_llim + manip_segment];
        if ((manip_mask.y >> manip_bitpos) & 0x1) {
          const size_t write_idx = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
          __stwb(&poly_psw.zfrc[write_idx], sh_zfrc[pos]);
#  ifndef TCALC_IS_SINGLE
          __stwb(&poly_psw.zfrc_ovrf[write_idx], sh_zfrc_overflow[pos]);
#  endif
        }
      }
      pos += blockDim.x;
    }
    
    // No asynchronous scheduling will be provided for advancing work units that merely transmit
    // virtual site forces.  If the transmission is done as part of the valence work units, which
    // it will be in the dynamics applications that make up the more common use cases, the work
    // units will advance with the valence counter of a MolecularMechanicsControls object.
    if (threadIdx.x == 0) {
      vwu_idx += gridDim.x;
    }
    __syncthreads();
  }
}
#endif

#undef EXCL_GMEM_OFFSET
