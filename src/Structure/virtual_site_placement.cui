// -*-c++-*-
#include "copyright.h"

// The "standalone" pre-processor directive constructs a separate kernel to implement the virtual
// site placement.  Without the standalone directive, the code still assumes the availability of
// critical variables, including a phase space synthesis writeable abstract named poly_psw, a
// const valence kit (which includes valence work units and imported atom maps) named poly_vk,
// and a cache resource kit named gmem_r.
#ifdef VSITE_STANDALONE
__global__ void	__launch_bounds__(small_block_size, 4)
kPlaceVirtualSites(PsSynthesisWriter poly_psw, const SyValenceKit<TCALC> poly_vk,
                   const SyAtomUpdateKit<TCALC2, TCALC4> poly_auk,
                   const CacheResourceKit<TCALC> gmem_r) {
  __shared__ int2 vwu_map[vwu_abstract_length];
  __shared__ int vwu_task_count[vwu_abstract_length];
  __shared__ int vwu_padded_task_count[vwu_abstract_length];
  __shared__ int vwu_idx;

  if (threadIdx.x == 0) {
    vwu_idx = blockIdx.x;
  }
  __syncthreads();
  while (vwu_idx < poly_vk.nvwu) {

    // The instruction set map is read and stored in __shared__ for convenience, and to ensure
    // that it never leaves cache.  The instructions themselves are "streamed," which for purposes
    // of this documentation means read from global, used once, and not cached.  Each block must be
    // at least vwu_abstract_length + warp_size_int in size.  Currently, that is 96 on NVIDIA GPUs
    // 128 on commodity AMD GPUs, and only 64 on Intel GPUs.
    if (threadIdx.x < vwu_abstract_length) {
      vwu_map[threadIdx.x] = __ldcv(&poly_vk.vwu_abstracts[(vwu_idx * vwu_abstract_length) +
                                                           threadIdx.x]);
      vwu_task_count[threadIdx.x] = vwu_map[threadIdx.x].y - vwu_map[threadIdx.x].x;
      vwu_padded_task_count[threadIdx.x] = devcRoundUp(vwu_task_count[threadIdx.x], warp_size_int);
    }
    __syncthreads();

    // Import atomic coordinates, properties, and (if appropriate) velocities.  This employs all
    // threads of the block, breaking up each set of information at the warp level.
    const int import_llim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].x;
    const int import_hlim = vwu_map[(size_t)(VwuAbstractMap::IMPORT)].y;
    const int import_count  = import_hlim - import_llim;
    const int import_stride = devcRoundUp(import_hlim - import_llim, warp_size_int);
    int pos = threadIdx.x;
    while (pos < import_stride) {
      if (pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + pos;
        __stwb(&gmem_r.xcrd[write_idx], __ldcs(&poly_psw.xcrd[read_idx]));
#ifndef TCALC_IS_SINGLE
        __stwb(&gmem_r.xcrd_ovrf[write_idx], __ldcs(&poly_psw.xcrd_ovrf[read_idx]));
#endif
      }
      pos += blockDim.x;
    }
    while (pos < 2 * import_stride) {
      const int rel_pos = pos - import_stride;
      if (rel_pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.ycrd[write_idx], __ldcs(&poly_psw.ycrd[read_idx]));
#ifndef TCALC_IS_SINGLE
        __stwb(&gmem_r.ycrd_ovrf[write_idx], __ldcs(&poly_psw.ycrd_ovrf[read_idx]));
#endif
      }
      pos += blockDim.x;
    }
    while (pos < 3 * import_stride) {
      const int rel_pos = pos - (2 * import_stride);
      if (rel_pos < import_count) {
        const size_t read_idx  = __ldca(&poly_vk.vwu_imports[import_llim + rel_pos]);
        const size_t write_idx = EXCL_GMEM_OFFSET + rel_pos;
        __stwb(&gmem_r.zcrd[write_idx], __ldcs(&poly_psw.zcrd[read_idx]));
#ifndef TCALC_IS_SINGLE
        __stwb(&gmem_r.zcrd_ovrf[write_idx], __ldcs(&poly_psw.zcrd_ovrf[read_idx]));
#endif
      }
      pos += blockDim.x;
    }
    __syncthreads();
#endif

#ifdef VSITE_STANDALONE
  }
}
#endif
